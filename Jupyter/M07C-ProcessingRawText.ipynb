{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Modern Data Science\n**(Module 07: Natural Language Processing)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are  are free to use, change and distribute this package.\n- If you found any issue/bug for this document, please submit an issue at [tulip-lab/mds](https://github.com/tulip-lab/mds/issues)\n\nPrepared by and for \n**Student Members** |\n2006-2019 [TULIP Lab](http://www.tulip.org.au)\n\n---", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Session C - Processing Raw Text", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Accessing Text from the Web and from Disk\n\n### Ebook\nText number 2554 is an English translation of `Crime and Punishment`, and we can access it as follows.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "import nltk\nfrom urllib import request"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "url=\"http://www.gutenberg.org/files/2554/2554-0.txt\""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "response = request.urlopen(url)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "raw = response.read().decode('utf8')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "type(raw)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(raw)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "raw[:76]"
        }, 
        {
            "source": "* Note\n\nThe `read()` process will take a few seconds as it downloads this large book. If you're using an internet proxy which is not correctly detected by Python, you may need to specify the proxy manually, before using `urlopen`, as follows:\n``` Python\n\n    proxies = {'http': 'http://www.someproxy.com:3128'}\n    request.ProxyHandler(proxies)\n    \n```", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "The variable `raw` contains a string with 1,176,967 characters. This is the raw content of the book, including many details we are not interested in such as whitespace, line breaks and blank lines. Notice the `\\r` and `\\n` in the opening line of the file, which is how Python displays the special carriage return and line feed characters. For our language processing, we want to break up the string into words and punctuation. This step is called **tokenization**, and it produces our familiar structure, a list of words and punctuation.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "import nltk, re, pprint\nfrom nltk import word_tokenize"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tokens = word_tokenize(raw)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "type(tokens)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(tokens)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tokens[1:10]"
        }, 
        {
            "source": "Notice that NLTK was needed for **tokenization**, but not for any of the earlier tasks of opening a URL and reading it into a string. If we now take the further step of creating an NLTK text from this list, we can carry out all of the other linguistic processing, along with the regular list operations like **slicing**:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "text = nltk.Text(tokens)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "type(text)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "text[1024:1062]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "text.collocations()"
        }, 
        {
            "source": "Notice that Project Gutenberg appears as a collocation. This is because each text downloaded from Project Gutenberg contains a header with the name of the text, the author, the names of people who scanned and corrected the text, a license, and so on. Sometimes this information appears in a footer at the end of the file. We cannot reliably detect where the content begins and ends, and so have to resort to manual inspection of the file, to discover unique strings that mark the beginning and the end, before trimming raw to be just the content and nothing else:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "raw.find(\"PART I\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "raw.rfind(\"End of Project Gutenberg's Crime\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "raw = raw[5336:1157743]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "raw.find(\"PART I\")"
        }, 
        {
            "source": "The `find()` and `rfind()` (\"reverse find\") methods help us get the right index values to use for slicing the string. We overwrite `raw` with this slice, so now it begins with \"PART I\" and goes up to (but not including) the phrase that marks the end of the content.\n\nThis was our first brush with the reality of the web: texts found on the web may contain unwanted material, and there may not be an automatic way to remove it. But with a small amount of extra work we can extract the material we need.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Dealing with HTML", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "url = \"http://www.gutenberg.org/files/2554/2554-h/2554-h.htm\"\nhtml = request.urlopen(url).read().decode('utf8')\nhtml[:60]"
        }, 
        {
            "source": "You can type `print(html)` to see the HTML content in all its glory, including meta tags, an image map, JavaScript, forms, and tables.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "from bs4 import BeautifulSoup"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "raw = BeautifulSoup(html).get_text()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "tokens = word_tokenize(raw)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tokens"
        }, 
        {
            "source": "This still contains unwanted material concerning site navigation and related stories. With some trial and error you can find the start and end indexes of the content and select the tokens of interest, and initialize a text as before.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tokens = tokens[110:390]\ntext = nltk.Text(tokens)\ntext"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "text.concordance('Crime')"
        }, 
        {
            "source": "### Reading Local Files", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In order to read a local file, we need to use Python's built-in `open()` function, followed by the `read()` method. Suppose you have a file `document.txt`, you can load its contents like this:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "import wget"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "link_to_data = 'https://github.com/tulip-lab/mds/raw/master/Jupyter/data/document.txt'\n\nDataSet = wget.download(link_to_data)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "f = open('document.txt')\nraw = f.read()"
        }, 
        {
            "source": "If the interpreter couldn't find your file, you would have seen an error like this:\n\n<img src='https://github.com/tulip-lab/mds/raw/master/Jupyter/image/error-example.png' width = '600' height = '600' align = center />", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "To check that the file that you are trying to open is really in the right directory, examining the current directory within Python.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "import os\nos.listdir()"
        }, 
        {
            "source": "Assuming that you can open the file, there are several methods for reading it.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "raw"
        }, 
        {
            "source": "We can also read a file one line at a time using a `for` loop:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "f = open('document.txt', 'rU')\nfor line in f:\n    print(line.strip())"
        }, 
        {
            "source": "Here we use the `strip()` method to remove the newline character at the end of the input line.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "raw = open('document.txt').read()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "type(raw)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "tokens = nltk.word_tokenize(raw)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "type(tokens)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "words = [w.lower() for w in tokens]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "type(words)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "vocab = sorted(set(words))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "type(vocab)"
        }, 
        {
            "source": "we can concatenate strings with strings, and lists with lists, but we cannot concatenate strings with lists.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "query = \"who knows?\"\nbeatles = ['john', 'paul', 'george', 'ringo']\nquery + beatles"
        }, 
        {
            "source": "## Strings", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Basic Operations with Strings", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Strings are specified using single quotes or double quotes, as shown below. If a string contains a single quote, we must backslash-escape the quote so Python knows a literal quote character is intended, or else put the string in double quotes. Otherwise, the quote inside the string will be interpreted as a close quote, and the Python interpreter will report a syntax error:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "monty = 'Monty Python'\nmonty"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "circus = \"Monty Python's Flying Circus\"\ncircus"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "circus = 'Monty Python's Flying Circus'\ncircus"
        }, 
        {
            "source": "Sometimes strings go over several lines. Python provides us with various ways of entering them. In the next example, a sequence of two strings is joined into a single string. We need to use backslash or parentheses so that the interpreter knows that the statement is not complete after the first line.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "couplet = \"Shall I compare thee to a Summer's day?\"\\\n          \"Thou are more lovely and more temperate:\""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(couplet)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "couplet = (\"Rough winds do shake the darling buds of May,\"\n          \"And Summer's lease hath all too short a date:\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(couplet)"
        }, 
        {
            "source": "Besides, we can use a triple-quoted string as follows:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "couplet = \"\"\"Shall I compare thee to a Summer's day?\nThou are more lovely and more temperate:\"\"\"\nprint(couplet)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "couplet = '''Rough winds do shake the darling buds of May,\nAnd Summer's lease hath all too short a date:'''\nprint(couplet)"
        }, 
        {
            "source": "First let's look at the + operation, known as **concatenation**. It produces a new string that is a copy of the two original strings pasted together end-to-end. Notice that concatenation doesn't do anything clever like insert a space between the words. We can even multiply strings :", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "'very' + 'very' + 'very'"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "'very'*3"
        }, 
        {
            "source": "We've seen that the addition and multiplication operations apply to strings, not just numbers. However, note that we cannot use subtraction or division with strings:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "'very' - 'y'"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "'very' / 2"
        }, 
        {
            "source": "These error messages are another example of Python telling us that we have got our data types in a muddle. In the first case, we are told that the operation of subtraction (i.e., `-`) cannot apply to objects of type `str` (strings), while in the second, we are told that division cannot take `str` and `int` as its two operands.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Accessing Individual Characters", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "strings are indexed, starting from zero. When we index a string, we get one of its characters (or letters). A single character is nothing special \u2014 it's just a string of length 1.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "monty[0]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "monty[1]"
        }, 
        {
            "source": "As with lists, if we try to access an index that is outside of the string we get an error:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "monty[20]"
        }, 
        {
            "source": "Again as with lists, we can use negative indexes for strings, where `-1` is the index of the last character. Positive and negative indexes give us two ways to refer to any position in a string. In this case, when the string had a length of 12, indexes `5` and `-7` both refer to the same character (a space). (Notice that `5 = len(monty) - 7`.)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "monty[-1]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "monty[5]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "monty[-7]"
        }, 
        {
            "source": "We can write `for` loops to iterate over the characters in strings. This `print` function includes the optional `end=' '` parameter, which is how we tell Python to print a space instead of a newline at the end.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sent = 'colorless green ideas sleep furiously'\nfor char in sent:\n    print(char, end=' ')"
        }, 
        {
            "source": "We can count individual characters as well. We should ignore the case distinction by normalizing everything to lowercase, and filter out non-alphabetic characters:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "from nltk.corpus import gutenberg"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "raw = gutenberg.raw('melville-moby_dick.txt')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "fdist = nltk.FreqDist(ch.lower() for ch in raw if ch.isalpha())"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "fdist.most_common(5)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "[char for (char, count) in fdist.most_common()]"
        }, 
        {
            "source": "### Accessing Substrings\n\nA substring is any continuous section of a string that we want to pull out for further processing. We can easily access substrings using the same slice notation we used for lists (see as follow figure). For example, the following code accesses the substring starting at index `6`, up to (but not including) index `10`:\n\n<img src='https://github.com/tulip-lab/mds/raw/master/Jupyter/image/acc-substrings.png' width = '450' height = '450' align = center />\n\nString Slicing: The string \"Monty Python\" is shown along with its positive and negative indexes; two substrings are selected using \"slice\" notation. The slice `[m,n]` contains the characters from position `m` through `n-1`.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "monty[6:10]"
        }, 
        {
            "source": "Here we see the characters are 'P', 'y', 't', and 'h' which correspond to `monty[6]` ... `monty[9]` but not `monty[10]`. This is because a slice *starts* at the first index but finishes *one before* the end index.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We can also slice with negative indexes \u2014 the same basic rule of starting from the start index and stopping one before the end index applies; here we stop before the space character.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "monty[-12:-7]"
        }, 
        {
            "source": "As with list slices, if we omit the first value, the substring begins at the start of the string. If we omit the second value, the substring continues to the end of the string:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "monty[:5]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "monty[6:]"
        }, 
        {
            "source": "We test if a string contains a particular substring using the `in` operator, as follows:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "phrase = 'And now for something completely different'\nif 'thing' in phrase:\n    print('found \"thing\"')"
        }, 
        {
            "source": "We can also find the position of a substring within a string, using `find()`:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "monty.find('Python')"
        }, 
        {
            "source": "### The Difference between Lists and Strings", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Strings and lists are both kinds of sequence. We can pull them apart by indexing and slicing them, and we can join them together by concatenating them. However, we cannot join strings and lists:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "query = 'Who knows?'\nbeatles = ['John', 'Paul', 'George', 'Ringo']\nquery[2]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "beatles[2]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "query[:2]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "beatles[:2]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "query + \" I don't\""
        }, 
        {
            "source": "We cannot join strings and list. If you join them, you will report an <font color='red'>'TypeError'</font>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "beatles + 'Brian'"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "beatles + ['Brian']"
        }, 
        {
            "source": "Lists and strings do not have exactly the same functionality. Lists have the added power that you can change their elements:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "beatles[0] = \"John Lennon\""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "del beatles[-1]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "beatles"
        }, 
        {
            "source": "On the other hand if we try to do that with a *string* \u2014 changing the 0th character in `query` to <font color='green'>'F'</font> \u2014 we get:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "query[0]='F'"
        }, 
        {
            "source": "This is because strings are **immutable** \u2014 you can't change a string once you have created it. However, lists are **mutable**, and their contents can be modified at any time. As a result, lists support operations that modify the original value rather than producing a new value.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Regular Expressions", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "As a powerful way of searching, replacing, and parsing text with complex patterns of characters, regular expressions are the most significant tools in data parsing. They figure into all kinds of text-manipulation tasks. Searching and search-and-replace are among the most common uses. Regular expressions tend to be easier to write than they are to read. This is less of a problem if you are the only one who ever needs to maintain them. But if several people need to, the syntax can turn into more of a hindrance than an aid. For example,\n```python\n    ^(|(([A-Za-z0-9]+_+)|([A-Za-z0-9]+\\-+)|([A-Za-z0-9]+\\.+)|([A-Za-z0-9]+\\++))*[A-Za-z0-9]+@((\\w+\\-+)|(\\w+\\.))*\\w{1,63}\\.[a-zA-Z]{2,6})$\n```\nis a regular expression for validating email addresses.\nPlease don't try to parse it yourself, \nsince an experienced regular expression user might take a while to parse it.\nIn this section, we will first go through some good introductory materials of regular expressions,\nand then show you some fundamentals of how to use regular expressions in search text.\n\nThere are a couple of good online materials that introduce regular expressions in Python. \nWe strongly suggest that you study this chapter together with these materials. \nThey are \n* [Regular Expression HOWTO](https://docs.python.org/2/howto/regex.html) from Python's office website: An introductory tutorial to using regular expressions in Python with the `re` module. \ud83d\udcd6\n* [Regular Expressions](http://www.diveintopython3.net/regular-expressions.html), chapter 5 of \"**Dive into Python 3**\": A series of examples inspired by real world problems are used to show you how to generate regular expressions for parsing street name, Roman numerals, and phone numbers. \ud83d\udcd6\n\nThe complete list of meta-characters and their behaviour in the context of regular expressions can be found [here](https://docs.python.org/2/library/re.html). Besides, there is an alternative material if you would like to view, which is \n\n* [RegexOne](http://regexone.com): An interactive tutorial on learning regular expressions with simple exercises.\n\nBefore we go through some basics of regular expressions in python, we would like to point out [RegExr](http://regexr.com) by Grant Skinner. It is an online tool to learn, build, & test regular expressions. RegExr provides us with syntax highlighting, contextual help, video tutorial, reference, and searchable community patterns.\nYou will find a lot of good information in the six tabs provides on its website. In addition, pop-ups appear when you hover over the regular expression or target text in RegExr, giving you helpful information linking you between a regular expression and the corresponding matches in text. \nThese resources are one of the reasons why RegExr is among our favourite online Regex checkers.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "To use regular expressions in Python we need to import the `re` library using: `import re`.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "import re"
        }, 
        {
            "source": "### Backslash\n\n**First, what is '\\'? **\n\n'\\', backslash or escape-character, is used to indicate special forms or to allow special characters to be used without invoking their special meaning.\n\n\n\n\n**How about r\"\" ? When to use it? **\n\nr\"\" is Python\u2019s string literal prefix notation, which has nothing to do with regular expression.  By using r\"\" or r'', Python will not handle special characters in any special way, in another word, it treated the contents as raw string. For example, r\"\\t\" represents\na two-character string containing '\\' and 't', whereas \"\\t\" represents tab.\n\nSometimes you can use them interchangeably.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "str1 = re.findall('\\t', \"Please find \\t\")\nprint (str1)\n\nstr2 = re.findall(r'\\t', \"Please find \\t\")\nprint (str2)"
        }, 
        {
            "source": "Sometimes not!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "str1=re.match(r\"\\W(.)\\1\\W\", \" ff \")\nprint (str1)\n\nstr2=re.match(\"\\W(.)\\1\\W\", \" ff \")\nprint (str2)\n\nstr3=re.match(\"\\\\W(.)\\\\1\\\\W\", \" ff \")\nprint (str3)"
        }, 
        {
            "source": "\"\\W(.)\\1\\W\" doesn't match ?  What is the difference? ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "str4=\"\\W(.)\\1\\W\"\nprint (str4)\nstr4"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "str4=r\"\\W(.)\\1\\W\"\nprint (str4)\nstr4"
        }, 
        {
            "source": "Now you might be able to guess, what \"\\W(.)\\1\\W\" will match", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "str2=re.match(\"\\W(.)\\1\\W\", \" f\\x01 \")\nprint (str2)"
        }, 
        {
            "source": "It matches with non-word + any one character  + \"\\x01\" + non=word.\n\n*Conclusion -- always fist validate your regular expression, then test with Python*\n\n\\* is ??  <br>\n\\* is a wildcard similar with ? and +  <br>\n\\* matches 0+ <br>\n? matches 0-1 <br>\n\\+ matches 1+ <br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "str1 = re.findall(r'.*', 'Please find all.')\nprint (str1)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "str1 = re.findall(r'.?', 'Please find all.')\nprint (str1)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "str1 = re.findall(r'.+', 'Please find all.')\nprint (str1)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "str1 = re.findall(r'l+', 'Please find all')\nprint (str1)"
        }, 
        {
            "source": "### Matching String Literals\nMatching strings with one or more literal characters, called string literals, is similar to the way you might do a search in a word editor or when submitting a keyword to a search engine. When you search for a string of text, you are searching with a string literal.\nLet's start with a very simple scenario. \nIf we have a sentence like\n```\n    Today is 26 jan 2016, not 25 Jan 2016.\n```\nAnd want to see if the string contains the word `Jan`  using a Python regular expression,\nwe'd use the following", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import re # The Regular Expressions library\nstr = \"Today is 26 jan 2016, not 25 Jan 2016.\" \ns = re.search(\"Jan\", str)\nprint (s)"
        }, 
        {
            "source": "The simple pattern used above is just something like 'J' followed by 'a' followed by 'n' (i.e., 'Jan').\nThe `search()` method scans through the string, looking for any location where 'Jan' appears. If a match is found, a match object instance corresponding to the first match is returned. Our search was successful, as the code prints out the match object\nas \n```\n    <_sre.SRE_Match object at 0x103ed47e8>\n```\nThis is Python's way of saying 'True' or 'Yes'. If no match is found, it will print out 'None'. \nFor example, try the following ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print (re.search(\"Feb\", str))"
        }, 
        {
            "source": "The returned match object contains information about the match: where it starts and ends, the substring it matched, and more. You can query the match object for information about the matching string. The most important ones are:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print (s.group())\nprint (s.start()) \nprint (s.end())\nprint (s.span())"
        }, 
        {
            "source": "The `group()` method returns the string \"Jan\" matched by the regular expression. \nThe `start()` method returns the starting position of \"Jan\", which is equal to the index of 'J' in the whole string.\nGo ahead, count the characters in \"Today is 18 Jan 2016.\", starting at \"T\", then try:\n```python\n   str.index('J')\n```\nIt should give the same integer as that given by `s.start()`. \nThe `end()` method returns the ending position of the match, \nand the span() method returns a tuple containing the (start, end) positions.\nThis scenario is so simple that you don't need a regular expression.\nInstead, you can use a string function, `find()`, which gives you the start position of the target string.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "str.find(\"Jan\")"
        }, 
        {
            "source": "How about finding both \"Jan\" and \"jan\"? \nThe `find()` method can only find the first match of a given regular expression. \nThere are two pattern methods that return all of the matches for a pattern encoded in a given regular expression. \nThey are `findall()` and `finditer()`.\nThe former returns a list of matching strings, \nand the latter returns a sequence of match object instances as an iterator. \nLet's try!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print (re.findall(\"Jan\", str))\nfor m in re.finditer(\"Jan\", str):\n    print (m.group())\n    print (m.span())"
        }, 
        {
            "source": "However, using \"Jan\" can find the one with uppercase \"J\", but not the one with lowercase 'j'. \nThe reason is that string matching is case-sensitive in regular expressions. \nIf you want to match both lower- and uppercase, you can: \n1. Convert all the characters in the string into either lower- or uppercase ones, then use either `re.findall(\"jan\", str)` or `re.findall(\"JAN\", str)` respectively to find the two appearances of \"Jan\",\n2. Update our regular expression to account for both 'J' and 'j', and retrieve both \"jan\" and \"Jan\" in their original form, like: \n```python\n    [Jj]an\n```\nwhere '[ ]' indicates a set of characters, and '[Jj]' will match 'J' or 'j', which is also known as a character class.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "re.findall(r\"[Jj]an\", str)"
        }, 
        {
            "source": "Our second choice is to use grouping in regular expressions. For multiple options we place them in brackets () and separate them by a pipe |. So we could use:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "re.findall(r\"(Jan|jan)\", str)"
        }, 
        {
            "source": "Let's move one-step further to find all the words with only alphabetic characters using only regular expressions. \nIt is not feasible to use grouping to enumerate all the possible words. \nInstead, we are going to use '[ ]' together with '+'.\nYou have seen '[ ]' above. '+' means matching 1 or more repetitions of the preceding regular expression. \nFor example,\n'an+' will match \u2018a\u2019 followed by any non-zero number of \u2018n\u2019s; \nit will not match just \u2018a\u2019. \nTo match non-zero numbers of either lower- or uppercase characters, we derive the following regular expression:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "re.findall(r\"[a-zA-Z]+\", str)"
        }, 
        {
            "source": "In the example above, we represent a range of characters by giving two characters separated by a '-'. For example [a-z] will match any lowercase ASCII letters, and [A-Z] will match any uppercase ASCII letters. Put the two together, we derive the regular expression that matches any lower- or uppercase letters.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Matching Digits\nThere are several ways to represent digits in regular expressions:\n* [0-9]: A range that matches the range of digits 0 through 9, which is the same as \"[0123456789]\".\n* \\d: A character shorthand to match the digits, which is pre-defined in most regular expression engines.\nIt is equivalent to [0-9].\n\nNote that the character shorthand for digits is shorter and simpler, \nbut it doesn\u2019t have the power or flexibility of the range. \nWith a range, you can pick the exact digits you want to match. \nFor example, if you want to match a sequence of the binary digits, like '0010101011', \nyou would use\n```python\n    [01]+\n```\n\nTo match numbers that have more than one digit, for example, '12' and '123',\nyou can repeat either representation as many times as you want, like\n* [0-9][0-9] or \\d\\d matches two-digits numbers from 00 to 99.\n* [0-9][0-9][0-9] or \\d\\d\\d matches three digits numbers from 000 to 999.\n\nHowever, the above approach gets redundant if you try to match '100000' for example.\nIn this case, we can specify the number of occurrences of those digits by using \ncurly brackets, like:  \n* [0-9]{2} or \\d{2} that matches numbers from 00 to 99.\n* \\d{1,3} that matches numbers from 0 to 999.\n\nLet's try to extract year for the give string,", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "s = re.search(r'\\d{4}', str) \nprint(s.group()) "
        }, 
        {
            "source": "As we discussed before, the `search()` method returns the first match found in the string.\nHowever, if search stops when it finds the first occurrence, what is the point of group?\n\nRegular expressions allow us to not just match text but also to extract information for further processing. This is done by defining groups of characters and capturing them using the special parentheses, i.e., ( and ) meta-characters. Any sub-pattern inside a pair of parentheses will be captured as a group.\nLet's try to find a pair of words separated by a white space in the following simple\nstring\n```python\n    Isaac Newton, Data Scientist\n```\nThe regular expression we are going to use is \n```python\n    ([a-zA-Z]+) ([a-zA-Z]+)\n```\nIt uses two groups. One is used to match the first word in the pair and another matches\nthe second word. Note that there is a white space between the two groups.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "m = re.match(r\"([a-zA-Z]+) ([a-zA-Z]+)\", \"Isaac Newton, Data Scientist\")\nprint(m.group(0) + \"\\n\" + m.group(1)  + \"\\n\" + m.group(2))"
        }, 
        {
            "source": "As you can see, `m.group(0)` returns the entire match. `m.group(1)` returns the match of the first parenthesized subgroup. And `m.group(2)` returns the match of the second parenthesized subgroup.\nYou can also retrieve the two groups by using the `groups()` methods.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "m.groups()"
        }, 
        {
            "source": "In Python regular expressions, you can also name each group in a regular expression using \n```python\n    (?P<name>...)\n```\nThe substring matched by the group is accessible via the symbolic group name 'name'.\nFor example:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "m = re.match(r\"(?P<first_name>[a-zA-Z]+) (?P<last_name>[a-zA-Z]+)\", \"Isaac Newton\")\nm.groupdict()"
        }, 
        {
            "source": "### More on Regular Expression Syntax\nWe have shown you how to match words and digits in the previous two sections. Here we would like to list some meta-characters that are used very often in regular expressions:\n* \\D: Matches characters that are not digits, which is equivalent to [^0-9] or [^\\d].\n* \\w: Matches any alphanumeric character, which is equivalent to [a-zA-Z0-9].\n* \\W: Matches any non-alphanumeric character; which is equivalent to [^a-zA-Z0-9] or [^\\w].\n* \\s: Matches any whitespace character; which is equivalent to [ \\t\\n\\r\\f\\v], where \\t indicates taps, \\n  line feeds, \\r carriage returns, \\f form feeds and \\v vertical tabs.\n* \\S: Matches any non-whitespace character; which is equivalent to  [^ \\t\\n\\r\\f\\v].", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Raw Strings in Python Regular Expressions\nWe have been using 'r' in our regular expressions, what does it mean?\nIt is Python's raw string notation for regular expressions.\nIt has been used to work around the backslash plague.\n\nIn regular expressions the backslash character ('\\')  is often used to escape characters that otherwise have a special meaning, such as newline, backslash itself, or the quote character. For example, to match a literal backslash, one has to write '\\\\\\\\\\\\\\\\' as the regular expression string. This is because the regular expression must be '\\\\\\\\', and each backslash must be expressed as '\\\\\\\\' inside a regular Python string literal. Let's assume that you would like find all the LaTeX commands in a given LaTeX file. Those commands always start with a backslash, like '\\\\usepackage',\n'\\\\section', '\\\\title', etc. The regular expression without raw string notation is:\n```python\n    \\\\\\\\\\\\w+\n```\nRefer to the previous section for the meaning of \"\\w\".\nIn contrast, one can prefiex the string literals with a letter 'r' or 'R' to form a raw string notation, which tells \nthe regular expression engine not to handle backslashes in any special way. With the raw string notation, the regular expression above can be simplified to \n```python\n    r\"\\\\\\w+\"\n```\nLet's try them out:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "m1 = re.match(\"\\\\\\\\\\w+\", \"\\section\")\nprint (m1.group())\n\nm2 = re.match(r\"\\\\\\w+\", \"\\section\")\nprint (m2.group())"
        }, 
        {
            "source": "The two lines of matching code above are functionally identical. But it is easy to interpret the regular expression using raw string notation. Therefore, when writing regular expression in Python, it is recommended that you use raw strings instead of regular Python strings. \n\n- - -", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Parsing Dates with Regular Expressions\n\n\nThis section will show you how to parse dates in simple data formats, \ne.g., mm/dd/yyyy, and dd/mm/yyyy. You might think that something as conceptually trivial as a date should be an easy job for a regular expression. But it isn\u2019t, for reasons like: \n* The problem of leading zeros: humans are very sloppy with writing dates. Sometimes we omit the leading zeros, and write dates like \"1/1/2016\" and \"1/01/2016\". Therefore, should the regular expression for dates allow leading zeros to be omitted?\n* Different date delimiters: besides forward slashes, we can also use white spaces, or hyphens to separate day, month and year.\n* Matching a given range of numbers: regular expressions don't deal directly with numbers and don't understand the numerical meanings that humans assign to strings of digits. They treat numbers, like 123, as strings of characters displayed as digits, 1, 2, and 3. Therefore, we cannot tell a regular expression to match a given range of numbers directly. For instance, to match months that are in a range from 1 to 12 and to match days from 1 to 31.\n\nTherefore, you have to choose how simple or how accurate you want your regular expression to be.\nIf you already know your text doesn\u2019t contain any invalid dates, you could use a trivial regex such as\n```python\n    r\"\\d{2}/\\d{2}/\\d{4}\"\n```\nThe fact that this matches things like 00/00/0000 is irrelevant if those don\u2019t occur in your text.\nIn most cases, you won't know whether your text has invalid dates or not. \n\nSo given that a basic date is day, month and year, and are all digits, which of the three is easiest to parse with regular expressions?\nGive month a try. First define our own method 'month' which accepts a pattern and a month (both text) as arguments and reports if there is a match:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "def month(pattern, m):\n    if re.match(pattern, m):\n        print (m + \" is a month\")\n    else:\n        print (m + \" is NOT a month\")"
        }, 
        {
            "source": "It seems that it is trivial to write a regular expression to match the 12 months from 1 to 12 with or without \nleading zeros. \nLet's first assume that all months are represented by two digits. \nIn other words, we append a zero to the left if the month is in between January to September. \nThe simplest regular expression one can think could be\n```python\n    r\"\\d\\d\"\n```\nTry it out", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "month(r'\\d\\d', \"12\")\nmonth(r\"\\d\\d\", \"03\")\nmonth(r\"\\d\\d\", \"00\")\nmonth(r\"\\d\\d\", \"13\")\nmonth(r\"\\d\\d\", \"3\")"
        }, 
        {
            "source": "The regular expression we used matches exactly two-digit numbers from 00 to 99. \nAlthough it can match all the months represented by two digits, the problems are that \n* It cannot match months represented by a single digit, e.g., 1 (January), 2 (February), etc.\n* It matches numbers that do not represent any month. \n  So one does need to validate the given number to make sure it is in the right range.\n\nTackling the first problem, we can use curly brackets `{m, n}` to specify the minimum and maximum occurrences of digits. A month can have at least one digit and at most two digits. \nSo the regular expression should look like\n```python\n    r\"\\d{1,2}\"\n```", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "month(r\"\\d{1,2}\", \"03\") \nmonth(r\"\\d{1,2}\", \"3\") \nmonth(r\"\\d{1,2}\", \"00\") \nmonth(r\"\\d{1,2}\", \"0\")\nmonth(r\"\\d{1,2}\", \"13\")"
        }, 
        {
            "source": "However, this regular expression still matches invalid months, such as \"00\", \"0\" and \"13\".\nThe months must be restricted to numbers between 1 and 12.\nWe use alternation inside a group to match various pairs of digits to form a range of one- or two-digit numbers.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "month(r\"([1-9]|1[0-2])\", \"3\") \nmonth(r\"([1-9]|1[0-2])\", \"0\") \nmonth(r\"([1-9]|1[0-2])\", \"01\") "
        }, 
        {
            "source": "In the above code `[1-9]` matches months that can be represented by a single digit, and `1[0-2]` matches October, November and December. Let's further update the regular expression to allow leading zeros by adding `0?`:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "month(r\"(0?[1-9]|1[0-2])\", \"03\")"
        }, 
        {
            "source": "It seems that we have constructed a regular expression that can handle months represented by either one- or two-digits numbers. But sooner or later you will find the following problem: ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "month(r\"(0?[1-9]|1[0-2])\", \"13\") \nmonth(r\"(0?[1-9]|1[0-2])\", \"99\") "
        }, 
        {
            "source": "Why?\n\nSome of these patterns seem right but don't always work. \nRegular expressions are quite specific, like mini programs.\nYou have to get them right and then they will very effectively block everything that doesn't match.\nWe very specifically say what we want, as opposed to listing all the exceptions we don't want.\nwhich is easier?\nFor example testing all exceptions, case by case:\n* is the input empty (and this in itself is trouble, one space or two? ' ', tab, CR, LF etc.)\n* is the input the correct type (character, number etc.)\n* the correct format\n* correct range\n* positive, negative\n* uppercase, lowercase, etc.\n\nWatch out for the difference between greediness & laziness in regular expressions. \nGreediness means match longest possible string.\nLaziness means match shortest possible string. \nOr, put another way, laziness will stop as soon as the condition is satisfied, \nbut greediness means it will stop only once the condition is not satisfied any more - this is quite different.\n\nConsider also Start of String and End of String anchors. The caret ^ matches the position before the first character in the string. Applying \"^a\" to \"abc\" matches the whole string. \"^b\" does not match \"abc\" at all, because the b cannot be matched right after the start of the string, matched by ^. Similarly, $ matches right after the last character in the string.\n\nSo here's what we want:\n```python\n    r\"^(0?[1-9]|1[0-2])$\" \n```\nLet's test it now:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "pattern =  \"^(0?[1-9]|1[0-2])$\" "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "month(pattern,\"03\") \nmonth(pattern,\"0\")\nmonth(pattern,\"033\")\nmonth(pattern,\"003\")\nmonth(pattern,\"99\")\nmonth(pattern,\"3\")"
        }, 
        {
            "source": "Similarly, you can write regular expressions to validate days. \nWe will leave this for you to do as an exercise.\nNext, we are going to show you the regular expressions for handling years in 20th and 21st centuries. \nThese years are between 1900 and 2099.\nThe first two digits are either 19 or 20, which can be captured by a group alternating between these two numbers\n```python\n    (19|20)\n```\nEach of the last two digits contains numbers between 0 and 9, which can be easily captured by\n```\n    \\d{2}\n```\nPut them together and we have\n```\n    r\"(19|20)\\d{2}\"\n```", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def year(pattern, m):\n    if re.match(pattern, m):\n        print (m + \" is a year\")\n    else:\n        print (m + \" is NOT a year\")\n        \nyear(r\"(19|20)\\d{2}\", \"1800\")\nyear(r\"(19|20)\\d{2}\", \"1900\")\nyear(r\"(19|20)\\d{2}\", \"2099\")\nyear(r\"(19|20)\\d{2}\", \"2100\")"
        }, 
        {
            "source": "Dealing with leap years is not trivial. Can one write a regular expression that can distinguish days in February in either leap years or non-leap years? It is easy to write regular expressions to match February 29th regardless of the year. Allowing February 29th only in leap years would require us to spell out all the years that are leap years, and all the years that aren\u2019t. Therefore, it seems that regular expressions are not a good choice here. Handling leap years require an extra bit of code. Maybe it's better to do it in two stages:\n1. Does it look like a date? (use regex), then\n2. is it a date? (code, e.g. convert to numeric then > 0 and < 13)\n\nFor example, the regular expression we found here:\n```python\n    r'(19|20)\\d\\d[- /.](0[1-9]|1[012])[- /.](0[1-9]|[12][0-9]|3[01])'\n```\nmatches a date in traditional date format from between 1900-01-01 and 2099-12-31, with a choice of four separators.\nHowever, there are dates that match the regular expression but aren't valid.\nFor example:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "pattern = r'(19|20)\\d\\d[- /.](0[1-9]|1[012])[- /.](0[1-9]|[12][0-9]|3[01])'\nyear(pattern, \"2016-02-31\") "
        }, 
        {
            "source": "It is impossible for February in any year can have more than 29 days. Instead of using regular expressions to validate \ndates, you can also use Python's `datatime` module. If a given date string cannot be converted to a Python Date object, then the date wouldn't be valid.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# use python datetime libraries\nimport datetime as dt\ntoday = \"2016-02-31\"\nmydt = dt.datetime.strptime(today, '%Y-%m-%d') "
        }, 
        {
            "source": "Therefore, \nif you get regular expressions right, they can be very useful as anything that doesn't match the pattern will get blocked. However, getting them wrong will result in many problems.\n- - -", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Extract IPs, dates, and email address with regular expressions\n\nWith following tasks we will use the mail box data ([mbox-short.txt](http://www.pythonlearn.com/code3/mbox-short.txt)) provided by the book [Python for Informatics: Exploring Information](http://www.pythonlearn.com/book.php#python-for-informatics). \n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "!pip install wget"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import wget\n\nlink_to_data = 'https://github.com/tulip-lab/mds/raw/master/Jupyter/data/mbox-short.txt'\n\nDataSet = wget.download(link_to_data)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "!ls"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "with open('mbox-short.txt','r') as infile:\n    text = infile.read()"
        }, 
        {
            "source": "### Find IP addresses \n\nIn this task we will need to \n1. find all IP addresses in the mbox-short dataset.\n2. print unique IP addresses \n\nLet's have a try first: ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "str1 = re.findall(r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b', \"This is a IP address 111.23.39.99\")\nstr1"
        }, 
        {
            "source": "![](https://github.com/tulip-lab/mds/raw/master/Jupyter/image/regeximg3.png)\n\nFrom https://regexper.com/", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "str1= re.findall(r'(?:[0-9]{1,3}\\.){3}[0-9]{1,3}', text)\nif len(str1)>0:\n    print(str1)"
        }, 
        {
            "source": "By running the code above, we are able to print all IP addresses. \n\nNext can we save all unique IP address in a list? We will need to read the whole txt file in to 'text', and then apply re.findall function. set() function returns the unique values.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "str1=re.findall(r'(?:[0-9]{1,3}\\.){3}[0-9]{1,3}', text)\nset(str1)"
        }, 
        {
            "source": "### Extract All date time \n\n\nIn the next task, we need to extract all date time from the file. We trust that all date time are valid for now. \n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "str1=re.findall(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})', text)\nset(str1)"
        }, 
        {
            "source": "From the extract datetime string, extract date and hour information by using nested group", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "str2=re.findall(r'((\\d{4}-\\d{2}-\\d{2} \\d{2}):\\d{2}:\\d{2})', text)\nset(str2)"
        }, 
        {
            "source": "### Extract author's email address\n\n\nThere are many email addresses included in the file. We would like to extract email addresses from the Author the format is normally:\n\n\"Author: stephen.marquard@uct.ac.za\"\n\nNow lets see if we can use the following regular expression:\n```python\nr\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\"\n```\nwhich was copied and pasted from http://emailregex.com/\n\nDoes it work in the task?", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "str1=re.findall(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\", text)\nset(str1)"
        }, 
        {
            "source": "What if I only want email address after Author ? ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "str1=re.findall(r'Author: ([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)', text)\nstr1"
        }, 
        {
            "source": "##  Pre-Processing Text\n\nThe possible steps of text pre-processing are nearly the same for all text analysis tasks, though which pre-processing steps are chosen depends on the specific task. The basic steps are as follows:\n* Tokenization\n* Case normalization\n* Removing Stop words\n* Stemming and Lemmatization\n* Sentence Segmentation\n\nWe will walk you through each of these steps with some examples. First, you need to \ndecide <font color=\"red\">the scope of the text to be used in the downstream text analysis tasks</font>. Should you use an entire document?\nOr should you break the document down into sections, paragraphs, or sentences. Choosing \nthe proper scope depends on the goals of the analysis task.\nFor example, you might choose to use an entire document in document classification and clustering tasks\nwhile you might choose smaller units like paragraphs or sentences in document summarization and information\nretrieval tasks. The scope chosen by you will have an impact on the steps needed in the pre-processing process.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Tokenization\n\nText is usually represented as sequences of characters by computers. \nHowever, most natural language processing (NLP) and text mining tasks\n(e.g., parsing, information extraction, machine translation, document classification, information\nretrieval, etc.) need to operate on tokens. \nThe process of breaking a stream of text into tokens is often referred to as **tokenization**.\nFor example, a tokenizer turns a string such as \n```\n    A data wrangler is the person performing the wrangling tasks.\n```\ninto a sequence of tokens such as\n```\n    \"A\" \"data\" \"wrangler\" \"is\" \"the\" \"person\" \"performing\" \"the\" \"wrangling\" \"tasks\"\n```\n\nThere is no single right way to do tokenization. \nIt completely depends on the corpus and the text analysis task you are going to perform. It is important to ensure that your tokenizer produces proper token types for your downstream text analysis tools. \nAlthough word tokenization is relatively easy compared with other NLP or text mining task, errors made in this phase will propagate into later analysis and cause problems.\nIn this section, we will demonstrate the process of chopping character sequences into pieces with different tokenizers. \n\nThe major question of the tokenization phase is what counts as a token.\nDifferent linguistic analyses might have different notions of tokens.\nIn different languages, a token could mean different things. \nHere we are not going to dive into the linguistic aspect of what counts as a token,\nas it goes beyond the scope of this unit.\nWe rather consider English text.\nIn English, a token can be a string of alphanumeric characters separated by spaces, which\nseems quite easy.\nHowever, things get considerably worse when we start considering words having\nhyphens, apostrophes, periods and so on. In a word tokenization task, should we\nremove hyphens? Should we keep periods? \nAccording to different text analysis tasks, \ntokens can be unigram words, multi-word phrases (or collocations), or \nother meaningful and identifiable linguistic elements.\nTherefore, working out word tokens is not an easy task in pre-processing natural language text.\nReading materials associated with this section are [1], section 3.7 of [2], [3] and section 4.2.2 of [6].\nYou might be interested in watching a YouTube video on [word tokenization](https://www.youtube.com/watch?v=jBk24DI8kg0).\n\n#### Standard Tokenizer\n\nFor English, a straightforward tokenization strategy is to use white spaces as token delimiters. \nThe whitespace tokenizer simply splits the text on any sequence of whitespace, tab, or newline characters.\nConsider the following hypothetical text.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "raw = \"\"\"The GSO finace group in  U.S.A. provided Cole with about\nUS$40,000,555.4 in funding, which accounts for 35.3% of Cole's revenue (i.e., AUD113.3m), \nas the ASX-listed firm battles for its survival.\nMr. Johnson said GSO's recapitalisation meant \"the current shares are worthless\".\"\"\""
        }, 
        {
            "source": "As a starting point, let's tokenize the text above by using any whitespace characters as token delimiters.\nAs mentioned, these characters include whitespace (' '), tab ('\\t'), newline ('\\n'), return ('\\r'), and so on.\nYou have learnt in week 2 that those characters are together represented by a built-in regular expression abbreviation '\\s'.\nThus, we will use '\\s' rather than writing it as something like '[ \\t\\n]+'.\n\nThere are multiple ways of tokenizing a string with whitespaces.\nThe simplest approach might be using Python's string function `split()`.\nThis function returns a list of tokens in the string.\nAnother way is to use Python's regular expression package, `re` as\n```python\n    import re\n    re.split(r\"\\s+\", raw)\n```\nThe output should be exactly the same as that given by the string function `split()`.\nHere we further demonstrate the use of <font color=\"blue\">RegexpTokenzier</font> from Natural Language Toolkit (NLTK).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "import nltk\nfrom nltk.tokenize import RegexpTokenizer"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tokenizer = RegexpTokenizer(r\"\\s+\", gaps=True)\ntokens = tokenizer.tokenize(raw)\ntokens"
        }, 
        {
            "source": "A <font color=\"blue\">RegexpTokenizer</font> splits a string into tokens using a regular expression.\nRefer to its online [documentation](http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.regexp.RegexpTokenizer) \nfor more details.\nIts constructor takes four arguments.\nThe compulsory argument is the pattern used to build the tokenizer.\nIt is in the form of a regular expression. \nIn the example above, we used `\\s+` to match 1 or more whitespace characters.\nIf the pattern defines separators between tokens, the value of `gaps` should be\nset to `True`. Otherwise, the pattern should be used to find the tokens.\nNLTK also provides a whitespace tokenizer, `WhitespaceTokenizer[source]`, which is\nequivalent to our tokenizer. Try\n```python\n    from nltk.tokenize import WhitespaceTokenizer\n    WhitespaceTokenizer().tokenize(raw)\n```\n\nIt seems that word tokenization is quite simple if words in a language are all\nseparated by whitespace characters. \nHowever, this is not the case in many languages other than English, such\nas Chinese, Japanese, Korean and Ancient Greek. \nIn those languages, text is written without any whitespaces between words. \nSo the whitespace tokenizer is of no use at all.\nTo handle them, we need more advanced tokenization techniques, often referred to as\nword segmentation, which is an important and challenging task in NLP. \nHowever,\ndiscussing word segmentation is beyond our scope here.\n\nIt is not surprising that the whitespace tokenizer is insufficient even for English, since English does not just contains sequences of alphanumeric characters separated by white spaces. \nIt often contains punctuation, hyphen, apostrophe, and so on.\nSometimes whitespace does not necessarily indicate a word break. \nFor example, non-compositional phrases (e.g., \"real estate\" and \"shooting pain\") and proper nouns (e.g., \"The New York Times\") have a different meaning than the sum of their parts. They cannot be split in the process of word tokenization.\nThey must be treated as a whole in, for instance, information retrieval.\n\nBack to our example, \nthe whitespace tokenizer still gives us word like \"(i.e.,\", \"funding,\" and \"worthless\".\".\nWe would like to remove parentheses, some punctuations, quotation marks and other non-alphanumeric characters.\nA simple and straightforward strategy is to use all non-alphanumeric characters as token delimiters.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tokenizer = RegexpTokenizer(r\"\\W+\", gaps=True) \ntokenizer.tokenize(raw)"
        }, 
        {
            "source": "In regular expressions, '\\W' indicates any non-alphanumeric characters (equivalent to `[^a-zA-Z0-9]`) while '\\w' indicates any alphanumeric characters. \nThe counterpart is to extract tokens that only consist of alphanumeric characters without the empty strings. Try the following out yourself:\n```python\n    tokenizer = RegexpTokenizer(r\"\\w+\")\n    tokenizer.tokenize(raw)\n```\n\nThese two strategies are simple to implement, but there are cases where they may not match the desired behaviour. \nFor example, the whitespace tokenizer cannot properly handle non-alphanumeric characters, while the non-alphanumeric tokenizer might over-tokenise some tokens with periods, hyphens, apostrophes, etc.\nIn the rest of this section, we will discuss the main problems that you might face while tokenising free language text. You will soon find that tokenizers should often be customized to deal with different datasets.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Periods in Abbreviations\n\nWord tokens are not always surrounded by whitespace characters. Punctuation, such as commas, semicolons, and periods, are often used in English, as they are vital to disambiguate the meaning of sentences. However, it is problematic for computers to handle punctuation, especially periods, properly in tokenization. \nIn this part we will focus on the handling of periods.\n\nPeriods are usually used to mark the end of sentences. Difficulty arises when the period marks abbreviations (including acronyms). Please refer to **\"Step 2: Handling Abbreviations\" in [3]** for a detailed discussion on abbreviations.  In the case of abbreviations, particularly acronyms, separating tokens on punctuation and other non-alphanumeric characters would put different components of the acronym into different tokens, as you have seen in our example, where \"U.S.A\" has been put into three tokens, \"U\", \"S\" and \"A\", losing the meaning of the acronym. To deal with abbreviations, one approach is to maintain a look-up list of known abbreviations during tokenization. Another approach aims for smart tokenization. Here we will show you how to use regular expressions to cover most but not all abbreviations.\n\nAn acronym is often formed from the initial components in multi-word phrases.  Some contains periods, and some do not. Common acronyms with periods are for example, \n* U.S.A\n* U.N.\n* U.K.\n* B.B.C\n\nOther abbreviations with a similar pattern are, for instance, \n* A.M. and P.M.\n* A.D. and B.C.\n* O.K.\n* i.e.\n* e.g.\n\nFor abbreviations like those, it is not hard to figure out the pattern and the corresponding regular expression.  Each of those abbreviations contains at least a pair of a letter (either uppercase or lowercase) and a period.  The regular expression is\n```python\n    r\"([a-zA-z]\\.)+\"\n```\nTo see the graphical representation of the regular expression, please click <a href=\"https://regexper.com/#(%5Ba-zA-z%5D%5C.)%2B\">here</a>. \nPut it into <font color=\"blue\">RegexpTokenizer</font>,", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tokenizer = RegexpTokenizer(r\"(?:[a-zA-Z]\\.)+\")\ntokenizer.tokenize(raw)"
        }, 
        {
            "source": "Observe that\n1. We introduced <font color=\"red\">(?: )</font> in the regular expression to avoid just selecting substrings that match the pattern. `(?: )` is a non-capturing version of regular parentheses. If the parentheses are used to specify the scope of the pattern, but not to select the matched material to be output, you have to use `(?: )`. To check out how `?:` affects the output, try to remove it and run the tokenizer again. You will get the following output\n```\n    ['e.', 'A.', 'l.', 'r.']\n```\nIt just returns the last substrings that match the pattern.\n2. The code also returned 'l.' and 'r.' that are part of 'survival.' and 'Mr.' \nThe period in 'survival.' marks the end of a sentence. \nIndeed, it is very challenging to deal with the period at the end of each sentence, as it can also be part of an abbreviation if the abbreviation appears at the end of a sentence.\nFor example, the following sentence ends with 'etc.'\n```\n    I need milk, eggs, bread, etc.\n```\n\nNext, let\u2019s further consider some more general abbreviations, like\n* Mr. and Mrs.\n* Dr.\n* st.\n* Wash. and Calif. (abbreviations for two states in U.S., Washington and California)\n\nIn those abbreviations, the period is always preceded two or more letters in English alphabet. Turn this pattern into a regular expression\n```\n    r\"[a-zA-z]{2,}\\.\"\n```", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tokenizer = RegexpTokenizer(r\"[a-zA-z]{2,}\\.\")\ntokenizer.tokenize(raw)"
        }, 
        {
            "source": "It is not surprising that the ouput contains \"survival.\" again. \nThe issue of working out which punctuation marks indicate the end of a setence will be discussed in section 2.5.\nLet's put all the cases together. \nThe regular expression can be generalised to\n```python\n    r\"([a-zA-Z]+\\.)+\"\n```\nwhich matches both acronyms and abbreviations like \"Dr.\"", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "As we mentioned early in this chapter, the issues of tokenization are language specific.\nThe language of the document to be tokenized should be known a priori.\nTake computer technology as an example.\nIt has introduced new types of character sequences that a tokenizer should probably treat as a single token, including email addresses, web URLs, IP addresses, etc. One solution is to simply ignore them by using a non-alphanumeric-based tokenizer. \nHowever, this comes the cost of losing the original meaning of those kinds of tokens. For instance, if an IP address, like \"172.19.197.106\", is tokenized into individual numbers, \"172\", \"19\", \"197\", and \"106\".\nIt is no longer an IP address, and these numbers can be anything.\nTo account for strings like\n* \"172.19.197.106\"\n* \"www.monash.edu.au\"\n\nyou can simply update our regular expression accounting for abbreviations to \n```python\n    (\\w+\\.?)+\n```\n\nTry it out on http://regexr.com/.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### 2.1.3 Currency and Percentages\n\nWhile analysing financial document, such as finance reports, a financial analyst might be interested in monetary numerals mentioned in the reports. One interesting research question in both finance and computer science is whether one can use finance reports to help predict the stock market prices. In this case, it would be good for a tokenizer to keep all the monetary numerals.\n\nCurrency is usually expressed in symbols and numerals (e.g., $10).\nThere are many different ways of writing about different currencies.\nFor example,\n* A three-letter currency abbreviations followed by figures, for example,\n```\n    AUD100, EUR500, CNY330 \n```\n\n* A letter or letters symbolising the country followed the, for example,\n```\n    A$100 (= AUD100), US$10 (= USD10), C$5 (= CAD5),\n```\n\n* A currency symbols ($, \u00a3, \u20ac, \u00a5, etc.) followed by figures, for examples\n```\n    \u00a3100.5, \u20ac30.0\n```\n\nWhile the number of digits in the integer part is more than three, commas are often inserted between every three digits, like\n```\n    AUD100, 000 \n```\nLet's construct a regular expression that can account for all the following monetary numerals\n```\n1. $10,000.00\n2. \u20ac10,000,000.00\n3. \u00a55.5555\n4. AUD100\n5. A$10.555\n```\nThe regular expression should looks like as follows (<a href=\"https://regexper.com/#(%3F%3A%5BA-Z%5D%7B1%2C3%7D)%3F%5B%5C%24\u00a3\u20ac\u00a5%5D%3F(%3F%3A%5Cd%7B1%2C3%7D%2C)*%5Cd%7B1%2C3%7D(%3F%3A%5C.%5Cd%2B)%3F\"> the graphical representation</a>):\n```python\n    r'''(?x)          \n        ([A-Z]{1,3})? # (1)\n        [\\$\u00a3\u20ac\u00a5]?      # (2)\n        (\\d{1,3},)*   # (3)\n        \\d{1,3}       # (4)\n        (?:\\.\\d+)?    # (5)\n    '''\n```\n\n(1) matches the start of monetary numerals, which consists of one or up to 3 uppercase letters that indicate a country symbol or a currency abbreviation.\n<br/>\n(2) together with (1), matches the start of monetary numerals, which consists of either only a currency symbol or a country symbol plus a currency symbol.\n<br/>\n(3) accounts for the integer part that contains more than three digits. It matches all digits in the integer part except for the last three digits.\n<br/>\n(4) matches the last three digits in the integer part.\n<br/>\n(5) matches the fractional part.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tokenizer = RegexpTokenizer(r\"(?:[A-Z]{1,3})?[\\$\u00a3\u20ac\u00a5]?(?:\\d{1,3},)*\\d{1,3}(?:\\.\\d+)?\")\ntokenizer.tokenize(raw)"
        }, 
        {
            "source": "Refer back to our example text \"raw\", can you find any issue rather than the percentage (35.5%)? The regular expression cannot handle \"AUD113.3m\", where the \"m\" indicates million. Without 'm', the number 'AUD113.3' loses its meaning in the original context. Therefore, you have seen that there might not be a regular expression that can handle all possible ways of representing currency.\n\nNow, we have constructed a regular expression for currencies, even though it is not perfect.\nNext, we move to working out the regular expression for percentages, things becomes quite easy.\nPercentages usually have the following forms\n* 23%\n* 23.23%\n* 23.2323%\n* 100.00%\n\nThe maximum number of digits in the integer part is 3, the minimun is 1, so the regular expression is '\\d{1,3}'.\nA percentage can have either one or no fractional part, which can be matched by '(\\.\\d+)?'.\nAdding % to the end, we have (<a href=\"https://regexper.com/#%5Cd%7B1%2C3%7D(%5C.%5Cd%2B)%25\">the graphical representation</a>)\n```python\n    r\"\\d{1,3}(\\.\\d+)%\"\n```", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tokenizer = RegexpTokenizer(r\"\\d{1,3}(?:\\.\\d+)?%\")\ntokenizer.tokenize(raw)"
        }, 
        {
            "source": "The above code should give you the only percentage in our example text. \nCompare the regular expression matching percentages with that matching currency,\nyou will find that the former is similar to the last bits of the latter, except for the percentage sign.\nBesides, there are other numerical and special expressions that\nwe can not easily handle with regular expressions. For example, these expressions include\nemail addresses, time, vehicle licence numbers, phone numbers, etc.\nIf you are interested in dealing with them, you could read the \u201cRegular Expressions Cookbook\u201d by Jan Goyvaerts and Steven Levithan. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Hyphens and Apostrophes \n\nIn English, hyphenation is used for various purposes. The hyphen can be used to form certain compound terms, including hyphenated compound nouns, verbs and adjectives. It can also be used for word division. There are many sources of hyphens in texts. Thus, should one count a sequence of letters with a hyphen as one word to two? Unfortunately, the answer seems to be sometimes one, sometimes two. \nFor example, if the hyphen is used to split up vowels in words, such as \"co-operate\", \"co-education\" and \"pre-process\", these words should be regarded as single token. In contrast, if the hyphen is used to group a couple of words together, for example, \"a state-of-the-art algorithm\" and \"a money-back guarantee\", these hyphenated words should be separated into individual words.\nTherefore, handling hyphenated words automatically is one of the most difficult tasks in pre-processing text data.\n\n\"**The Art of Tokenization**\" [3] categorizes different hyphens into three types:\n* **End-of-Line Hyphen**: In professionally printed material (like books, and newspapers), the hyphen is used to divide words between the end of one line and the beginning of the next in order to perform justification of text during typesetting. It seems to be easy to handle these kinds of hyphens by simply removing them and joining the parts of a word at the end of one line and the beginning of the next.\n* **Lexical Hyphen**: Words with a lexical hyphen are better to be treated as a single word. They are typically included in a dictionary. For example, words contains certain prefixes, like \"co-\", \"pre-\", \"multi-\", etc., and other words like \"so-called\", \"forty-two\"\n* **Sententially Determined Hyphenation**: This type of hyphen is often created dynamically. It includes, for example, nouns modified by an 'ed'-verb (e.g., \"text-based\" and \"hand-made\") and sequences of words used as a modifier in a noun group, as in \"the 50-cent-an-hour raise\". In these cases, we might want to treat those tokens joined by hyphens as individual words.\n\nThe use of hyphens in many such cases is extremely inconsistent, which further increase the complexity of dealing with hyphens in tokenization. People often resort to using either some heuristic rules or treating it as a machine learning problem. However, these go beyond our scope here. It is clear that handling hyphenation is much more complicated than one can expect. You should also be clear that there is no way of handling all the cases above.\n\nLet's assume that we are going to treat all strings of two words separated by a hyphen as a single token, how can we extract them from texts without breaking them into pieces.  In our example text, we are going to view \"ASX-listed\" as a single token. The pattern here is  a sequence of alphanumeric character plus \"-\" and plus another sequence of alphanumeric character.\nThe corresponding regular expressions should be \n```python\n    r\"\\w+-\\w\"\n```", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tokenizer = RegexpTokenizer(r\"\\w+-\\w+\")\ntokenizer.tokenize(raw)"
        }, 
        {
            "source": "Similar to hyphens, how to handle an apostrophe in tokenization is another complex question. The apostrophe in English is often used in two cases:\n* Contractions: a shortened version of a word or multiple words. \n    * don't (do not)\n    * she'll (she will)\n    * you're (you are)\n    * he's (he is or he has)\n    * you'd (you would)\n* Possessives: used to indicate ownership/possession with nouns.\n    * the cat's tail\n    * Einstein's theory\n    \nShould we treat a string containing apostrophes as a single word or two words?\nPerhaps, you might think we should separate English Contractions into two words, and regard possessives as a single word. \nHowever, distinguishing contractions from possessives is not easy.\nFor example, should \"cat's\" be \"cat has/is\" or the possessive case of cat.\nThus some processor in NLP splits the strings in either case into two words, while others do not.\nHere we again assume that we are going to retrieve all strings with an apostrophe as single words.\nThe regular expression is quite similar to the one for handling hyphens.\n```\n     r\"\\w+'\\w+\"\n```", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tokenizer = RegexpTokenizer(r\"\\w+'\\w+\")\ntokenizer.tokenize(raw)"
        }, 
        {
            "source": "Now let's generalise the `\\w+` to permit word-internal hyphens and apostrophes (<a href=\"https://regexper.com/#%5Cw%2B(%3F%3A%5B-'%5D%5Cw%2B)%3F\">the graphical representation</a>):\n```python\n    \\w+(?:[-']\\w+)? \n```\n\nYou have learnt some simple approaches for handling different issues in word tokenization, which turns out to be far more difficult than you might have expected. It is clear that different NLP and text mining tasks on different text corpora need different word tokenization strategies, as you must decide what counts as a word. Besides the `RegexpTokenizer`, NLTK implements a set of other word tokenizaton modules. Please refer to [its official webpage](http://www.nltk.org/api/nltk.tokenize.html) for more details.\nSo far that we have only considered well-written text, but there are other types of natural language texts, such the transcripts of speech corpora and some non-standard texts like tweets that provide their own additional challenges.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Case Normalization\nAfter word tokenization, you may find that words can contain either upper- or lowercase letters. \nFor example, you might have \"data\" and \"Data\" appearing in the same text.\nShould one treat them as two different words or as the same word?\nMost English texts are written in mixed case. \nIn other words, a text can contain both upper- and lowercase letters.\nCapitalization helps readers differentiate, for example, between nouns and proper nouns.\nIn many circumstances, however, an uppercase word should be treated no differently than in lower case appearing in a document, and even in a corpus.\nTherefore, a common strategy is to reduce all letters in a word to lower case.\nIt is very simple to do so.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tokens = [token.lower() for token in tokens]\ntokens"
        }, 
        {
            "source": "It is often a good idea to do case normalization. For example, with case normalization, you can match \"data wrangling\" with \"Data Wrangling\" in an information retrieval task. But for other tasks, like named entity recognition, one would better to keep capitalised words (e.g., pronouns) left as capitalised.\nPeople have tried some simple heuristics that just makes some token lowercase. \nHowever, there is a trade-off between getting capitalization right and simply using lowercase regardless of the correct case of words.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Removing Stop words\n[Stopwords](https://en.wikipedia.org/wiki/Stop_words) are words that are extremely common and carry little lexical content. For many NLP and text mining tasks, it is useful to remove stopwords in order to save storage space \nand speed up processing, and the process of removing these words is usually called \u201cstopping.\u201d \nAn example stopword list from NLTK is shown bellow:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "from nltk.corpus import stopwords\nstopwords_list = stopwords.words('english')\nstopwords_list"
        }, 
        {
            "source": "The above list contains 127 stopwords in total, which are often [function words](https://en.wikipedia.org/wiki/Function_word) in English, like articles (e.g., \"a\", \"the\", and \"an\"), \npronouns (e.g., \"he\", \"him\", and \"they\"), particles (e.g., \"well\", \"however\" and \"thus\"), etc.\nIt is easy to use NLTK's built-in stopword list to remove all the stopwords from a tokenised text.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "filtered_tokens = [token for token in tokens if token not in stopwords_list]\nfiltered_tokens"
        }, 
        {
            "source": "We have removed 13 stopwords. The number of tokens left is 28. \nTo check what stopwords have been excluded from the filtered list, you simply change `not in` to `in`.\n\nThere is no single universal list of stop words used by all NLP and text mining tools.\nDifferent stopword lists are available online. For example, the English stopword list \navailable at [Kevin Bouge's website](https://sites.google.com/site/kevinbouge/stopwords-lists) \nwhich contains 570 stopwords, a quite fine-grained stopword list. \nAt the same website, you can also download stopword lists for 27 languages other than English.\nPlease download the English stopwords list from Kevin Bourge's website, and save it into the folder where\nyou keep this IPython Notebook file. \nWe will try out the aforementioned stopword lists on the large\n[Reuters corpus](http://about.reuters.com/researchandstandards/corpus/). ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import wget\n\nlink_to_data = 'https://github.com/tulip-lab/mds/raw/master/Jupyter/data/stopwords_en.txt'\n\nDataSet = wget.download(link_to_data)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "!ls"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "import nltk\nreuters = nltk.corpus.reuters.words()\n\nstopwords_list_570 = []\nwith open('stopwords_en.txt') as f:\n    stopwords_list_570 = f.read().splitlines()"
        }, 
        {
            "source": "Remove stop words accroding to NLTK's built-in stopword list.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "filtered_reutuers = [w for w in reuters if w.lower() not in stopwords_list]\nlen(filtered_reutuers)*1.0/len(reuters)"
        }, 
        {
            "source": "Remove stop words according to the downloaded stop word list. (Note: the following script will run a couple of minutes due to data structure used in search.)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "filtered_reutuers = [w for w in reuters if w.lower() not in stopwords_list_570]\nlen(filtered_reutuers)*1.0/len(reuters)"
        }, 
        {
            "source": "Thus, with the help of these two stopword lists, we can filter about 36% and 34% of the words respectively.\nWe have significantly reduced the size of the Reuters corpus. \nThe question is: Have we lost lots of information due to removing stopwords? \nFor the large majority of NLP and text mining tasks and algorithms, stopwords usually appear to be of little value and have little impact on the final results, as the presence of stopwords in a text does not really help distinguishing it from other texts. \nIn contrast, text analysis tasks involving phrases are the exception because phrases lose their meaning if some of the words are removed. \nFor example, if the two stopwords in the phrase \"a bed of roses\" are removed, its original meaning in the context of IR will be lost.\n\nStopwords usually refer to the most common words in a language. \nThe general strategy for determining whether a word is a stopword or not is to compute its total number of appearances in a corpus. \nWe will cover more about removing common words other than stopwords while we further explore text data in next chapter.\nHere we would like to point out that failing to remove those common words could lead to skewed analysis results.\nFor example, while analysing emails we usually remove headers (e.g., \"Subject\", \"To\", and \"From\") and sometimes\na lengthy legal disclaimer that often appears in many corporate emails.\nFor short messages, a long disclaimer can overwhelm the actual text when performing any sort of text analysis.\nFor more discussion on stopping, please read [5] and watch an 8-mintue YouTube video on [Stop Words](https://www.youtube.com/watch?v=w36-U-ccajM).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Stemming and Lemmatization\n\nAnother question in text pre-processing is whether we want to keep word forms like \"educate\", \"educated\", \"educating\", \nand \"educates\" separate or to collapse them. Grouping such forms together and working in terms of their base form is \nusually known as stemming or lemmatization.\nTypically the stemming process includes the identification and removal of prefixes, suffixes, and pluralisation, \nand leaves you with a stem.\nLemmatization is a more advanced form of stemming that makes use of, for example, the context surrounding the words, \nan existing vocabulary, morphological analysis of words and other grammatical information (e.g., part-of-speech tags) \nto determine the basic or dictionary form of a word, which is known as the lemma.\nSee Wikipedia entries for [stemming](https://en.wikipedia.org/wiki/Stemming) \nand [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation).\n\nStemming and lemmatization are the basic text pre-processing methods for texts in languages like English, French, \nGerman, etc. \nIn English, nouns are inflected in the plural, verbs are inflected in the various tenses, and adjectives are \ninflected in the comparative/superlative. \nFor example,\n* watch &#8594; watches\n* party &#8594; parties\n* carry &#8594; carrying\n* love &#8594; loving\n* stop &#8594; stopped\n* wet &#8594; wetter\n* fat &#8594; fattest\n* die &#8594; dying\n* meet &#8594; meeting\n\nIt is not hard to find that they all follow some inflections rules. \nFor instance, to get the plural forms of nouns endings with consonant 'y', one often changes the ending \n'y' to 'ie' before adding 's'. \nIndeed most existing stemming algorithms make intensive use of this kind of rules.\n\nIn morphology, the derivation process creates a new word out of an existing one often by adding either \na prefix or a suffix. It brings considerable sematic changes to the word, often word class is changed, for example,\n* dark &#8594; darkness\n* agree &#8594; agreement\n* friend &#8594; friendship\n* derivation &#8594; derivational\n\nThe goal of stemming and lemmatization is to reduce either inflectional forms or derivational forms of \na word to a common base form. \nBefore we demonstrate the use of several state-of-the-art stemmers and lemmatizers implemented in NLTK, please read\n[4] and section 3.6 in [2].\nIf you are a visual learner, you could watch the YouTube video on \n[Stemming](https://www.youtube.com/watch?v=2s7f8mBwnko) from Prof. Dan Jurafsky.\n\nNLTK provides several famous stemmers interfaces, such as\n\n* Porter Stemmer, which is based on \n[The Porter Stemming Algorithm](http://tartarus.org/martin/PorterStemmer/)\n* Lancaster Stemmer, which is based on \n[The Lancaster Stemming Algorithm](http://delivery.acm.org/10.1145/110000/101310/p56-paice.pdf?ip=130.194.73.168&id=101310&acc=ACTIVE%20SERVICE&key=65D80644F295BC0D%2E54DA4E88E6052E5D%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&CFID=586402953&CFTOKEN=41173049&__acm__=1456460730_26a9cd5f8f70e5d3e101f527c10e1a82),\n* Snowball Stemmer, which is based on [the Snowball Stemming Algorithm](http://snowball.tartarus.org/)\n\nLet's try the three stemmers on the words listed above.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "words = ['watches', 'parties', 'carrying', 'loving', 'stopped', 'wetter', 'fattest', \n          'dying', 'darkness', 'agreement', 'friendship', 'derivational', 'denied',  'meeting']"
        }, 
        {
            "source": "Porter Stemming Algorithm is the one of the most common stemming algorithms.\nIt makes use of a series of heuristic replacement rules.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\n['{0} -> {1}'.format(w, stemmer.stem(w)) for w in words]"
        }, 
        {
            "source": "The Porter Stemmer works quite well on general cases, like 'watches' &#8594; 'watch' and 'darkness' &#8594; 'dark'.\nHowever, for some special cases, the Porter Stemmer might not work as expected, \nlike  'carrying'  &#8594; 'carri' and 'derivational' &#8594; 'deriv'. \nNote that a concept called \"list comprehension\" supported by Python is used here.\nIf you would like to know more about list comprehension, please click [here](http://www.secnetix.de/olli/Python/list_comprehensions.hawk).\n\nThe Lancaster Stemmer is much newer than the Porter Stemmer, published in 1990.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from nltk.stem import LancasterStemmer\nstemmer = LancasterStemmer()\n['{0} -> {1}'.format(w, stemmer.stem(w)) for w in words]"
        }, 
        {
            "source": "After comparing the output from the Lancaster Stemmer and that from the Porter Stemmer, you might think that\nthe Lancaster Stemmer could be a bit more aggressive than the Porter Stemmer, since it gets 'agreement' &#8594; 'agr' and 'derivational' &#8594; 'der'. \nAt the same time, it seems that the Lancaster Stemmer can handle words like 'parties' and 'carrying' quite well.\n\nNow let's try the Snowball Stemmer.\nThe version in NLTK is available in 15 languages.\nDifferent from the previous two stemmers, you need to specify which language the Snowball Stemmer will be applied to in its class constructor.\nIt works in a similar way to the Porter Stemmer.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from nltk.stem import SnowballStemmer\nstemmer = SnowballStemmer('english')\n['{0} -> {1}'.format(w, stemmer.stem(w)) for w in words]"
        }, 
        {
            "source": "A stemmer usually resorts to language-specific rules. \nDifferent stemmers implementing different rules and behave differently, \nas shown above.\nThe use of inflection and derivation is very complex in English.\nThere might not exist a set of rules that can cover all the cases.\nTherefore, the stemmers that you have played will always generate some out-of-vocabulary words.\n\nRather than using a stemmer, you can use a lemmatizer that utilises\nmore information about the language to accurately identify the lemma\nfor each word.\nAs pointed out in \"**Stemming and lemmatization**\", \n> Stemmers use language-specific rules, but they require less knowledge than a lemmatizer, which needs a complete vocabulary and morphological analysis to correctly lemmatize words\n\nThe WordNet lemmatizer implemented in NLTK is based on WordNet's built-in morphologic function, and returns the input word unchanged if it cannot be found in WordNet, which sounds more reasonable\nthan just chopping off prefixes and suffixes. In NLTK, you can use it in the following way:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n['{0} -> {1}'.format(w, lemmatizer.lemmatize(w)) for w in words]"
        }, 
        {
            "source": "It is a bit strange that the lemmatizer did nothing to nearly all the words, except for 'watches', 'parties'\nHowever, if we specify the POS tag of each word, what will happen?\nLet try a couple of words in our list.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "lemmatizer.lemmatize('dying', pos='v')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "lemmatizer.lemmatize('meeting', pos='v')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "lemmatizer.lemmatize('meeting', pos='n')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "lemmatizer.lemmatize('wetter', pos='a')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "lemmatizer.lemmatize('fattest', pos='a')"
        }, 
        {
            "source": "If we know the POS tags of the words, the WordNet Lemmatizer can accurately identify the corresponding lemmas.\nFor example, the word 'meeting' with different POS tag, the WordNet Lemmatizer gives you different lemmas.\nWithout giving the POS tags, it uses noun as default.\n\nBoth stemming and lemmatization can significantly reduce the number of words in a vocabulary.\nIn other words, the downstream text analysis tools can benefit from them by saving running time\nand memory space. In contrast, can stemming and lemmatization improve the performance\nof those tools? It is a quite arguable question. \nAs pointed out in [4], stemming and lemmatization can increase recall but harm precision in information\nretrieval. Researchers have also found that classifying English document tasks often do not gain \nfrom stemming and lemmatization.\nHowever, it might not be the case when we change our language to something rather than English, for example, German.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Sentence Segmentation\n\nSentence segmentation is also known as sentence boundary disambiguation or sentence boundary detection.\nThe following is the Wikipedia definition of sentence boundary disambiguation:\n>Sentence boundary disambiguation (SBD), also known as sentence breaking, is the problem in natural language processing of deciding where sentences begin and end. Often natural language processing tools require their input to be divided into sentences for a number of reasons. However sentence boundary identification is challenging because punctuation marks are often ambiguous. For example, a period may denote an abbreviation, decimal point, an ellipsis, or an email address - not the end of a sentence. About 47% of the periods in the Wall Street Journal corpus denote abbreviations. As well, question marks and exclamation marks may appear in embedded quotations, emoticons, computer code, and slang.\n\nSBD is one of the essential problems for many NLP tasks, like Parsing, Information Extraction, Machine Translation, and Document Summarizations. \nThe accuracy of the SBD system will directly affect the performance of these applications. \n\nSentences are the basic textual unit immediately above the word and phrase. \nSo what is a sentence? Is something ending with one of the following punctuations \".\", \"!\", \"?\"?\nDoes a period always indicate sentence boundaries?\nFor English texts, it is almost as easy as finding every occurrence of those punctuations.\nHowever, some periods occur as part of abbreviations, monetary numerals and percentages, as we \nhave discussed in sections 1.2 and 1.3. \nAlthough you can use a few heuristic rules to correctly\nidentify the majority of sentence boundaries, SBD is much more complex that we can expect,\nplease read section 4.2.4 of [6] and watch a Youtube video on [Sentence segmentation](https://class.coursera.org/nlp/lecture/5). \ndiscussing more advanced techniques for SBD goes beyond our scope.\nInstead, we will show you some sentence segmentation tools implemented in NLTK.\nPlease also note that there are other tools or packages containing a sentence tokenizer,\nfor example, Apache OpenNLP, Stanford NLP toolkit, and so on.\n\nThe NLTK's [Punkt Sentence Tokenizer](http://www.nltk.org/api/nltk.tokenize.html) was designed to split \ntext into sentences \"*by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences.*\u201d It contains a pre-trained sentence tokenizer for English.\nLet's test it out with a couple of examples extracted from the book, called \"Moby Dick\", on Project Gutenberg, by \nHerman Melville.\nFirst construct a pre-trained English sentence tokenizer,", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "import nltk.data\nsent_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
        }, 
        {
            "source": "Following the intruction on the official website of Punkt Sentence Tokenizer, we tokenize two snippets extracted\nfrom \"Moby Dick\":", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "text1 = '''And so it turned out; Mr. Hosea Hussey being from home, but leaving \nMrs. Hussey entirely competent to attend to all his affairs. Upon making known our desires \nfor a supper and a bed, Mrs. Hussey, postponing further scolding for the present, ushered us \ninto a little room, and seating us at a table spread with the relics of a recently concluded repast, \nturned round to us and said\u2014\"Clam or Cod?\"'''\nprint('\\n-----\\n'.join(sent_detector.tokenize(text1.strip())))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "text2 = '''A clam for supper? a cold clam; is THAT what you mean, Mrs. Hussey?\" says I, \"but\nthat's a rather cold and clammy reception in the winter time, ain't it, Mrs. Hussey?\"'''\nprint('\\n-----\\n'.join(sent_detector.tokenize(text2.strip())))"
        }, 
        {
            "source": "You can also use `sent_tokenize`, an instance of Punkt Sentence Tokenizer.\nThis instance has already been trained on and works well for many European languages.\n```python\n    from nltk.tokenize import sent_tokenize\n    sent_tokenize(text1)\n```\nYou should get similar outputs as above.\n\nComparing the two results we notice that the sentence tokenizer has troubles in recognizing abbreviations.\nIt got \"Mrs.\" right in the first snippet but not the second. Regarding this type of issues, please read a blog post on sentence tokenizer [7].\n* * *", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Additional Reading and Resources\n\n1. \"[Tokenization](http://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)\" \ud83d\udcd6 .\n2. \"[Processing Row Text](http://www.nltk.org/book_1ed/ch03.html)\", chapter 3 of\nof \"Natural Language Processing with Python\".\n3. \"[The Art of Tokenization](https://www.ibm.com/developerworks/community/blogs/nlp/entry/tokenization?lang=en)\": An IBM blog on tokenization. It gives a detailed discussion about word tokenization and its challenges \ud83d\udcd6 .\n4. \"[Stemming and lemmatization](http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\" \ud83d\udcd6 .\n5. \"[Dropping common terms: stop words](http://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html)\" \ud83d\udcd6 .\n6. \"[Corpus-Based Work](http://cognet.mit.edu.ezproxy.lib.monash.edu.au/system/cogfiles/books/9780262312134/pdfs/9780262312134_chap4.pdf)\", Chapter 4 of \"Foundations of statistical natural language processing\" by Christopher D. Manning \ud83d\udcd6 .\n7. \"[Testing out the NLTK sentence tokenizer](http://www.robincamille.com/2012-02-18-nltk-sentence-tokenizer/)\"\n1. \"[Accessing Text Corpora and Lexical Resources](http://www.nltk.org/book/ch02.html): Chapter 2 of \"Natural Language Processing with Python\" By Steven Bird, Ewan Kelin & Edward Loper \ud83d\udcd6 .\n2. \"[Corpus Readers](http://www.nltk.org/howto/corpus.html#tagged-corpora)\": An NLTK tutorial on accessing the contents of a diverse set of corpora.", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}