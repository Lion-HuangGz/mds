{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# FLIP(01):  Advanced Data Science\n**(Module 01: A Touch of Data Science)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n- If you found any issue/bug for this document, please submit an issue at [tulip-lab/mds](https://github.com/tulip-lab/mds/issues)\n\nPrepared by and for \"\n**Student Members** |\n2006-2019 [TULIP Lab](http://www.tulip.org.au)\n\n---\n\n\n# Session I - Data Parsing  \n\n## Content\n\n1. [Parsing CSV files](#CSV)\n    * Importing CSV data\n    * Manipulating the data\n\n2. [Parsing JSON files](#JSON)\n    * Acquiring JSON Data From the Internet\n    * Parsing the \"Melbourne_bike_share.json\" File\n\n3. [Parsing XML files](#XML)\n    * Loading and Exploring an XML file\n    * Extracting XML data into DataFrame\n\n4. [Parsing Excel files](#Excel)\n    * Introduction to Excel\n    * Parsing Excel with Pandas\n\n---\n\nDue to advances in technologies for data storage, data from various sources is always stored in different formats\nand file types. Some data formats store data in a way that can be easily handled by a machine, such as CSV, JSON, and XML.\nThose formats are usually referred to as machine-readable formats.\nIn contrast, some other data formats or file types store data in a way meant to be read by a human \nusing front-end desktop tools.\nThose formats or file types are often referred to as hard-to-parse formats.\nWe will use a series of examples to demonstrate how to extract data stored in \nboth machine-readable and hard-to-parse formats,\nand then store the extracted data in formats that can be easily adopted by the downstream data wranngling tasks.\nThis notebook will cover how to read the common machine-readable formats:\n* **CSV**: Comma Separated Values\n* **JSON**: JavaScript Object Notation\n\nIn most cases, the two formats togeather with XML are the best available resource while you are scraping data from\nthe web or requesting data directly from an organization or agency. \nThey are more easily used and ingested by programming languages, like Python.\nOur suggestion is that you should try your best to get data in these formats, before you start looking\ninto other formats that might be hard to parse, like PDFs.\n\nThere are many ways of reading and storing data in those formats, \nwhich depends on the programming language you use.\nHere we are going to focus on Python.\nSearching the Internet, you will find there are a lot of online tutorials on handling data stored in different\ndata formats with Python.\nWe suggest the following:\n* \"*Data Loading, Storage, and File Formats*\", Chapter 6 of \"**Python for Data Analysis**\": This chapter covers reading files in a variety of formats, loading data from databases and interacting with Internet via APIs. Please read pages 155-166, and download and run the Python scripts from [the author's github site](https://github.com/pydata/pydata-book). \ud83d\udcd6\n\nThe dataset used in this chapter was downloaded from\n[data.gov.au](https://data.melbourne.vic.gov.au/Transport-Movement/Melbourne-bike-share/tdvh-n9dv). \nIt is available in the following formats: CSV, JSON, XML, RDF, etc.\nThe first two formats are used, i.e., the following two files\n* Melbourne_bike_share.csv\n* Melbourne_bike_share.json\n\nIn the following sections, you will learn how to scrape data from the two \nexample files, and store the extracted data into Pandas DataFrame. \n\n### Example scenario\nAssume that you are going to analyze and predict bicycle hubway station status to answer the following questions:\n* What do usage patterns look like with respect to specific stations and how that translates to imbalances in the system?\n* Can we integrate these explanatory variables and these usage patterns into a predictive algorithm that would predict empty and full stations in the near future?\n* What form should that algorithm take?\n* How do environmental variables affect the future state of Hubway stations?\n\nSee <a href=\"http://cs109hubway.github.io/classp/\"><font color=\"red\">Predicting Hubway Stations status in Boston</font></a> for more discussion.\n\nThe first step we have to do is to acquire the hub station data and as well as weather data. Here, for demonstration purpose, we use the Melbourne bike share data published by the government. The files have been downloaded and come along with this notebook.\n\n* * *", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id = \"CSV\"></a>\n\n## <span style=\"color:#0b486b\">1. Parsing CSV file</span>\n\nA CSV is a Comma Separated Values file, which allows data to be saved in a tabular format.\nEach row of the file is a data record; each column is a field (or an attribute).\nEach data record consists of one or more fields, separated by commas.\nAs one of the most popular file formats,\nit is supported by any spreadsheet programs, such as \nMicrosoft Excel, Open Office Calc, and Google Spreadsheets,\nBecause of its simplicity,\nit differs from other spreadsheet file types, such as Excel, in that one can only store a single sheet in a file. \nIt cannot be used to store cell, columns or row styling, figures and formulas.\nTo make our CSV file, i.e., Melbourne_bike_share.csv, easier to view here, \na sample of the data with trimmed down records is shown below.\nYou should see something similar to this when you open the excel file in your text editor,\n\n![CSV](https://github.com/tulip-lab/sit742/raw/master/Jupyter/image/csv1.png \"CSV File\")\n\n\nNote that tabs can also be used to separate values of different fields.\nThis type of files is usually called TSV, Tab Separated Values. \nSometimes TSVs get classified as CSVs.\nThe only difference between CSVs and TSVs is the delimiter.\nEssentially, the two types of files will act the same in Python and most of the other\nprogramming languages. \nIt is worth mentioning that they often take the form of a text file containing information \nseparated by commas.\nThis section will show you how to use Pandas \n[read_csv()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) function to\nload our CSV file, and how to tidy the loaded data a bit.\nBefore we start importing our CSV file, it might be good for you to read [Pandas tutorial\non reading CSV files](http://pandas.pydata.org/pandas-docs/stable/io.html#io-read-csv-table) \ud83d\udcd6.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We can have different ways to inspect your data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!pip install wget"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import wget\n\nlink_to_data = 'https://github.com/tulip-lab/mds/raw/master/Jupyter/data/Melbourne_bike_share.csv'\n\nDataSet = wget.download(link_to_data)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!ls"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with open(\"Melbourne_bike_share.csv\", 'r') as f:\n    for line in f.readlines()[:10]:\n        print (line)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with open(\"Melbourne_bike_share.csv\", 'r') as f:\n    for line in f.readlines()[-10:]:\n        print (line)"
        }, 
        {
            "source": "### Importing CSV data\nImporting CSV files with Pandas <a href='http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html'><font color = \"blue\">read_csv()</font></a> function and converting the data into a form Python can understand \nis simple. \nIt only takes a couple of lines of code.\nThe imported data will be stored in Pandas DataFrame.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import pandas as pd\ncsvdf = pd.read_csv(\"Melbourne_bike_share.csv\")\ntype(csvdf)"
        }, 
        {
            "source": "Or you can use the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html\"><font color='blue'>read_table()</font></a> function", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "csvdf_1 = pd.read_table(\"Melbourne_bike_share.csv\", sep=\",\")\ntype(csvdf_1)"
        }, 
        {
            "source": "Now, the data should be loaded into Python. \nLet's have a look at the first 5 records in the dataset.\nThere are a coupe of ways to retrieve these records.\nFor example, you can use \n* <font color='blue'>csvdf.head(n = 5)</font>: It will return first `n` rows in a DataFrame, n = 5 by default.\n* <font color='blue'>csvdf[:5]</font>: It uses the slicing method to retrieve the first 5 rows\n\nRefer to \"[Indexing and Selecting Data](http://pandas.pydata.org/pandas-docs/stable/indexing.html)\"\nfor how to slice, dice, and generally get and set subsets of pandas objects.\nHere, we use the `head` function.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "csvdf.head()\n#csvdf.loc[:4]\n#csvdf[:5]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "csvdf.tail()"
        }, 
        {
            "source": "Currently, the row indices are integers automatically generated by Pandas.\nSuppose you want to set IDs as row indices and delete the ID column.\nResetting the row indices can be easily done with the following DataFrame function\n```python\n    DataFrame.set_index(keys, drop=True, append=False, inplace=False, verify_integrity=False)\n```\nSee its [API webpage](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html) \nfor the detailed usage.\nThe keys are going to be the IDs in the first column. \nBy setting `inplace = True`, the corresponding change is done inplace and won't return a new DataFrame object.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#len(csvdf.ID.unique())"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "csvdf.set_index(csvdf.ID, inplace = True)\ncsvdf.head()"
        }, 
        {
            "source": "To remove the ID column that is now redundant, you use DataFrame `drop` function and set `inplace = True`\n```python\n    DataFrame.drop(labels, axis=0, level=None, inplace=False, errors='raise')\n```", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "csvdf.drop('ID', 1, inplace = True)\ncsvdf.head()"
        }, 
        {
            "source": "Instead of using the above method of setting row indices to IDs, you can specify which column to \nbe used as row indices while reading the CSV file. See the API reference page for\n[pandas.read_csv](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html).\nTo do so, you can use the <font color='blue'>index_col</font> argument of <font color='blue'>read_csv()</font>.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "csvdf = pd.read_csv(\"Melbourne_bike_share.csv\", index_col = \"ID\")\ncsvdf.head()"
        }, 
        {
            "source": "Similarly, with the <font color='blue'>read_table()</font> function, you can also set the value of <font color='blue'> index_col</font> to \"ID\".", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Manipulating the Data\n\nSo far, you have learned a little bit about the Melbourne_bike_share data.\nLet's further process the data by splitting the coordinates into latitude and longitude.\nFirst figure out what type of data we're dealing with, i.e., the data type of the \"Coordinates\" column.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "type(csvdf['Coordinates']) \n# type(csvdf.Coordinates)"
        }, 
        {
            "source": "The data type of this column is Pandas Series, i.e., \na one-dimensional labeled array capable of holding any data type.\nNext, in order to split the coordinates, you should know the data type of those coordinates. Are they strings?\nLet's check them by printing the first element in the Series and its type.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print (csvdf['Coordinates'].iloc[0])\ntype(csvdf['Coordinates'].iloc[0]) "
        }, 
        {
            "source": "Those coordinates are indeed strings. Thus, to extract both latitude and longitude, you \ncan either use regular expressions introduced in the previous chapter or common string operations.\n\nTo use regular expressions, the key is figuring out the patterns of characters. Then\naccording to those patterns, you formulate your regular expressions.\nLooking at the first couple of coordinates in the Series object, i.e.:\n```\n    (-37.814022, 144.939521)\n    (-37.817523, 144.967814)\n    (-37.84782, 144.948196)\n```\nYou will find that latitudes are always negative real values, and longitudes are positive real values.\nThat is because Australia lies between latitudes 9\u00b0 and 44\u00b0S, and longitudes 112\u00b0 and 154\u00b0E.\nThe regular expression is\n```\n    r\"-?\\d+\\.?\\d*\"\n```\n\n![RegEx](https://github.com/tulip-lab/sit742/raw/master/Jupyter/image/regex1.jpg \"RegEx\")\n\nIt contains four parts\n* \"-?\": optionally matches a single '-'.\n* \"\\d+\": matches one or more digits.\n* \"\\\\.?\": optionally matches a single dot.\n* \"\\d*\": matches zero or more digits.\n\nThe following code extracts all real values matching this regular expression.\nThe <font color=\"blue\">re.findall()</font> returns all matched values in a Python list.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import re\nstr1 = csvdf['Coordinates'].iloc[0] # csvdf.Coordinates\nre.findall(r\"-?\\d+\\.?\\d*\", str1)"
        }, 
        {
            "source": "Using common string operations might be simpler than using regular expressions. \n<font color=\"blue\">str.split()</font> is the function used here to extract both latitudes and longitudes.\nHowever, you should choose a proper delimiter to split a string.\nFirst, split the string by ',':", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "s = csvdf['Coordinates'].iloc[1].split(', ') # assuming they're all '(x, y)'\nprint ('lat = ', s[0], ' long = ', s[1])"
        }, 
        {
            "source": "The printout shows that the latitude contains '(', and the longitude contains ')'.\nYou should consider removing both the left and the right parentheses. \nOf course, the `split` function can be used again. \nNote that the goal here is to remove the leading and trailing parentheses.\nPython string class provides two functions to do the two operations,\nwhich are:\n* <font color=\"blue\">string.lstrip()</font>: returns a copy of the string with leading characters removed\n* <font color=\"blue\">string.rstrip()</font>: returns a copy of the string with trailing characters removed.\n\nLet's try the two functions.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print (s[0].lstrip('('))\nprint (s[1].rstrip(')'))"
        }, 
        {
            "source": "The latitude and longitude in the first coordinate have been successfully extracted.\nNext, we are going to apply the extracting process to every coordinate in the DataFrame.\nThere are multiple ways of doing that. \nThe most straightforward way is to write a FOR loop to iterate over all the coordinates,\nand apply the above scripts to each individual coordinate. \nTwo Pandas Series can be then used to store latitudes and longitudes.\nHowever, we are going to show you how to use some advanced Python programming functionality.\n\nPandas Series class implements an [`apply()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.apply.html) method that applies a given function\nto all values in a Series object, and returns a new one.\nPlease note that this function can only works on single values. \nTo apply <font color=\"blue\">str.split()</font> to every coordinate and\nget latitudes and longitudes, you can use the following two lines of code:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "csvdf['lat'] = csvdf['Coordinates'].apply(lambda x: x.split(', ')[0])\ncsvdf['lon'] = csvdf['Coordinates'].apply(lambda x: x.split(', ')[1])\ncsvdf.head()"
        }, 
        {
            "source": "The first line extracts all the latitudes and store them in a column in our DataFrame.\nThe second line extracts all the longitudes.\nYou might wonder what \"lambda\" is in the code. \nIt is a Python keyword used to construct small anonymous functions at runtime. (See [Section 4.7.5. Lambda Expressions](https://docs.python.org/2/tutorial/controlflow.html) \ud83d\udcd6 )\nYou can use a similar approach to remove the heading and trailing parentheses.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "csvdf['lat'] = csvdf['lat'].apply(lambda x: x.lstrip('('))\ncsvdf['lon'] = csvdf['lon'].apply(lambda x: x.rstrip(')'))\ncsvdf.drop('Coordinates', 1, inplace = True)\ncsvdf.head()"
        }, 
        {
            "source": "So far, we have split the \"Coordinates\" column into two columns, i.e., \"lat\" and 'lon' in the DataFrame,\nand dumped the \"Coordinates\" column.\nThe last step is to infer better type for object columns. \nAll the numerical values and dates are encoded as strings in the current DataFrame.\nWe would like to convert those values to types that they are supposed to have.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "csvdf = csvdf.convert_objects(convert_numeric = True) \ncsvdf.dtypes"
        }, 
        {
            "source": "However, dates are still strings, which means the `convert_object` function cannot convert data strings to datatime\nobject.\nHere you need to force them to be converted to datatime object with [`pd.to_datetime`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "csvdf['UploadDate'] = pd.to_datetime(csvdf['UploadDate'])\nprint (csvdf.dtypes)\ncsvdf"
        }, 
        {
            "source": "Finally, you have loaded the given CSV file into Python with Pandas. \nYou have also tidied the data a bit by getting latitudes and longitudes out\nfrom the strings.\n\nBesides `read_csv`, there are other parsing functions in pandas for \nreading tabular data as a DataFrame object. They include\n* [`read_table`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html): Reads general delimited file into DataFrame. The default delimiter is '\\t'.\n* [`read_fwf`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_fwf.html): Reads a table of fixed-width formatted lines into DataFrame.\n* [`read_clipboard`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_clipboard.html): Reads text from clipboard and passes to read_table. See read_table for the full argument list.\n* * *", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id = \"JSON\"></a>\n\n## <span style=\"color:#0b486b\">2. Parsing JSON files</span>\n\nJSON (JavaScript Object Notation) is one of the most commonly used formats \nfor transferring data between web services and other applications via HTTP requests.\nNowadays, many sites have JSON-enabled APIs and \nJSON is quickly becoming the encoding protocol of choice.\nAs a light weighted data-interchange format inspired by JavaScript, \nit is clean, easy to read, and easy to parse.\nHere is a simple example adapted from [Wikipedia page on JSON](https://en.wikipedia.org/wiki/JSON)\n```\n[\n{\n  \"firstName\": \"John\",\n  \"lastName\": \"Smith\",\n  \"age\": 25,\n  \"address\": {\n    \"streetAddress\": \"21 2nd Street\",\n    \"city\": \"New York\",\n    \"state\": \"NY\",\n    \"postalCode\": \"10021\"\n   }\n}\n]\n\n```\n\nFrom the above example, you will see that each data record looks like a [Python dictionary](https://docs.python.org/2/tutorial/datastructures.html#dictionaries). \nA JSON file usually contains a list of dictionaries, which is defined by '[' and ']'.\nIn each of those dictionaries,\nthere is a key-value pair for each row and the key and value are separated by a colon.\nDifferent key-value pairs are separated by commas.\nNote that a value can also be a dictionary, see \"address\" in the example.\nThe basic types are object, array, value, string and number.\nIf you would like to know more about JSON, please refer to \n* [Introducing to JSON](http://www.json.org/): the JSON org website gives a very good diagrammatic explanation \nof JSON \ud83d\udcd6.\n* [Introduction to JSON](https://www.youtube.com/watch?v=WWa0cg_xMC8): a 15-minutes Youtube video on JSON, recommended for visual learners.\n\n(Of course, you can also go and find your own materials on JSON by searching the Internet.)\n\nIn the rest of this section, we will start from an simple example, walking through steps of acquiring JSON Data from Google Maps Elevation API and normalizing those data into a flat table. Then, we revisit the dataset mentioned in the previous section (except that it is now in JSON format), parsing the data and store them in a Pandas DataFrame object.\nBefore we start, it might be good for you to view one of the following tutorials on parsing JSON files:\n* [Working with JSON data](http://wwwlyndacom.ezproxy.lib.monash.edu.au/Python-tutorials/Working-JSON-data/122467/142575-4.html): A Lynda tutorial on parsing JSON data. You need a Monash account to access this website.\n[here](http://resources.lib.monash.edu.au/eresources/lynda-guide.pdf) is the lynda settup guide.\n* A [Youtube video](https://www.youtube.com/watch?v=9Xt2e9x4xwQ ) on extracting data from JSON files (**optional**).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Acquiring JSON Data From the Internet\nThis section will start with showing you how to acquire a small chunk of JSON data\nfrom Internet via HTTP requests and load it into Python with `json` library. \nThe example we used is inspired by the question asked in [Stack Overflow](http://stackoverflow.com/questions/21104592/json-to-pandas-dataframe).\nIn the example, the goal is to extract elevation data from a \n[Google Maps Elevation API](https://developers.google.com/maps/web-services/overview) along\na path specified by latitude and longitude, and convert the JSON data\ninto a Pandas DataFrame object, which could look similar to (but the actual values might vary!)\n\n||elevation|location.lat|location.lng|resolution|\n|------|------|------|------|------|\n|0|243.346268|42.974049|-81.205203|19.087904|\n|1|244.131866|42.974298|-81.195755|19.087904|\n\n\nThe first step is to make a HTTP request to get the data from the Google Maps API.\nHere we are going to use [`urllib2`](https://docs.python.org/2/library/urllib2.html) library.\nIt defines a set of functions and classes that help in opening URLs.\n\nIn order to run the following code, please following the instruction on https://developers.google.com/maps/documentation/elevation/start\nto request a API key.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "locations = \"42.974049,-81.205203|42.974298,-81.195755\"\ntry:\n    from urllib2 import Request, urlopen # for python 2\nexcept ImportError:\n    from urllib.request import urlopen, Request # for python 3\n\napi_key = \"YOUR API-KEY\" #use your own API key her\nrequest = Request(\"https://maps.googleapis.com/maps/api/elevation/json?locations=\"+locations+\"&key=\"+api_key)\nprint(request)\nresponse = urlopen(request)\nelevations = response.read()\n#elevations.splitlines()"
        }, 
        {
            "source": "#### If you don't have the API, used the pre-dumped json data by enabling the following code", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import wget\n\nlink_to_data = 'https://github.com/tulip-lab/mds/raw/master/Jupyter/data/elevations.json'\n\nDataSet = wget.download(link_to_data)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!ls"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import json\nwith open(\"elevations.json\", \"r\") as f:\n    elevations=json.load(f)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# elevations = elevations.decode('UTF-8')"
        }, 
        {
            "source": "In the above code, we have:\n1. Imports Request class and the <font color=\"blue\">urlopen() </font> function from `urllibs` module.\n2. Defines a path with the coordinates of the start and end points\n3. Creates a URL Request object. Note that you can change the output format by replacing '/json' with '/xml'.\n4. Opens the URL, and returns a file-like object.\n5. Reads data returned from the HTTP request.\n\nThe returned data is actually stored in a string. \nYou can check it out using Python's built-in function `type`, \n```python\n    type(elevations)\n```\nWhat does the data look like?\nIn stead of printing the data in one single string, one can use\n```python\n    elevations.splitlines()\n```\nto print the data as a list of lines in the string, breaking\nat line boundaries, i.e., '\\n'. \nThe printout you get should look like\n```\n['{',\n '   \"results\" : [',\n '      {',\n '         \"elevation\" : 243.3462677001953,',\n '         \"location\" : {',\n '            \"lat\" : 42.974049,',\n '            \"lng\" : -81.205203',\n '         },',\n '         \"resolution\" : 19.08790397644043',\n '      },',\n '      {',\n '         \"elevation\" : 244.1318664550781,',\n '         \"location\" : {',\n '            \"lat\" : 42.974298,',\n '            \"lng\" : -81.19575500000001',\n '         },',\n '         \"resolution\" : 19.08790397644043',\n '      }',\n '   ],',\n '   \"status\" : \"OK\"',\n '}']\n```\nIt is easy to dump the data into a JSON file, which just takes three lines of code:\n```python\n    import json\n    with open(\"data/elevations.json\", \"w\") as outfile:\n         json.dump(elevations, outfile)\n```\n\nTo read the acquired JSON data, you can use the `json` module as follows:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import json\ndata = json.loads(elevations)\nprint (type(data))\ndata"
        }, 
        {
            "source": "It loads the data into a Python dictionary.\nThe data we want is stored in the first entry.\nThe value of this entry is a list of two dictionaries, each of which corresponds to a record.\nsee [JSON encoder and decoder](https://docs.python.org/2/library/json.html) for more on reading\nJSON files.\n\nAs mentioned earlier in this section, \nwe will convert the JSON data into Pandas DataFrame.\nTherefore, Pandas functions on reading JSON are to be used.\nIf you would like to know about those functions, you can read Pandas tutorial on [Reading JSON](http://pandas.pydata.org/pandas-docs/stable/io.html#io-json-reader) (**optional**).\nLet's first try the <font color=\"blue\">read_json()</font> function.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Unfortunately, the DataFrame returned by `read_json` is not the one we want.\nYou might wonder why the `read_json` function did not return the DataFrame we want.\nThere is a straight forward answer.\nLet's try to build a DataFrame from `data` returned by \n```\n    data = json.loads(elevations)\n```\nWhat do you get?", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import pandas as pd"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "pd.DataFrame(data)"
        }, 
        {
            "source": "You have got a DataFrame that is exactly the same as the one returned by `read_json`.\nThis is due to Pandas' way of constructing a DataFrame from a dictionary. \nSee [Intro to Data Structures](http://pandas.pydata.org/pandas-docs/stable/dsintro.html)\nfor constructing a DataFrame from a dictionary\nand \"Object Creation\" in [10 Mintues to Pandas](http://pandas.pydata.org/pandas-docs/stable/10min.html) \ud83d\udcd6.\nIt is not hard to figure out that dictionary keys \nare used as column \nlabels, and values of whatever data types are put as column values.\n\nWhat we want is to flatten out JSON object into a flat table.\nFortunately, Pandas provides a JSON normalization function [(<font color=\"blue\">json_normalize()</font>)](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html)\nthat takes a dict or list of dicts and normalize semi-structured data into a flat table. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pandas.io.json import json_normalize\njson_normalize(data['results'])"
        }, 
        {
            "source": "Eventually, the <font color=\"blue\">json_normalize()</font> function returns the DataFrame we want.\nHowever flattening objects with embedded arrays/lists is not as trivial.\nSee [Flattening JSON objects in Python](https://gist.github.com/amirziai/2808d06f59a38138fa2d)\nfor more information.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Parsing the \"Melbourne_bike_share.json\"  File\nNow that you have learned how to use `json` module and Pandas together to parse a simple JSON file.\nIn this section we will walk you through the process of extracting bike hub station statistical data from \"Melbourne_bike_share.json\". Then produce the same DataFrame as the one in Section 1.\n\nRemember that the first step is always to glance through the JSON file with your favorite editor.\nBelow is the first 20 lines from our JSON file.\n\n![JSON](https://github.com/tulip-lab/sit742/raw/master/Jupyter/image/json20.png \"JSON File\")\n\nThis JSON file is much more complex that the one used in the previous section\nIt might take a bit of time to figure out that this file is a dictionary of \ntwo large dictionaries, one with key \"meta\", and another with \"data\".\nThe \"meta\" dictionary contains all the meta information, including column names.\nThe \"data\" dictionary actually contains the data we want.\nIn the following subsection, we will show you how to extract records from the \"data\"\ndictionary, while leaving the task of extracting column labels from the \"meta\" dictionary as an exercise.\nSimilarly, our JSON data can be read into Python as follows.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import wget\n\nlink_to_data = 'https://github.com/tulip-lab/mds/raw/master/Jupyter/data/Melbourne_bike_share.json'\n\nDataSet = wget.download(link_to_data)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import json\nwith open(\"Melbourne_bike_share.json\") as json_file:\n    json_data = json.load(json_file)\nprint (type(json_data))\njson_data['meta']['view']"
        }, 
        {
            "source": "The loaded JSON data has been saved in a Python dictionary with two entries, one for \"data\" and another for \"meta\".\nUsing `json_normalize`, you can flatten the \"data\" dictionary into a table and save it in a DataFrame.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "df = json_normalize(json_data,'data')\ndf.head()"
        }, 
        {
            "source": "We seem to have a lot of extra columns.\nThe data we want starts at column 8.\nTherefore, dump all the irrelevant preceding columns.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "try:\n    df.drop(xrange(8), axis=1, inplace=True)\nexcept:\n    df.drop(range(8), axis=1, inplace=True)\n\ndf.head()"
        }, 
        {
            "source": "Renaming all the columns with the field names given by the CSV file. \nYou can programmatically extract field names from the \"meta\" dictionary.\nWe will leave it for you to do as an exercise.\nSimilar to parsing CSV file, IDs are unique and can be set to row indices. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "df.columns = ['id','featurename','terminalname','nbbikes','nbemptydoc','uploaddate','coordinates']\ndf.set_index(df.id, inplace= True)\ndf.drop('id', 1, inplace = True)\ndf.head()"
        }, 
        {
            "source": "What's in the last two columns?\n\"uploaddate\" is supposed to have a standard datetime format in the column,\nand coordinates should be pairs of latitude and longitude.\nBoth of them should be real numbers.\nAt the moment, a datetime is encoded as a 64-digit integer (i.e., datetimes in milliseconds since epoch),\nand a coordinate is a Python list as\n```python\n [u'{\"address\":\"\",\"city\":\"\",\"state\":\"\",\"zip\":\"\"}',\n u'-37.814022',\n u'144.939521',\n None,\n False]\n```\nLet's first convert those integers into standard datetime.\nThe following Python code converts \none of these integers into a standard datetime using Python\n[`datatime`](https://docs.python.org/2/library/datetime.html) module:\n```python\n    import datatime\n    date = datetime.datetime.fromtimestamp(df.iloc[0,4])\n    print data\n```\nThe output is \n```\n    2016-01-28 23:45:05\n```\nSimilar to the way of splitting coordinates in Section 2.1, \none can use `pandas.Series.apply` to invoke  `datetime.datetime.fromtimestamp`\non each individual integer in the column. \nPlease try this method by yourself.\n\nInstead, we will show you a pandas specific way of converting \ntimestamp values in milliseconds into standard datetime.\nHere we use Pandas [`to_datetime`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html)\nfunction.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df['uploaddate'] = pd.to_datetime(df['uploaddate'], unit='s')\ndf.head()"
        }, 
        {
            "source": "Note that the unit argument must be explicitly specified. It can take values on (D,s,ms,us,ns).\nWithout specifying its value, `1453985105`, for example, will be converted to some strange date as\n```\n    Timestamp('1970-01-01 00:00:01.453985105')\n```\nYou can compare the converted dates with those in the DataFrame constructed from our CSV file.\nFor example,", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print (csvdf.iloc[0,4]) # the csv date\nprint (df.iloc[0,4]) "
        }, 
        {
            "source": "The difference is due to that two files were downloaded one after another.\nHowever, the time format is the same.\n\nThe last step is to extract latitudes and longitudes into two columns.\nEach coordinate in the last column of the DataFrame is a Python list.\nThe second and the third entries are latitude and longitude respectively.\nIt is very easy to get the two entries into a list.\nWe will apply the following anonymous function to all the coordinates one after another\n```python\n    lambda col: col[i]\n```\nwhere i = 1 or 2. While i = 1, it returns latitudes; i = 2, it returns longitudes.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df['lat'] = df['coordinates'].apply(lambda col: col[1]) # arrrrgh\ndf['lon'] = df['coordinates'].apply(lambda col: col[2])\ndf.head()"
        }, 
        {
            "source": "Now, dump the \"coordinates\" columns and change data type of each column.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [], 
            "source": "df.drop('coordinates', 1, inplace = True)\ndf = df.convert_objects(convert_numeric=True) \ndf"
        }, 
        {
            "source": "<a id = \"XML\"></a>\n\n## <span style=\"color:#0b486b\">3. Parsing XML Files</span>\n\n\n[XML](https://www.w3.org/XML/), Extensible Markup Language, is a markup language much like HTML.\nIt is a simple and flexible data format that defines a set of rules for encoding documents in a way that \nis both human and machine readable. As a self-descriptive markup language, XML plays an important role in many information systems. It stores data in plain text format, which provides a platform-independent way of storing, transporting, and sharing data. In this chapter we are going to learn how to parse and extract data from XML files with Python.\n\n\nFirst and foremost, you will need to have some basic understanding about XML.\nThere are a lot of good introductory materials freely available online. \nWe suggest the following two sections of Chapter 12 in \"**Dive Into Python 3**\":\n* [12.2 A 5-Minute Crash Course in XML](http://www.diveintopython3.net/xml.html#xml-intro) \ud83d\udcd6\n* [12.3 The Structure Of An Atom Feed](http://www.diveintopython3.net/xml.html#xml-structure) \ud83d\udcd6\n\nIf you are quite familiar with XML, you can skip the above materials and jump directly into the parsing sections.\n\nXML files are not as easy as the CSV or JSON files to preview and understand.\nThe data we are going to parse is the XML version for the \"Melbourne bike share\" dataset downloaded from\n[data.gov.au](https://data.melbourne.vic.gov.au/Transport-Movement/Melbourne-bike-share/tdvh-n9dv).\n\nLet's first open the file in your favorite editor to preview it. Note that it is always necessary to inspect the file before we parse it, as the inspection can give an idea of what the format of the file is, what information it stores, etc. If you scroll through the opened file, you will find that the data has been encompassed in XML syntax, using things called tags. The following figure shows a snippet of the data.\n\n![XML](https://github.com/tulip-lab/sit742/raw/master/Jupyter/image/xml_example.png \"XML File\")\n\n\nAfter inspecting the file, you should find that data values can be stored in two places in an XML file, which are:\n* in between two tags, for example, \n    ```html\n        <featurename>Harbour Town - Docklands Dve - Docklands</featurename>\n    ```\n    where the value is \"Harbour Town - Docklands Dve - Docklands\" for the <featurename> tag.\n* as an attribute of a tag, for example\n    ```html\n        <coordinates human_address=\"{&quot;address&quot;:&quot;&quot;,&quot;city&quot;:&quot;&quot;\n        ,&quot;state&quot;:&quot;&quot;,&quot;zip&quot;:&quot;&quot;}\" \n        latitude=\"-37.814022\" longitude=\"144.939521\" needs_recoding=\"false\"/>\n    ```\n    where the value of latitude is -37.814022 and longitude is 144.939521. \n\nThe attributes in XML store rich information about a specific tag.\nComparing XML with JSON, you will find that the XML tags and attributes hold data in \na similar way to the JSON keys. \nThe advantage of XML is that each tag in XML can hold more than one attribute, and\nmore values can be stored in one node. See the \"coordinate\" tag above.\n\nNow, how can we extract data stored either in between tags or as attributes?\nThe goal is to parse the XML file, extract relevant information, and store the information in Pandas DataFrame that looks like\n\n![XML](https://github.com/tulip-lab/sit742/raw/master/Jupyter/image/parsed_xml.png \"XML File\")\n\n\nIn the following sections, we will demonstrate the process of loading and exploring a XML file, extracting\ndata from the XML file and storing the data in Pandas DataFrame.\n* * * ", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "### Loading and Exploring an XML file\n\nPython can parse XML files in many ways.\nYou can find several Python libraries for parsing XML from \n[\" XML Processing Modules\"](https://docs.python.org/2/library/xml.html).\nHere we will show you how to use the following Python libraries\nto parse our XML file.\n* ElementTree\n* lxml\n* beautifulsoup\n\nThere are a couple of good materials worth reading\n* The office ElementTree [API](https://docs.python.org/2/library/xml.etree.elementtree.html#module-xml.etree.ElementTree) documentation, which provides not only the API reference but also a short tutorial on using ElementTree. \ud83d\udcd6\n* [Parsing XML](http://www.diveintopython3.net/xml.html#xml-parse), Section 12.4 in Chapter 12 of \"**Dive into Python**\" does a good job on elaborating the process of parsing an example XML file with ElementsTree. \ud83d\udcd6\n\nIf you are a visual learner, we suggest the following YouTube video\n* [Parsing XML files in Python](https://www.youtube.com/watch?v=c2qlCZhkwtE)\n\nWe strongly suggest that you read these materials, although we are going to reproduce some of their content\nalong with our own XML file.\n\nLet's start with ElementTree. \nThere are several ways to import the data, which depends on how the data is stored.\nHere we will read the file from disk.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!pip install wget"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import wget\n\nlink_to_data = 'https://github.com/tulip-lab/mds/raw/master/Jupyter/data/Melbourne_bike_share.xml'\n\nDataSet = wget.download(link_to_data)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!ls"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import xml.etree.ElementTree as etree    \ntree = etree.parse(\"Melbourne_bike_share.xml\")  "
        }, 
        {
            "source": "In the ElementTree API, an element object is designed to store data in a hierarchical structure according to the XML tag structure.\nEach element has a number of properties associated with it, for example, a tag, a text string,\na set of attributes and a set of child elements.\nThe <font color=\"blue\">parse()</font> function is one of the entry points of the ElementTree library.\nIt parses the entire XML document at once into an ElementTree object that contains a hierarchy of Element objects. \nsee [\"How ElementTree represents XML\"](http://infohost.nmt.edu/tcc/help/pubs/pylxml/web/etree-view.html). \ud83d\udcd6\n\nThe first element in every XML document is called the root element,\nand an XML document can only have one root.\nHowever, the returning ElementTree object is not the root element. \nInstead, it represents the entire document.\nTo get a reference to the root element, call <font color=\"blue\">getroot()</font> method.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "root = tree.getroot()     \nroot.tag"
        }, 
        {
            "source": "As expected, the root element is the <font color='orange'>response</font> element. See the original XML file.\nYou can also check the number of children of the root element by typing\n```python\n    len(root)\n```\nIt will give you one. To get the only child, one can use the <font color=\"blue\">getchildren()</font> method.\nBut it will result in a warning message\nthat looks like \n```python\n    /Users/land/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: DeprecationWarning: This method \n    will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n    from ipykernel import kernelapp as app.\n```\nThis is because the method has already been deprecated in Python 2.7.\nIndeed, an element acts like a list in the ElementTree API.\nThe items of the list are the element\u2019s children.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "for child in root:           \n    print (child)"
        }, 
        {
            "source": "The <font color='orange'>root</font> list only contains its direct children elements. The children elements of each entry in the list are not included. \n\nEach element can also have its own set of attributes. The <font color=\"orange\">attrib</font> property of an element is a mutable \nPython dictionary. \nDoes the root have attributes? Let's check it out.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "root.attrib"
        }, 
        {
            "source": "It returns a empty dictionary. \nSo far, the element tree seems to be empty.\nNow you need to <font color='red'>either examine the original xml to discover the structure,\nor further traverse the element hierarchy by iteratively printing out all the elements and \ndata contained therein </font>.\nThe <font color='orange'>root</font> element has only one child.\nIt can be accessed by index, for example:\n```python\n    root[0]\n```\nA FOR loop can be used to print out all the children of <font color='orange'>root[0]</font>.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print (\"the total number of rows: \", len(root[0]))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "for child in root[0]:\n    print (child)"
        }, 
        {
            "source": "The tag of each child is the same, called 'row', which stores information about one bike station.\nLet's keep on retrieving the children of these rows. Instead of doing that for \nall the rows, we retrieve the children of <font color=\"orange\">root[0][0]</font> and that should correspond to the first record.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "for child in  root[0][0]:\n    print (child)"
        }, 
        {
            "source": "Fortunately, the tags of the retrieved child elements correspond to the column names in the DataFrame.\nThus, all the tags storing the data we want have been found. \nTo confirm it you can inspect the original XML file \nor simply look at the figure shown in Section 1. \nAnother way of exploring the element hierarchy is to use the iteration function of ElementTree, `iter()`.\nThe iterator loops over all elements in the tree, in section order.\nEach element is represented as a Python tuple, where the first entry is a tag,\nthe second is the text, and the last is a dictionary of attributes.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "for elem in tree.iter():\n    print (elem.tag, elem.text, elem.attrib)"
        }, 
        {
            "source": "Besides ElementTree, there are other Python libraries that can be used to parse XML files.\nHere we show two of them, which are **`lxml`** and **`BeautifulSoup`**.\n\n####  The lxml package\n[**`lxml`**](http://lxml.de) is an open source third-party library that builds on top of two C libraries \nlibxml2 and libxslt.\nIt is mostly compatible but superior to the well-known ElementTree API.\nTo study **`lxml`** in detail, you should refer to:\n* [the lxml.etree tutorial](http://lxml.de/tutorial.html), a tutorial on XML processing with lxml.etree.\n* and [Going Further With lxml](http://www.diveintopython3.net/xml.html#xml-lxml), Section 12.6 in Chapter 12 of \"**Dive into Python 3**\". \ud83d\udcd6 \n\nHere we are going to briefly show you how to extract the text content of an element tree\nusing **XPath**.\n**XPath** allows you to extract the separate text chunks into a list:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [], 
            "source": "from lxml import etree\nltree = etree.parse(\"Melbourne_bike_share.xml\")\nfor el in ltree.xpath('descendant-or-self::text()'):\n    print (el)"
        }, 
        {
            "source": "In the <font color='blue'>xpath()</font> function,\nthe <font color='orange'>descendant-or-self::</font> is an axis selector that limits the search to the context node, its children, their children, and so on out to the leaves of the tree. The <font color = 'blue'>text()</font> function selects only text nodes, discarding any elements, comments, and other non-textual content. The return value is a list of strings.\nRead [XPath processing](http://infohost.nmt.edu/tcc/help/pubs/pylxml/web/xpath.html) \ud83d\udcd6 for a short introduction\nto `xpath` and [W3C's website on Xpath](http://www.w3.org/TR/xpath/) for a detailed introduction to XPath.\nNote that <font color='blue'>lxml</font> is significantly faster than the built-in <font color='blue'>ElementTree</font> library on parsing large xml documents.\nIf your XML files are very large, you should consider using <font color='blue'>lxml</font>.\n\n#### The Beautiful Soup Pacakge\n[Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/) is an another Python library for pulling data out of HTML and XML files. It provides Pythonic idioms for iterating, searching, and modifying the parsed tree.\nWe begin by reading in our XML file and creating a Beautiful Soup object with the BeautifulSoup function. In regard to the assessment, we suggest the use of beautiful soup.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from bs4 import BeautifulSoup\nbtree = BeautifulSoup(open(\"Melbourne_bike_share.xml\"),\"lxml-xml\") "
        }, 
        {
            "source": "There are two different ways of passing an XML document into the BeautifulSoup constructor.\nOne is to pass in a string, another is to parse an open filehandle. the above example follows the second approach.\nThe second argument is the parser to be used to parse the document.\nBeautiful Soup presents the same interface to a number of different parsers, but each parser is different. Different parsers will create different parsed trees from the same document.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(btree.prettify())"
        }, 
        {
            "source": "The soup object contains all of the XML content in the original document.\nThe XML tags contained in the angled brackets provide structural information (and sometimes formatting).\nIf you were to take a moment to print out the parsed tree, you would find Beautiful Soup did a good job.\nIt provides a structural representation of the original XML document. \nNow it is easy for you to eyeball the document and the tags or attributes containing the data we want. <font color=\"red\">We will stop here and leave the extraction of the data with Beautiful Soup as a simple exercise for you.</font>\nThe documentation of how to use Beautiful Soup can be found [here](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "###  Extracting XML data into DataFrame\nSo far we have loaded XML into an element tree and have also found all the tags that contain the data we want. \nWe have worked with our XML file in a top-down fashion, starting with the root element, \nthen getting its child elements, and so on. \nWe have also gained a brief idea of **lxml** and **beautiful soup**.\nThis section will show you how to extract the data from all the tags and put it into Pandas DataFrame, a common\nand standard storage structure we used in the previous chapter. \nThis structure will also be used in the following chapters. \nBefore we walk through the extracting process, please read: \n* [Searching For Nodes Within An XML Document](http://www.diveintopython3.net/xml.html#xml-find) Section 12.5 in Chapter 12 of \"**Dive into Python 3**\". \ud83d\udcd6 \n\nLet's first just look at one tag, i.e., '*featurename*'.\nSince we don't know where it is, the code should loop over all the elements in the tree.\nTo produce a simple list of the featurenames, the logic could be simplified using \n`findall()` to look for all the elements with tag name '*featurename*'.\nBoth the ElementTree and the Element classes implement `findall(match)` function.\nThe one implemented by the ElementTree class finds all the matched subelements starting from root.\nThe other implemented by the Element finds those sub-elements starting from a given Element in the tree.\nAll the matched elements returned by the function are stored in a list.\nThe `match` argument should take values on either tag names or paths to specific tags.\nTry \n```python\n    tree.findall('featurename')\n```\nand \n```python\n    tree.findall('row/featurename')\n```\nWhat did you get?\n\nThe '*featurename*' tag is not the child or grandchild of the root element.\nIn order to get all the '*featurename*', \nwe should first figure out the path from the root to the '*featurename*' tag.\nBy looking at the original file or basing on what we learnt from the previous section, we know the path is\n```html\n    row/row/featurename\n```\nThus:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "elements = tree.findall('row/row/featurename')\nelements"
        }, 
        {
            "source": "The above list should contain 50 Elements corresponding to '*featurename*'.\nAs you may notice, the items returned by <font color=\"blue\">findall()</font> are Element objects, each representing a node in the\nXML parse tree. \nWhat we want is the data stored in those objects.\nTo pull out the data, we can access the element properpties: tag, attrib and text.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "featurename = [elem.text for elem in elements]\nfeaturename"
        }, 
        {
            "source": "You might wonder whether there is another way to extract the text stored in the '*featurename*' tag.\nIt might be possible that the structure of an XML file is quite complex (more complex that our example XML file) \nand it is not easy to figure out the path. \nThere are other ways to search for descendant elements, i.e., children, grandchildrens, \nand any element at any nesting level. \nUsing the same function, <font color = 'blue'>findall()</font>, we can construct an XPath argument to look for all\n'*featurename*' elements.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tree.findall('.//featurename')"
        }, 
        {
            "source": "It is very similar to the previous example, except for the two forward slashes at the beginning of the query.\nThe two forward slashes are short for <font color='orange'>/descendant-or-self::node()/</font>. \nHere <font color='orange'>.//featurename</font> selects any 'featurename' element in the XML document. \nSimilarly, we can extract the text with <font color='orange'>Element.text</font>.\n\nRemember that to visit the elements in the XML document in order, \nyou can use <font color='blue'>iter()</font> to create an iterator that iterates over all the ElementTree instances in a tree.\nWe have shown you how to explore the element hierarchy with this iteration fucntion.\nHere you are going to learn how to find specifc elements.\n[ElementTree's API](https://docs.python.org/2/library/xml.etree.elementtree.html#xml.etree.ElementTree.Element.findall)\nshows that <font color='blue'>iter()</font> function can take an argument <font color='blue'>tag</font>.\nIf the tag is specified, the iterator loops over all elements in the tree and returns \na list of elements having the specified tag.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "featurename = [] \nfor elem in tree.iter(tag = 'featurename'):\n   featurename.append(elem.text) \nfeaturename"
        }, 
        {
            "source": "The code pulls out data from all elements with a tag equal to '*featurename*', and stores the text in a list.\nSimilarly, you can retrieve data from elements having the following tags: 'id', 'terminalname', 'nbbikes',\n'nbemptydoc', and 'uploaddate' as follows. Note that we only print out the first 10 records of the retrieved data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "id = [] \nfor elem in tree.iter(tag='id'):\n       id.append(elem.text) \nid[:10]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "terminalname = []\nfor elem in tree.iter(tag='terminalname'):\n       terminalname.append(elem.text) \nterminalname[:10]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "nbbikes = []\nfor elem in tree.iter(tag='nbbikes'):\n       nbbikes.append(elem.text)  \nnbbikes[:10]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "nbemptydoc  = []\nfor elem in tree.iter(tag='nbemptydoc'):\n       nbemptydoc.append(elem.text) \nnbemptydoc[:10]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "uploaddate = []\nfor elem in tree.iter(tag='uploaddate'):\n       uploaddate.append(elem.text)  \nuploaddate[:10]"
        }, 
        {
            "source": "As mentioned in the introduction section, latitudes and longitudes\nare stored as attributes in 'coordinates' elements. \nExtracting them needs to access specific attributes that corresponds\nto latitude and longitude.\nRecall that attributes are dictionaries. \nTo extract a specific attribute value, you can use the \nsquare brackets along with the attribute name as the key to obtain its value.\nLet's first extract all the latitudes and longitudes and store them in two lists,\n\"lat\" and \"lon\" respectively.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "lat = []\nlon = []\nfor elem in tree.iter(tag='coordinates'):\n    lat.append(elem.attrib['latitude'])\n    lon.append(elem.attrib['longitude'])\nprint (lat[0:10])\nprint (lon[0:10])"
        }, 
        {
            "source": "The last step is to store the extracted data into Pandas DataFrame.\nThere are multiple ways of constructing a DataFrame object. \nHere you are going to generate a DataFrame by passing a Python dictionary to DataFrame's constructor\nand setting the index to IDs.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import pandas as pd \ndataDict = {}\ndataDict['Featurename'] = featurename\ndataDict['TerminalName'] = terminalname\ndataDict['NBBikes'] = nbbikes\ndataDict['NBEmptydoc'] = nbemptydoc\ndataDict['UploadDate'] = uploaddate\ndataDict['lat'] = lat\ndataDict['lon'] = lon\ndf = pd.DataFrame(dataDict, index = id)\ndf.index.name = 'ID'\ndf.head()"
        }, 
        {
            "source": "<a id = \"Computing\"></a>\n\n## <span style=\"color:#0b486b\">4. Parsing Excel Files</span>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "So far, you have learnt how to work with data in the formats that are machine readable, \nsuch as CSV, JSON and XML. \nThe approaches used to import data in those formats are generally standard. \nHowever, not all data can easily be imported into Python or other programming languages without \na fair amount of work.\nFor example, with data stored in spreadsheets and PDFs. \nIn these circumstances, data is generated purely for human consumption.\nThe person who generated the data often tries to make it easily readable for human, \ndisregarding the importance of releasing it in a machine readable format. \n\nWe will provide some generic instructions on how to scrape data from excel files. \nYou will find that the scraping process becomes much more difficult and time-consuming. \nBut the ultimate goal stays the same, i.e., extracting data and converting it into a machine readable format. \n\n\n### Introduction to Excel\n\nExcel is a popular spreadsheet application originally \ndeveloped for Windows. \nYou can also find free alternatives that run on Mac OS and Linux,\nfor example, LibreOffice Calc and OpenOffice Calc can both work with Excel files.\nAn Excel document is also called a workbook. \nIt is usually saved in a file with either .xlsx extension or .xls extension, \ndepending on the Excel version you use.\nA workbook can contain multiple worksheets, each of which is a grid of cells\nwhere you keep and manipulate the data. \nThose cells are arranged in numbered rows and letter-named columns.\nExcel can display not only tabular data but also data like line graphs, histograms and charts.\nIt also provides a set of data analysis functions for statistical, engineering and financial needs.\nPresumably, most of you know what a Excel file looks like. \nIf not, please find some Excel files online and have a look or open the Excel file used in this tutorial.\n\nThere are many ways of manipulating data stored in Excel spreadsheets. \nFor instance, \n\"[Working with Excel Files in Python](http://www.python-excel.org/)\" contains pointers to \nthe best information available about working with Excel files in Python. \nThe website lists the following Python packages that deal with Excel:\n\n* `openpyxl`: Reads/writes Excel 2010 xlsx/xlsm/xltx/xltm files.\n* `xlsxwriter`: write text, numbers, formulas and hyperlinks to multiple worksheets in an Excel 2007+ XLSX file.\n* `xlrd`: Extracts data from Excel files (.xls and .xlsx, versions 2.0 onwards).\n* `xlwt`: Writes and formats Excel files compatible with Microsoft Excel versions 95 to 2003.\n* `xlutils`: Contains a set of advanced tools for manipulating Excel files (requires `xlrd` and `xlwt`).\n\nYou would need to install each separately if you want to use them;\nhowever, in this tutorial we will use Pandas `ExcelFile` class that requires `xlrd` to demonstrate how to \nparse Excel files.\n\nSome tutorials on working with Excel files that might be of your interest:\n* [Working with Excel Spreadsheets](https://automatetheboringstuff.com/chapter12/): It utilizes openyxl to read\ndata from spreadsheets. Read the following sections:\n    * Reading Excel Documents \ud83d\udcd6\n    * Project: Reading Data from a Spreadsheet \ud83d\udcd6\n* [How to read Excel files with Python (xlrd tutorial)](https://www.youtube.com/watch?v=p0DNcTnreuY): \na Youtube video on extracting data from a simple Excel file. (Optional)\n\n\nThis tutorial will use a running example to show\nyou how to extract data from Excel spreadsheets step-by-step using Pandas.\nThe example we use in this tutorial is \"[Table 2: Nutrition](http://www.unicef.org/sowc2014/numbers/documents/excel/SOWC%202014%20Stat%20Tables_Table%202.xlsx)\" from Unicef's report on \n[The State of the Worlds Children](http://www.unicef.org/sowc2014/numbers/) for 2014.\nThe download link is located at the bottom of the webpage. \nPlease download the Excel file, and store it in the same folder as where \nthe notebook located.\n\nOur task is to extract the statistic data table on the child's issues of \nunderweight, stunting, wasting and overweight prevalence in different countries.\n\n### Parsing Excel with Pandas\nIn this section we will walk through the process of parsing our example Excel file with Pandas.\nA short tutorial on how to use Pandas `read_excel` function and the ExcelFile class  can \nbe found at Pandas [webpage on IO](http://pandas.pydata.org/pandas-docs/stable/io.html#io-excel-reader). \ud83d\udcd6  (Just read the section \"Reading Excel Files\".)\n\nBefore we start parsing our Excel file, \nwe need to first make sure the Python package `xlrd` is installed, \nas Pandas `ExcelFile` class makes use of `xlrd`. \nThe `xlrd` package can be run on Linux and Mac as well as Windows.\nHere we assume you use either Linux or Mac. \nIf you use Anaconda, you do not need to worry about this, \nas Anaconda includes the most popular Python packages for data analysis, including `xlrd`. \nOtherwise, you might need to install `xlrd` in order to run `read_excel`. \nTo install `xlrd`, you can use [pip](https://pypi.python.org/pypi/pip), \na Python package management system. \nIn your command line, simply type\n```shell\n    pip install xlrd\n```\n\nNow to start our script, \nwe need to import Pandas \nand open our Excel file by creating a Pandas `ExcelFile` object. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!pip install wget"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import wget\n\nlink_to_data = 'https://github.com/tulip-lab/mds/raw/master/Jupyter/data/SOWC2014.xlsx'\n\nDataSet = wget.download(link_to_data)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!ls"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import pandas as pd\nexcel_data = pd.ExcelFile('SOWC2014.xlsx')\nexcel_data"
        }, 
        {
            "source": "Unlike CSV files, an Excel file can have multiple worksheets.\nFor example, our Excel file contains two worksheets, one contains data notes,\nand the other contains the data we want.\nIn order to get our data, we will just pull the sheet with the data we want.\n\nIf your Excel file has a couple of worksheets and you can guess the index of \nthe worksheet that contains the data you want, or you have been told from which\nworksheet you are going to extract data, you can directly use Panda's \n[`read_excel`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html#pandas.read_excel) \nfuction\n```python\n    pandas.read_excel()\n```\nThis function reads an Excel table in a given worksheet into a Pandas DataFrame, \nwhere you can start further manipulating the data.\n\nHowever, in some cases, particularly while an Excel file has a lot of worksheets,\nit might be good to view all the sheets by their names.\nSo, let's check out what the names of the sheets we have in our Excel file are:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "excel_data.sheet_names"
        }, 
        {
            "source": "There are two worksheets in our Excel file.\nThe one that we are looking for is \"Table 2 \". \nSo, let's read the second worksheet into a Pandas DataFrame.\nNote that there is an extra space in the worksheet name.\nWithout this space, running the following parsing code \nwill result in the following error\n```\n    XLRDError: No sheet named <'Table 2'>\n```", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df = excel_data.parse('Table 2 ')\ndf.head()\n#df.shape"
        }, 
        {
            "source": "We have loaded the target worksheet into Python. \nThere are 322 rows and 28 columns (You can use `df.shape` to \nsee the dimensionality of the DataFrame).\n\nIf you scroll through the output, you will notice that the loaded data table is quite messy.\nThe messiness includes\n* Rows only contain missing values that are indicated by NaN in Pandas DataFrame.\n* Column heads are in three languages, i.e., English, French and Spanish.\n* Column heads in one language spread over multiple rows.\n* Country names also appear in three languages.\n* Notes shown in the original Excel file appear in rows towards the end of the data frame.\n\nRemember that our goal is to extract the data table in English. \nIt is clear that we need to further process the data frame. \nFor demonstration purpose,\nwe will try to keep the example as simple as possible,\nso we will not extract column heads here. \nInstead, if you are interested in programmatically extracting column heads, \nyou can try it by yourself. \n\n\n#### Task 1 drop useless columns and rows\n\nWe will start with removing country names in French and Spanish, \nwhich corresponds to remove two columns, labeled \"Unnamed: 1\" and \"Unnamed: 2\" in our data frame.\nTo do this, we are going to use DataFrame's [`drop()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html) function, \nwhich returns a new object with labels in requested axis removed.\nWe will frequently use this function later in this section.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df = df.drop(['Unnamed: 1', 'Unnamed: 2'], 1)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df.shape"
        }, 
        {
            "source": "Now you should have 26 columns.\nNext we are going to remove all the rows and columns that are empty, i.e., only contains NaNs.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df = df.dropna(0, how = 'all')\ndf = df.dropna(1, how = 'all')\ndf"
        }, 
        {
            "source": "Here we used the [`dropna`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html) function of DataFrame. The first argument is axis (0 means row, and 1 means column),\nand the second argument indicates deleting rows/columns with all NaNs. \nWe further removed 77 rows and 1 column. \n\nThe printout shows that\nthe very first column in the data frame only contains NaNs.\nThese NaNs are row indices.\nWe cannot delete it directly.\nInstead, we are going to reset the row indices with a list of integers.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df.index = range(len(df.index))\ndf.head(10)"
        }, 
        {
            "source": "After resetting all the row indices, and if you print out the\nfirst 15 rows using the slicing method:\n```\n    df[:15]\n```\nYou will find that the data we want starts from row index 9.\nThe first 9 rows contain column heads in three different languages.\nAs we mentioned before, to keep our script simple, we will not extract column heads here,\nrather we will delete them.\n\nSimilarly, if we print out the last 50 rows,\n```\n    df[-50:]\n```\nThe data we want ends at row 205. \nTherefore, we need to delete the first 9 rows and the \nlast 39 rows, and then reindex all the rows left.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Delete the first 9 rows\ndf = df.drop(df.index[0:9])\n# Delete the last 39 rows\ndf = df.drop(df.index[-39:])\n# Reindex rows\ndf.index = range(len(df.index))\ndf"
        }, 
        {
            "source": "####  Task 2 Set country index\n\nSo far we have extracted all the records (or rows) for 196 countries in our Excel file. \nLet's set the country names as row indices, and reset the column labels.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Set the country names as row indices\ndf = df.set_index(df['TABLE 2. NUTRITION'].values)\n\n# Delete \"TABLE 2. NUTRITION\" column, it is now redundant.\ndf = df.drop('TABLE 2. NUTRITION', 1)\n\n# Reindex columns\ndf.columns = list(range(len(df.columns))) \ndf.head()"
        }, 
        {
            "source": "#### Task 3 Tidy up all columns \n\nHowever, those records are still messy. \nAs you can see in the printout, there are a lot of NaNs, \nand cell values with both numbers and letters (e.g., \"6 x\", \" 39 x,y\",) spread over two columns.\nTherefore, we need to merge every two columns together. \n\nHow can we do that?\n\nLet us have a look at the first 10 rows and 2 columns.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df.iloc[:10, :2]"
        }, 
        {
            "source": "A close look at the printout will give you the following patterns:\n* If the cell contains only a float or '-', the corresponding cell value in the odd-numbered column is \"NaN\". \nSee the rows labeled \"**Afghanistan**``\", \"**Albania**\", etc.\n* If the original cell contains a float and a couple of letters, the cell in the even-numbered column contains the float, and the one in the odd-numbered column contains the letters. \nSee the rows labeled \"**Algeria**\", \"**Angola**\". etc.\n\nAssume that we are going to merge the two cells containing a float and letters respectively.\nWe need a FOR loop iterating over either odd- or even-numbered columns.\nWithin this FOR loop, another FOR loop is needed to iterate over rows.\nFor each row, we check if the cell in the odd-numbered column contains NaN.\nIf it does, we then merge it with the cell in the corresponding even-numbered column on the left.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# A FOR loop over odd-numbered columns\nfor col_idx in range(1, 24, 2): \n    # A For loop over rows\n    for row_idx in range(len(df)):\n        # A IF statement to check\n        #    1. If the cell value in the odd-numbered column is not NaN, then merge it the cell value in \n        #       the even-numbered column.\n        #    2. Otherwise, do nothing.\n        if not pd.isnull(df.iloc[row_idx, col_idx]):\n            df[col_idx-1][row_idx] = str(df[col_idx-1][row_idx]) + ' ' + str(df[col_idx ][row_idx])  \ndf.head()"
        }, 
        {
            "source": "The next step is to remove the odd-numbered columns in the data frame, as they are redundant now.\nTo do this, we are going to use DataFrame's `drop()` function again as follows", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "for col_idx in range(1, 24, 2): \n        df.drop(col_idx, 1, inplace = True)\ndf.head()"
        }, 
        {
            "source": "Now the data is in a pretty good shape aside from the column heads. \nWe can extract the column heads from the Excel file using either manual or programmatic method.\nHere we are going to do it manually. Considering that we are going to save results in an csv file. we will use the long name from the raw data \t\t\t\t\t\t\t\t\t\t\t\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df.columns = [\"Low birthweight (%)\", \\\n              \"Early initiation of breastfeeding (%)\", \\\n              \"Exclusive breastfeeding <6 months (%)\", \\\n              \"Introduction of solid, semi-solid or soft foods 6\u20138 months (%)\", \\\n              \"Breastfeeding at age 2 (%)\", \\\n              \"Underweight (%) moderate and severe\", \\\n              \"Underweight (%) severe\", \\\n              \"Stunting (%) moderate and severe\", \\\n              \"Wasting (%) moderate and severe\",\\\n              \"Overweight (%) moderate and severe\", \\\n              \"Vitamin A supplementation, full coverage(%)\", \\\n              \"Iodized salt consumption (%)\" ]\ndf.head()"
        }, 
        {
            "source": "Finally, we have extracted the data table from our Excel file, and put it into a Pandas DataFrame.\nThe DataFrame has 197 rows and 12 columns, where rows correspond to records for individual countries\nand columns are variables (or attributes). \nOur last step is to save the data table in a CSV file.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df.to_csv('en_final_table_2.csv')"
        }, 
        {
            "source": "What is the problem you get? Let's check the type of some values in the DataFrame using\n```\n    type(df.iloc[i,j])\n```\nwhere i indicates row index, and j indicates column index.\nYou will find that DataFrame's `read_excel` method has parsed all strings and special characters,\nlike '-', into Unicode objects.\nIf you print the DataFrame, however, you'll get the printed version of the Unicode.\nIn contrast, printing a value in a specific location, for example, \n```python\n    df.iloc[0,0]\n```\ngives you the original Unicode,\n```\n    u'\\u2013'\n```\nTherefore, you need to specify the encoding method while saving\nthe DataFrame into a CSV file.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df.to_csv('en_final_table_2.csv', sep=',', encoding='utf-8')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!ls"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }, 
        "anaconda-cloud": {}, 
        "name": "2. Parsing CSV & JSON Files.ipynb"
    }, 
    "nbformat": 4
}