{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern Data Science\n",
    "**(Module 07: Natural Language Processing)**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use, change and distribute this package.\n",
    "- If you found any issue/bug for this document, please submit an issue at [tulip-lab/mds](https://github.com/tulip-lab/mds/issues)\n",
    "\n",
    "Prepared by and for \n",
    "**Student Members** |\n",
    "2006-2019 [TULIP Lab](http://www.tulip.org.au)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session I - Text Classification\n",
    "\n",
    "## Contents\n",
    "\n",
    "1 [Supervised Classification](#Supervised)\n",
    "* Gender Identification\n",
    "* Choosing the Right Features\n",
    "* Error Aanalysis\n",
    "* Document Classification\n",
    "* Part-of-Speech Tagging\n",
    "* Exploiting Context \n",
    "* Sequence Classification\n",
    "* Other Methods for Sequence Classification\n",
    "\n",
    "\n",
    "\n",
    "2 [Further Examples of Supervised Classification](#Further)\n",
    "* Sentence Segmentation\n",
    "* Identifying Dialogue Act Types\n",
    "* Recognizing Textual Entailment\n",
    "* Scaling Up to Large Datasets\n",
    "\n",
    "\n",
    "3 [Evaluation](#Evaluation)\n",
    "* The Test Set / Accuracy\n",
    "* Precision and Recall\n",
    "* F-Measure\n",
    "* Confusion Matrices\n",
    "\n",
    "\n",
    "4 [Decision Trees](#Decision)\n",
    "\n",
    "\n",
    "5 [Naive Bayes Classifiers](#Naive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"Supervised\"></a>\n",
    "\n",
    "## <span style=\"color:#0b486b\">1. Supervised Classification</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is the task of choosing the correct class label for a given input.<BR> A classifier is called supervised if it is built based on training corpora containing the correct label for each input.\n",
    "\n",
    "<img src=\"http://www.nltk.org/images/supervised-classification.png\" width=\"700\"><BR>\n",
    "<center>(Figure 1) Supervised Classification</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import names as name2gender\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender_features('Shrek'): {'last_letter': 'k'}\n",
      "names ended with 'k':\n"
     ]
    }
   ],
   "source": [
    "print(\"gender_features('Shrek'):\", gender_features('Shrek'))\n",
    "print(\"names ended with 'k':\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(labeled_names): 7944\n",
      "[('Aamir', 'male'),\n",
      " ('Aaron', 'male'),\n",
      " ('Abbey', 'male'),\n",
      " ('Abbie', 'male'),\n",
      " ('Abbot', 'male'),\n",
      " ('Abbott', 'male'),\n",
      " ('Abby', 'male'),\n",
      " ('Abdel', 'male'),\n",
      " ('Abdul', 'male'),\n",
      " ('Abdulkarim', 'male')]\n"
     ]
    }
   ],
   "source": [
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "                 [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "print('len(labeled_names):', len(labeled_names))\n",
    "pprint(labeled_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(labeled_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_set): 7444\n",
      "len(test_set): 500\n",
      "[({'last_letter': 'e'}, 'female'),\n",
      " ({'last_letter': 'a'}, 'female'),\n",
      " ({'last_letter': 'e'}, 'female'),\n",
      " ({'last_letter': 'h'}, 'female'),\n",
      " ({'last_letter': 's'}, 'male'),\n",
      " ({'last_letter': 's'}, 'female'),\n",
      " ({'last_letter': 'a'}, 'female'),\n",
      " ({'last_letter': 'l'}, 'female'),\n",
      " ({'last_letter': 'n'}, 'male'),\n",
      " ({'last_letter': 'n'}, 'male')]\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(gender_features(name), gender) for (name,gender) in labeled_names]\n",
    "train_set = featuresets[500:]\n",
    "test_set = featuresets[:500] \n",
    "print('len(train_set):', len(train_set))\n",
    "print('len(test_set):', len(test_set))\n",
    "pprint(test_set[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "classifier=nltk.NaiveBayesClassifier.train(train_set)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.classify(gender_features('Neo')): male\n",
      "classifier.classify(gender_features('Trinity')): female\n",
      "classifier.classify(gender_features('Tony')): female\n"
     ]
    }
   ],
   "source": [
    "print(\"classifier.classify(gender_features('Neo')):\", classifier.classify(gender_features('Neo'))) \n",
    "print(\"classifier.classify(gender_features('Trinity')):\", classifier.classify(gender_features('Trinity')))\n",
    "print(\"classifier.classify(gender_features('Tony')):\", classifier.classify(gender_features('Tony')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.768\n"
     ]
    }
   ],
   "source": [
    "print('accuracy:', nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     35.6 : 1.0\n",
      "             last_letter = 'k'              male : female =     32.0 : 1.0\n",
      "             last_letter = 'f'              male : female =     15.4 : 1.0\n",
      "             last_letter = 'p'              male : female =     11.3 : 1.0\n",
      "             last_letter = 'd'              male : female =     10.7 : 1.0\n",
      "             last_letter = 'v'              male : female =     10.6 : 1.0\n",
      "             last_letter = 'o'              male : female =      8.9 : 1.0\n",
      "             last_letter = 'm'              male : female =      8.3 : 1.0\n",
      "             last_letter = 'r'              male : female =      6.7 : 1.0\n",
      "             last_letter = 'w'              male : female =      5.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> from nltk.classify import apply_features\n",
    ">>> train_set = apply_features(gender_features, labeled_names[500:])\n",
    ">>> test_set = apply_features(gender_features, labeled_names[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "from pprint import pprint\n",
    "from nltk.classify import apply_features \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = ([(name, 'male') for name in name2gender.words('male.txt')] + \\\n",
    "[(name,'female') for name in name2gender.words('female.txt')])\n",
    "random.shuffle(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(names): 7944\n",
      "[('Norma', 'female'),\n",
      " ('Kerri', 'female'),\n",
      " ('Elna', 'female'),\n",
      " ('Dyna', 'female'),\n",
      " ('Jerrylee', 'female'),\n",
      " ('Vanni', 'female'),\n",
      " ('Shaina', 'female'),\n",
      " ('Cherrita', 'female'),\n",
      " ('Aamir', 'male'),\n",
      " ('Poul', 'male')]\n"
     ]
    }
   ],
   "source": [
    "print('len(names):', len(names))\n",
    "pprint(names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender_features('Shrek'): {'last_letter': 'k'}\n",
      "[('Erik', 'male'),\n",
      " ('Brook', 'female'),\n",
      " ('Aleck', 'male'),\n",
      " ('Jack', 'male'),\n",
      " ('Izaak', 'male'),\n",
      " ('Hendrick', 'male'),\n",
      " ('Frank', 'male'),\n",
      " ('Isaak', 'male'),\n",
      " ('Hank', 'male'),\n",
      " ('Chuck', 'male')]\n"
     ]
    }
   ],
   "source": [
    "print(\"gender_features('Shrek'):\", gender_features('Shrek'))\n",
    "pprint([(name, gender) for (name,gender) in names if gender_features(name)['last_letter']=='k'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 106.22 MiB, increment: 3.75 MiB\n",
      "train_set: <class 'list'> 59616 bytes\n",
      "peak memory: 106.22 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit train_set = [(gender_features(name), gender) for (name,gender) in names][500:]\n",
    "print(\"train_set:\", type(train_set), sys.getsizeof(train_set), 'bytes')\n",
    "%memit classifier=nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%memit train_set2 = apply_features(gender_features, names[500:])\n",
    "print(\"train_set2:\", type(train_set2), sys.getsizeof(train_set2), 'bytes')\n",
    "%memit classifier=nltk.NaiveBayesClassifier.train(train_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_from_last_letter(names, letter): \n",
    "    li = []\n",
    "    for name, gender in names:\n",
    "        if name.endswith(letter):\n",
    "            li.append((name, gender))\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"ends with 'k'\")\n",
    "pprint(list_from_last_letter(names, 'k')[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Right Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import names as name2gender\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = ([(name, 'male') for name in name2gender.words('male.txt')] + \\\n",
    "[(name,'female') for name in name2gender.words('female.txt')])\n",
    "random.shuffle(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('len(names):', len(names))\n",
    "pprint(names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gender_features2(name):\n",
    "    features={}\n",
    "    features['firstletter']=name[0].lower()\n",
    "    features['lastletter']=name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features['count(%s)'%letter]=name.lower().count(letter)\n",
    "        features['has(%s)'%letter]=(letter in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"gender_features2('Shrek'):\")\n",
    "pprint(gender_features2('Shrek'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresets=[(gender_features2(name),gender) for (name, gender) in names]\n",
    "train_set=featuresets[500:]\n",
    "test_set=featuresets[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier=nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('accuracy:', nltk.classify.accuracy(classifier, test_set))\n",
    "classifier.show_most_informative_features(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Aanalysis \n",
    "\n",
    "<img src=\"http://www.nltk.org/images/corpus-org.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "development of the test set (dev - test) for each of several pieces, each of the test to perform error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import names as name2gender\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = ([(name, 'male') for name in name2gender.words('male.txt')] + \\\n",
    "[(name,'female') for name in name2gender.words('female.txt')])\n",
    "random.shuffle(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gender_features2(name):\n",
    "    features={}\n",
    "    features['firstletter']=name[0].lower()\n",
    "    features['lastletter']=name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features['count(%s)'%letter]=name.lower().count(letter)\n",
    "        features['has(%s)'%letter]=(letter in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_names=names[1500:] \n",
    "devtest_names=names[500:1500] \n",
    "# test_names=names[:500]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\"train_names: \", train_names)\n",
    "print (\"devtest_names: \", devtest_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = [(gender_features2(n), g) for (n,g) in train_names]\n",
    "devtest_set = [(gender_features2(n), g) for (n,g) in devtest_names]\n",
    "# test_set = [(gender_features2(n), g) for (n,g) in test_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errors=[]\n",
    "for(name, tag) in devtest_names:\n",
    "    guess=classifier.classify(gender_features(name))\n",
    "    if guess != tag:\n",
    "        errors.append((tag,guess,name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"error analysis (names ending with 'n')\")\n",
    "for (tag, guess, name) in sorted(errors):\n",
    "    if name.endswith('n'):\n",
    "        print('correct=%-8s guess=%-8s name=%-30s' % (tag, guess, name)) # the answer is that of the input data (name)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "document classification (film review by the sensibility analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "from pprint import pprint\n",
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_reviews.categories(): []\n"
     ]
    }
   ],
   "source": [
    "print(\"movie_reviews.categories():\", movie_reviews.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "# random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_words): 39768\n"
     ]
    }
   ],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words()) # the emergence of the word list and sequence alignment\n",
    "print(\"len(all_words):\", len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_features = list(all_words)[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk.__version__: 3.2.1\n"
     ]
    }
   ],
   "source": [
    "print(\"nltk.__version__:\", nltk.__version__)\n",
    "if nltk.__version__.startswith('3.'):\n",
    "    word_features = [k for (k,v) in all_words.most_common(2000)] # a list of common words (for nltk 3.x)\n",
    "else:\n",
    "    word_features = all_words.keys()[:2000] # a list of common words (for nltk 2.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_features: [',', 'the', '.', 'a', 'and', 'of', 'to', \"'\", 'is', 'in'] ...\n"
     ]
    }
   ],
   "source": [
    "print(\"word_features:\", word_features[:10], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'contains(smith)': False, 'contains(within)': False, 'contains(feels)': False, 'contains(thing)': False, 'contains(little)': True, 'contains(written)': False, 'contains(game)': False, 'contains(need)': False, 'contains(result)': False, 'contains(followed)': False, 'contains(dad)': False, 'contains(soundtrack)': False, 'contains(number)': False, 'contains(acts)': False, 'contains(vampires)': False, 'contains(war)': False, 'contains(impressive)': False, 'contains(indeed)': False, 'contains(fi)': False, 'contains(picture)': False, 'contains(slowly)': False, 'contains(quick)': True, 'contains(grows)': False, 'contains(want)': False, 'contains(thankfully)': False, 'contains(sitting)': False, 'contains(somehow)': False, 'contains(hard)': False, 'contains(production)': False, 'contains(william)': False, 'contains(out)': True, 'contains(music)': False, 'contains(toward)': False, 'contains(style)': False, 'contains(20)': False, 'contains(humorous)': False, 'contains(co)': False, 'contains(attempts)': False, 'contains(weak)': False, 'contains(allow)': False, 'contains(dvd)': False, 'contains(it)': True, 'contains(stories)': False, 'contains(of)': True, 'contains(cut)': False, 'contains(wild)': False, 'contains(charming)': False, 'contains(learn)': False, 'contains(real)': False, 'contains(narrative)': False, 'contains(robin)': False, 'contains(meets)': False, 'contains(characters)': False, 'contains(claire)': False, 'contains(van)': False, 'contains(role)': False, 'contains(crystal)': False, 'contains(eddie)': False, 'contains(players)': False, 'contains(viewer)': False, 'contains(dream)': False, 'contains(eyes)': False, 'contains(chinese)': True, 'contains(known)': True, 'contains(ass)': True, 'contains(cruise)': False, 'contains(wondering)': True, 'contains(jason)': False, 'contains(outstanding)': False, 'contains(you)': True, 'contains(themes)': False, 'contains(along)': True, 'contains(than)': False, 'contains(tell)': False, 'contains(calls)': False, 'contains(beach)': False, 'contains(simple)': False, 'contains(literally)': False, 'contains(train)': True, 'contains(supposedly)': False, 'contains(needed)': False, 'contains(appreciate)': False, 'contains(mean)': False, 'contains(scream)': True, 'contains(day)': True, 'contains(strike)': False, 'contains(fascinating)': False, 'contains(bloody)': False, 'contains(smart)': False, 'contains(gang)': False, 'contains(feet)': False, 'contains(six)': False, 'contains(meaning)': False, 'contains(thoroughly)': False, 'contains(food)': True, 'contains(recent)': False, 'contains(months)': False, 'contains(born)': False, 'contains(football)': False, 'contains(date)': False, 'contains(fame)': False, 'contains(loses)': False, 'contains(flynt)': False, 'contains(devil)': False, 'contains(children)': False, 'contains(jeff)': False, 'contains(sure)': False, 'contains(making)': True, 'contains(during)': False, 'contains(meeting)': False, 'contains(alan)': False, 'contains(point)': False, 'contains(amy)': False, 'contains(survive)': False, 'contains(short)': False, 'contains(throughout)': False, 'contains(lost)': False, 'contains(boss)': True, 'contains(appropriate)': False, 'contains(created)': False, 'contains(follow)': False, 'contains(dr)': False, 'contains(board)': False, 'contains(7)': False, 'contains(serious)': False, 'contains(driven)': False, 'contains(speaking)': False, 'contains(modern)': False, 'contains(have)': True, 'contains(song)': False, 'contains(touch)': False, 'contains(police)': False, 'contains(frightening)': False, 'contains(energy)': False, 'contains(air)': False, 'contains(half)': False, 'contains(award)': False, 'contains(nicely)': False, 'contains(name)': False, 'contains(obviously)': False, 'contains(outside)': False, 'contains(manages)': False, 'contains(face)': False, 'contains(tv)': False, 'contains(boyfriend)': False, 'contains(flaws)': False, 'contains(lack)': False, 'contains(moore)': False, 'contains(prove)': False, 'contains(particularly)': False, 'contains(jay)': False, 'contains(exception)': False, 'contains(anyway)': False, 'contains(stunning)': False, 'contains(plot)': True, 'contains(around)': False, 'contains(up)': False, 'contains(singing)': False, 'contains(content)': False, 'contains(1999)': False, 'contains(previous)': False, 'contains(always)': False, 'contains(case)': False, 'contains(i)': False, 'contains(godzilla)': False, 'contains(information)': False, 'contains(run)': False, 'contains(offers)': False, 'contains(joan)': False, 'contains(sees)': True, 'contains(actual)': False, 'contains(fantastic)': False, 'contains(debut)': False, 'contains(done)': False, 'contains(max)': False, 'contains(dog)': False, 'contains(sweet)': False, 'contains(takes)': False, 'contains(next)': False, 'contains(doctor)': False, 'contains(complete)': False, 'contains(])': False, 'contains(mike)': False, 'contains(others)': True, 'contains(starring)': False, 'contains(wonderful)': False, 'contains(toy)': False, 'contains(happening)': False, 'contains(generation)': False, 'contains(stone)': False, 'contains(deals)': False, 'contains(matter)': False, 'contains(slasher)': False, 'contains(city)': False, 'contains(directed)': False, 'contains(accent)': False, 'contains(release)': False, 'contains(tarantino)': False, 'contains(3)': False, 'contains(enjoy)': False, 'contains(screenplay)': False, 'contains(king)': False, 'contains(body)': False, 'contains(speed)': False, 'contains(slapstick)': False, 'contains(captain)': False, 'contains(kelly)': False, 'contains(once)': True, 'contains(whose)': False, 'contains(fairly)': True, 'contains(humans)': False, 'contains(should)': False, 'contains(damn)': False, 'contains(sea)': False, 'contains(sort)': False, 'contains(do)': True, 'contains(field)': True, 'contains(f)': False, 'contains(hate)': False, 'contains(ray)': False, 'contains(girlfriend)': True, 'contains(money)': False, 'contains(named)': False, 'contains(although)': False, 'contains(thought)': False, 'contains(poorly)': False, 'contains(campbell)': False, 'contains(younger)': False, 'contains(becoming)': False, 'contains(conflict)': False, 'contains(queen)': False, 'contains(walking)': False, 'contains(white)': False, 'contains(critics)': False, 'contains(used)': True, 'contains(gives)': False, 'contains(what)': True, 'contains(series)': True, 'contains(notice)': False, 'contains(biggest)': False, 'contains(review)': False, 'contains(1998)': False, 'contains(substance)': False, 'contains(russell)': False, 'contains(offensive)': False, 'contains(clever)': False, 'contains(fail)': False, 'contains(al)': False, 'contains(holds)': False, 'contains(titanic)': False, 'contains(seriously)': False, 'contains(through)': False, 'contains(realistic)': False, 'contains(presents)': False, 'contains(brothers)': False, 'contains(impact)': False, 'contains(itself)': False, 'contains(martial)': False, 'contains(version)': False, 'contains(m)': False, 'contains(wasted)': False, 'contains(eye)': False, 'contains(delivers)': False, 'contains(leaving)': False, 'contains(crazy)': False, 'contains(seem)': False, 'contains(pop)': False, 'contains(men)': False, 'contains(town)': False, 'contains(scene)': True, 'contains(hey)': False, 'contains(famous)': False, 'contains(across)': False, 'contains(forgotten)': False, 'contains(desire)': False, 'contains(wouldn)': False, 'contains(sit)': False, 'contains(business)': False, 'contains(many)': True, 'contains(rest)': False, 'contains(against)': False, 'contains(lawyer)': False, 'contains(include)': False, 'contains(urban)': False, 'contains(night)': False, 'contains(explain)': False, 'contains(near)': False, 'contains(dies)': False, 'contains(just)': True, 'contains(9)': False, 'contains(subtle)': False, 'contains(boys)': False, 'contains(heads)': False, 'contains(boat)': False, 'contains(large)': False, 'contains(tale)': False, 'contains(color)': False, 'contains(catherine)': False, 'contains(longer)': False, 'contains(masterpiece)': False, 'contains(guess)': False, 'contains(why)': False, 'contains(her)': False, 'contains(plenty)': False, 'contains(important)': False, 'contains(down)': False, 'contains(formula)': False, 'contains(criminal)': False, 'contains(tony)': False, 'contains(our)': False, 'contains(equally)': False, 'contains(minutes)': False, 'contains(somewhere)': False, 'contains(satire)': False, 'contains(blue)': False, 'contains(marriage)': False, 'contains(charm)': False, 'contains(presented)': False, 'contains(professional)': False, 'contains(decade)': False, 'contains(burton)': False, 'contains(lead)': True, 'contains(job)': False, 'contains(quality)': False, 'contains(anti)': False, 'contains(beyond)': False, 'contains(guilty)': False, 'contains(by)': True, 'contains(side)': False, 'contains(reasons)': False, 'contains(visual)': False, 'contains(front)': False, 'contains(incredible)': False, 'contains(guy)': True, 'contains(fighting)': False, 'contains(third)': True, 'contains(gay)': False, 'contains(trying)': False, 'contains(double)': False, 'contains(animation)': False, 'contains(annoying)': False, 'contains(believes)': False, 'contains(type)': False, 'contains(constant)': False, 'contains(reviews)': False, 'contains(o)': False, 'contains(batman)': False, 'contains(act)': False, 'contains(interest)': False, 'contains(realize)': False, 'contains(rich)': False, 'contains(box)': False, 'contains(nice)': False, 'contains(question)': False, 'contains(remember)': False, 'contains(ended)': False, 'contains(wrong)': True, 'contains(carter)': False, 'contains(further)': False, 'contains(travel)': False, 'contains(own)': True, 'contains(see)': False, 'contains(taylor)': False, 'contains(pull)': False, 'contains(pure)': False, 'contains(target)': False, 'contains(forever)': False, 'contains(set)': False, 'contains(died)': False, 'contains(folks)': False, 'contains(turns)': False, 'contains(british)': False, 'contains(rescue)': False, 'contains(hopes)': False, 'contains(at)': False, 'contains(routine)': False, 'contains(party)': False, 'contains(french)': False, 'contains(level)': False, 'contains(career)': False, 'contains(and)': True, 'contains(giant)': False, 'contains(leader)': False, 'contains(disturbing)': False, 'contains(1997)': False, 'contains(chance)': False, 'contains(seconds)': False, 'contains(he)': True, 'contains(guns)': False, 'contains(top)': True, 'contains(between)': True, 'contains(l)': False, 'contains(virtually)': False, 'contains(though)': False, 'contains(when)': True, 'contains(pass)': False, 'contains(amusing)': False, 'contains(fear)': False, 'contains(decent)': False, 'contains(perfect)': False, 'contains(shoot)': False, 'contains(managed)': False, 'contains(personal)': False, 'contains(free)': False, 'contains(meet)': False, 'contains(individual)': False, 'contains(romantic)': False, 'contains(science)': False, 'contains(problem)': False, 'contains(worse)': False, 'contains(adventure)': False, 'contains(least)': True, 'contains(serve)': False, 'contains(fiction)': False, 'contains(red)': False, 'contains(taking)': False, 'contains(send)': False, 'contains(exactly)': False, 'contains(knew)': False, 'contains(choice)': False, 'contains(pulp)': False, 'contains(anthony)': False, 'contains(animals)': False, 'contains(tension)': False, 'contains(carpenter)': False, 'contains(particular)': False, 'contains(gibson)': False, 'contains(s)': True, 'contains(unfunny)': False, 'contains(familiar)': False, 'contains(try)': False, 'contains(frame)': False, 'contains(tarzan)': False, 'contains(image)': False, 'contains(fate)': False, 'contains(princess)': False, 'contains(stage)': False, 'contains(easy)': False, 'contains(minute)': False, 'contains(--)': True, 'contains(bored)': False, 'contains(pieces)': False, 'contains(climax)': True, 'contains(finds)': False, 'contains(mel)': False, 'contains(look)': True, 'contains(mission)': False, 'contains(starts)': False, 'contains(shooting)': False, 'contains(miller)': False, 'contains(breaking)': False, 'contains(wonder)': False, 'contains(romance)': False, 'contains(starship)': False, 'contains(dangerous)': True, 'contains(hits)': False, 'contains(company)': False, 'contains(okay)': False, 'contains(hot)': False, 'contains(place)': True, 'contains(joel)': False, 'contains(needs)': False, 'contains(team)': False, 'contains(sexy)': False, 'contains(earlier)': False, 'contains(lines)': False, 'contains(rob)': False, 'contains(being)': False, 'contains(despite)': False, 'contains(superb)': False, 'contains(straight)': False, 'contains(disney)': False, 'contains(message)': False, 'contains(from)': True, 'contains(villains)': False, 'contains(moment)': False, 'contains(conclusion)': False, 'contains(step)': False, 'contains(limited)': False, 'contains(crap)': False, 'contains(been)': False, 'contains(history)': False, 'contains(michael)': False, 'contains(generally)': False, 'contains(except)': False, 'contains(be)': True, 'contains(feeling)': False, 'contains(turning)': False, 'contains(ultimately)': False, 'contains(woody)': False, 'contains(began)': False, 'contains(scott)': False, 'contains(horror)': False, 'contains(owner)': False, 'contains(return)': False, 'contains(same)': True, 'contains(last)': False, 'contains(brother)': False, 'contains(certain)': False, 'contains(my)': False, 'contains(travolta)': False, 'contains(second)': False, 'contains(provides)': False, 'contains(sent)': False, 'contains(couldn)': False, 'contains(involved)': False, 'contains(paul)': False, 'contains(man)': False, 'contains(bob)': False, 'contains(hardly)': False, 'contains(ready)': False, 'contains(extreme)': False, 'contains(come)': False, 'contains(ted)': False, 'contains(worthy)': False, 'contains(would)': False, 'contains(etc)': False, 'contains(sick)': False, 'contains(english)': False, 'contains(basically)': False, 'contains(read)': False, 'contains(twists)': False, 'contains(movie)': True, 'contains(fly)': False, 'contains(far)': False, 'contains(self)': False, 'contains(safe)': False, 'contains(falling)': False, 'contains(shots)': False, 'contains(future)': False, 'contains(every)': False, 'contains(totally)': False, 'contains(project)': False, 'contains(cartoon)': False, 'contains(truth)': False, 'contains(example)': False, 'contains(willing)': False, 'contains(create)': False, 'contains(directing)': False, 'contains(jackie)': True, 'contains(wall)': False, 'contains(cliches)': False, 'contains(intriguing)': False, 'contains(element)': False, 'contains(visuals)': False, 'contains(fantasy)': False, 'contains(spawn)': False, 'contains(cross)': False, 'contains(ve)': False, 'contains(edge)': False, 'contains(background)': False, 'contains(identity)': False, 'contains(involves)': True, 'contains(definitely)': False, 'contains(dreams)': False, 'contains(inside)': False, 'contains(son)': False, 'contains(bottom)': False, 'contains(industry)': False, 'contains(answer)': False, 'contains(avoid)': False, 'contains(appears)': False, 'contains(bright)': False, 'contains(force)': False, 'contains(cause)': False, 'contains(no)': False, 'contains(present)': False, 'contains(matt)': False, 'contains(loves)': False, 'contains(even)': False, 'contains(bland)': False, 'contains(friend)': False, 'contains(dramatic)': False, 'contains(george)': False, 'contains(charles)': False, 'contains(episode)': False, 'contains(talking)': False, 'contains(shock)': False, 'contains(woman)': True, 'contains(porn)': False, 'contains(thrown)': False, 'contains(carry)': False, 'contains(portrayal)': False, 'contains(each)': True, 'contains(flat)': False, 'contains(moments)': False, 'contains(impression)': False, 'contains(killer)': False, 'contains(word)': False, 'contains(super)': False, 'contains(interested)': False, 'contains(hero)': False, 'contains(beginning)': False, 'contains(bond)': False, 'contains(land)': False, 'contains(husband)': False, 'contains(local)': False, 'contains(affair)': False, 'contains(mars)': False, 'contains(community)': False, 'contains(occasionally)': False, 'contains(wish)': False, 'contains(jedi)': False, 'contains(care)': False, 'contains(expected)': False, 'contains(shallow)': False, 'contains(julie)': False, 'contains(off)': False, 'contains(rock)': False, 'contains(watching)': False, 'contains(floor)': False, 'contains(therefore)': False, 'contains(favorite)': False, 'contains(grand)': False, 'contains(able)': False, 'contains(four)': False, 'contains(hoping)': False, 'contains(his)': True, 'contains(protagonist)': False, 'contains(vincent)': False, 'contains(deep)': True, 'contains(fair)': False, 'contains(think)': False, 'contains(attempt)': False, 'contains(suddenly)': False, 'contains(;)': False, 'contains(blair)': False, 'contains(winner)': False, 'contains(wrote)': False, 'contains(gone)': False, 'contains(sexual)': False, 'contains(order)': False, 'contains(involving)': False, 'contains(considered)': False, 'contains(went)': False, 'contains(kevin)': True, 'contains(member)': False, 'contains(store)': False, 'contains(brain)': False, 'contains(time)': False, 'contains(bringing)': False, 'contains(buy)': False, 'contains(rush)': False, 'contains(troopers)': False, 'contains(woods)': False, 'contains(cheesy)': False, 'contains(bar)': False, 'contains(call)': False, 'contains(however)': True, 'contains(successful)': False, 'contains(professor)': False, 'contains(expectations)': False, 'contains(might)': False, 'contains(dull)': False, 'contains(makes)': False, 'contains(jackson)': False, 'contains(elizabeth)': False, 'contains(whole)': False, 'contains(year)': False, 'contains(relationship)': False, 'contains(films)': False, 'contains(new)': False, 'contains(lord)': False, 'contains(sam)': False, 'contains(songs)': False, 'contains(become)': False, 'contains(johnny)': False, 'contains(keep)': False, 'contains(star)': False, 'contains(spice)': False, 'contains(viewers)': False, 'contains(species)': False, 'contains(television)': False, 'contains(surprisingly)': False, 'contains(general)': False, 'contains(directors)': False, 'contains(mysterious)': False, 'contains(mind)': False, 'contains(ideas)': False, 'contains(actions)': False, 'contains(000)': False, 'contains(humanity)': False, 'contains(awful)': False, 'contains(older)': False, 'contains(rate)': False, 'contains(williams)': False, 'contains(merely)': False, 'contains(apparently)': False, 'contains(edward)': False, 'contains(ben)': False, 'contains(hand)': False, 'contains(list)': False, 'contains(instance)': False, 'contains(epic)': False, 'contains(dance)': False, 'contains(does)': False, 'contains(deal)': False, 'contains(villain)': False, 'contains(himself)': False, 'contains(mouth)': False, 'contains(surprised)': False, 'contains(truman)': False, 'contains(bus)': False, 'contains(genius)': False, 'contains(cinematographer)': False, 'contains(ability)': False, 'contains(race)': False, 'contains(christopher)': False, 'contains(relief)': False, 'contains(voice)': False, 'contains(novel)': False, 'contains(called)': False, 'contains(sandler)': False, 'contains(dimensional)': False, 'contains(editing)': False, 'contains(aren)': False, 'contains(viewing)': False, 'contains(getting)': True, 'contains(babe)': False, 'contains(cinema)': False, 'contains(among)': True, 'contains(proves)': False, 'contains(mother)': False, 'contains(julia)': False, 'contains(simply)': False, 'contains(approach)': False, 'contains(theaters)': False, 'contains(technology)': False, 'contains(title)': False, 'contains(introduced)': False, 'contains(twist)': False, 'contains(stop)': True, 'contains(news)': False, 'contains(gore)': False, 'contains(so)': False, 'contains(throw)': False, 'contains(start)': True, 'contains(featuring)': False, 'contains(hong)': True, 'contains(speak)': False, 'contains(remarkable)': False, 'contains(following)': False, 'contains(yes)': False, 'contains(central)': False, 'contains(heard)': False, 'contains(home)': False, 'contains(angry)': False, 'contains(dialogue)': False, 'contains(give)': False, 'contains(coming)': False, 'contains(over)': False, 'contains(battle)': False, 'contains(ground)': False, 'contains(brought)': False, 'contains(sign)': False, 'contains(almost)': False, 'contains(surprise)': False, 'contains(de)': False, 'contains(filled)': False, 'contains(age)': False, 'contains(talents)': False, 'contains(clearly)': False, 'contains(characterization)': False, 'contains(original)': False, 'contains(west)': False, 'contains(released)': False, 'contains(james)': False, 'contains(happy)': False, 'contains(destroy)': False, 'contains(happens)': False, 'contains(roger)': False, 'contains(agent)': False, 'contains(bizarre)': False, 'contains(jean)': False, 'contains(stock)': False, 'contains(interesting)': False, 'contains(fire)': False, 'contains(nasty)': False, 'contains(already)': False, 'contains(stuart)': False, 'contains(non)': True, 'contains(excuse)': False, 'contains(dying)': False, 'contains(matrix)': False, 'contains(cage)': False, 'contains(believable)': False, 'contains(screenwriter)': False, 'contains(dealing)': False, 'contains(period)': False, 'contains(are)': True, 'contains(fans)': False, 'contains(show)': False, 'contains(clear)': False, 'contains(pay)': False, 'contains(90)': False, 'contains(please)': False, 'contains(length)': False, 'contains(bit)': False, 'contains(crash)': False, 'contains(details)': False, 'contains(e)': False, 'contains(society)': False, 'contains(anyone)': False, 'contains(presence)': False, 'contains(open)': False, 'contains(scenes)': False, 'contains(moves)': False, 'contains(stuff)': False, 'contains(mediocre)': False, 'contains(experience)': False, 'contains(teenagers)': False, 'contains(station)': False, 'contains(audience)': False, 'contains(naked)': False, 'contains(worth)': True, 'contains(tim)': False, 'contains(offer)': False, 'contains(kept)': False, 'contains(hall)': False, 'contains(product)': False, 'contains(continues)': False, 'contains(roles)': False, 'contains(art)': False, 'contains(intelligent)': False, 'contains(match)': False, 'contains(uses)': False, 'contains(planet)': False, 'contains(actor)': False, 'contains(struggle)': False, 'contains(wars)': False, 'contains(pain)': False, 'contains(stephen)': False, 'contains(.)': True, 'contains(thinks)': False, 'contains(such)': False, 'contains(cameo)': False, 'contains(sarah)': False, 'contains(people)': False, 'contains(rather)': False, 'contains(writing)': False, 'contains(attention)': False, 'contains(major)': False, 'contains(adults)': False, 'contains(welcome)': False, 'contains(cold)': False, 'contains(humour)': False, 'contains(scary)': False, 'contains(south)': False, 'contains(left)': False, 'contains(chris)': False, 'contains(tried)': False, 'contains(years)': False, 'contains(on)': True, 'contains(ups)': False, 'contains(added)': False, 'contains(help)': True, 'contains(teen)': False, 'contains(predictable)': False, 'contains(leave)': False, 'contains(actors)': False, 'contains(cute)': False, 'contains(happened)': False, 'contains(officer)': False, 'contains(cannot)': False, 'contains(monster)': False, 'contains(most)': True, 'contains(legend)': False, 'contains(president)': False, 'contains(cool)': False, 'contains(task)': False, 'contains(grant)': False, 'contains(,)': True, 'contains(another)': False, 'contains(understand)': False, 'contains(innocent)': False, 'contains(highly)': False, 'contains(damon)': False, 'contains(lucas)': False, 'contains(remains)': False, 'contains(thomas)': False, 'contains(credit)': False, 'contains(places)': False, 'contains(spent)': False, 'contains(determined)': False, 'contains(sympathetic)': False, 'contains(discover)': False, 'contains(hilarious)': True, 'contains(tom)': False, 'contains(various)': False, 'contains(purpose)': False, 'contains(must)': False, 'contains(critic)': False, 'contains(them)': True, 'contains(fully)': False, 'contains(key)': True, 'contains(convincing)': False, 'contains(public)': False, 'contains(mob)': False, 'contains(originally)': False, 'contains(aliens)': False, 'contains(images)': False, 'contains(wit)': False, 'contains(finding)': False, 'contains(dennis)': False, 'contains(radio)': False, 'contains(douglas)': False, 'contains(2)': False, 'contains(scientist)': False, 'contains(portrayed)': False, 'contains(mark)': False, 'contains(instead)': False, 'contains(acted)': False, 'contains(line)': False, 'contains(caught)': False, 'contains(manage)': False, 'contains(hold)': False, 'contains(haven)': False, 'contains(fault)': False, 'contains(five)': False, 'contains(dumb)': False, 'contains(camera)': False, 'contains(graphic)': False, 'contains(absolutely)': False, 'contains(earth)': False, 'contains(taste)': False, 'contains(eventually)': False, 'contains(together)': False, 'contains(expecting)': False, 'contains(former)': False, 'contains(acting)': False, 'contains(plus)': False, 'contains(above)': False, 'contains(adds)': False, 'contains(control)': False, 'contains(ending)': False, 'contains(sorry)': False, 'contains(poor)': False, 'contains(serial)': False, 'contains(ii)': False, 'contains(best)': True, 'contains(build)': False, 'contains(creature)': False, 'contains(none)': False, 'contains(beauty)': False, 'contains(genre)': False, 'contains(ahead)': False, 'contains(winning)': False, 'contains(lot)': False, 'contains(giving)': False, 'contains(ways)': False, 'contains(gets)': True, 'contains(nudity)': False, 'contains(atmosphere)': False, 'contains(accident)': False, 'contains(ultimate)': False, 'contains(numerous)': False, 'contains(nights)': False, 'contains(cash)': False, 'contains(memorable)': False, 'contains(u)': False, 'contains(their)': False, 'contains(film)': False, 'contains(basic)': False, 'contains(apart)': False, 'contains(killed)': False, 'contains(rival)': False, 'contains(personality)': False, 'contains(pathetic)': False, 'contains(x)': False, 'contains(ridiculous)': False, 'contains(steven)': False, 'contains(possibly)': False, 'contains(missing)': False, 'contains(waiting)': False, 'contains(growing)': False, 'contains(feature)': False, 'contains(school)': False, 'contains(believe)': False, 'contains(spielberg)': False, 'contains(death)': False, 'contains(fat)': False, 'contains(leaves)': False, 'contains(addition)': False, 'contains(allen)': False, 'contains(found)': False, 'contains(skills)': False, 'contains(trailer)': False, 'contains(opposite)': False, 'contains(good)': False, 'contains(watched)': False, 'contains(hotel)': False, 'contains(members)': False, 'contains(seagal)': False, 'contains(telling)': False, 'contains(human)': False, 'contains(week)': False, 'contains(heavy)': False, 'contains(forward)': False, 'contains(trouble)': True, 'contains(video)': False, 'contains(singer)': False, 'contains(things)': True, 'contains(mentioned)': False, 'contains(work)': False, 'contains(confusing)': False, 'contains(hunting)': False, 'contains(share)': False, 'contains(chase)': True, 'contains(13)': False, 'contains(small)': False, 'contains(ice)': False, 'contains(horrible)': False, 'contains(laughing)': False, 'contains(elements)': False, 'contains(won)': False, 'contains(fast)': True, 'contains(first)': False, 'contains(gags)': False, 'contains(played)': True, 'contains(god)': False, 'contains(drive)': False, 'contains(any)': False, 'contains(if)': True, 'contains(bad)': False, 'contains(larry)': False, 'contains(door)': False, 'contains(mess)': False, 'contains(court)': False, 'contains(young)': False, 'contains(laughable)': False, 'contains(language)': False, 'contains(deliver)': False, 'contains(6)': False, 'contains(use)': False, 'contains(attractive)': True, 'contains(witty)': False, 'contains(finale)': False, 'contains(let)': False, 'contains(christmas)': False, 'contains(lucky)': False, 'contains(runs)': False, 'contains(partner)': False, 'contains(seems)': False, 'contains(whatever)': False, 'contains(not)': True, 'contains(funny)': True, 'contains(4)': False, 'contains(computer)': False, 'contains(brown)': False, 'contains(natural)': False, 'contains(both)': False, 'contains(speech)': False, 'contains(sheer)': False, 'contains(hair)': False, 'contains(to)': True, 'contains(haunting)': False, 'contains(parts)': False, 'contains(only)': True, 'contains(latter)': False, 'contains(process)': False, 'contains(military)': False, 'contains(difficult)': True, 'contains(mrs)': False, 'contains(lies)': False, 'contains(america)': False, 'contains(accept)': False, 'contains(comes)': False, 'contains(hope)': False, 'contains(figure)': False, 'contains(girl)': False, 'contains(usual)': False, 'contains(this)': True, 'contains(turned)': False, 'contains(lose)': False, 'contains(learns)': False, 'contains(die)': False, 'contains(all)': True, 'contains(rocky)': False, 'contains(eccentric)': False, 'contains(situation)': False, 'contains(reeves)': False, 'contains(power)': False, 'contains(world)': True, 'contains(may)': False, 'contains(evil)': False, 'contains(musical)': False, 'contains(right)': False, 'contains(talented)': False, 'contains(tells)': False, 'contains(emotion)': False, 'contains(student)': False, 'contains(either)': False, 'contains(remake)': False, 'contains(changed)': False, 'contains(ford)': False, 'contains(?)': False, 'contains(its)': False, 'contains(standard)': False, 'contains(revenge)': False, 'contains(lacking)': False, 'contains(oscar)': False, 'contains(terms)': False, 'contains(worst)': False, 'contains(price)': False, 'contains(stay)': False, 'contains(piece)': False, 'contains(doing)': False, 'contains(having)': False, 'contains(rarely)': False, 'contains(somewhat)': False, 'contains(provided)': False, 'contains(8)': False, 'contains(where)': True, 'contains(journey)': False, 'contains(going)': False, 'contains(disappointing)': False, 'contains(sharp)': False, 'contains(island)': False, 'contains(creative)': False, 'contains(disaster)': False, 'contains(long)': False, 'contains(check)': False, 'contains(stupid)': False, 'contains(responsible)': False, 'contains(joe)': False, 'contains(ask)': False, 'contains(sound)': False, 'contains(danny)': False, 'contains(kill)': False, 'contains(patch)': False, 'contains(looks)': True, 'contains(filmmakers)': False, 'contains(until)': False, 'contains(joke)': False, 'contains(make)': True, 'contains(wants)': False, 'contains(light)': False, 'contains(end)': False, 'contains(friends)': True, 'contains(three)': False, 'contains(enough)': False, 'contains(upon)': False, 'contains(join)': False, 'contains(whom)': False, 'contains(scale)': False, 'contains(someone)': False, 'contains(contact)': False, 'contains(media)': False, 'contains(mix)': False, 'contains(easily)': False, 'contains(asks)': False, 'contains(player)': False, 'contains(suspects)': False, 'contains(1)': False, 'contains(taken)': False, 'contains(streets)': False, 'contains(machine)': False, 'contains(baldwin)': False, 'contains(under)': False, 'contains(special)': False, 'contains(likable)': False, 'contains(vegas)': False, 'contains(laugh)': True, 'contains(failure)': False, 'contains(sight)': False, 'contains(bill)': False, 'contains(detail)': False, 'contains(running)': False, 'contains(break)': False, 'contains(became)': False, 'contains(affleck)': False, 'contains(explained)': False, 'contains(\")': True, 'contains(shouldn)': False, 'contains(forget)': False, 'contains(thinking)': False, 'contains(dude)': False, 'contains(cinematography)': False, 'contains(about)': True, 'contains(keeps)': True, 'contains(live)': False, 'contains(intense)': False, 'contains(crew)': False, 'contains(typical)': False, 'contains(!)': True, 'contains(producer)': False, 'contains(secret)': False, 'contains(got)': True, 'contains(before)': False, 'contains(person)': False, 'contains(boring)': False, 'contains(shot)': False, 'contains(japanese)': False, 'contains(terrible)': False, 'contains(doesn)': False, 'contains(amazing)': False, 'contains(files)': False, 'contains(books)': False, 'contains(heroes)': False, 'contains(storyline)': False, 'contains(aspects)': False, 'contains(had)': False, 'contains(liked)': False, 'contains(never)': True, 'contains(position)': False, 'contains(flick)': False, 'contains(events)': False, 'contains(admit)': False, 'contains(big)': False, 'contains(fun)': True, 'contains(problems)': True, 'contains(club)': False, 'contains(seemed)': False, 'contains(fellow)': False, 'contains(lives)': False, 'contains(success)': False, 'contains(opening)': False, 'contains(wise)': False, 'contains(me)': True, 'contains(get)': True, 'contains(shown)': False, 'contains(ship)': False, 'contains(includes)': False, 'contains(humor)': False, 'contains(filmmaking)': False, 'contains(wonderfully)': False, 'contains(test)': False, 'contains(sadly)': False, 'contains(*)': True, 'contains(gold)': False, 'contains(has)': True, 'contains(expect)': False, 'contains(physical)': True, 'contains(spirit)': False, 'contains(realizes)': False, 'contains(asked)': False, 'contains(government)': False, 'contains(arnold)': True, 'contains(era)': False, 'contains(talk)': True, 'contains(david)': False, 'contains(days)': False, 'contains(liners)': False, 'contains(pretty)': False, 'contains(street)': False, 'contains(say)': False, 'contains(tired)': False, 'contains(law)': False, 'contains(way)': True, 'contains(stands)': False, 'contains(reason)': False, 'contains(developed)': False, 'contains(hunt)': False, 'contains(political)': False, 'contains(angels)': False, 'contains(exciting)': False, 'contains(write)': False, 'contains(twice)': False, 'contains(entire)': False, 'contains(possible)': False, 'contains(back)': False, 'contains(am)': False, 'contains(fine)': False, 'contains(saying)': False, 'contains(working)': False, 'contains(opinion)': False, 'contains(road)': False, 'contains(names)': False, 'contains(direct)': False, 'contains(myself)': False, 'contains(isn)': False, 'contains(comedies)': False, 'contains(la)': False, 'contains(fake)': False, 'contains(count)': False, 'contains(fight)': True, 'contains(different)': False, 'contains(yourself)': False, 'contains(knows)': False, 'contains(led)': False, 'contains(works)': False, 'contains(love)': False, 'contains(cameron)': False, 'contains(one)': True, 'contains(twenty)': False, 'contains(camp)': False, 'contains(some)': False, 'contains(harry)': False, 'contains(certainly)': False, 'contains(hit)': False, 'contains(wanted)': False, 'contains(reality)': False, 'contains(lame)': False, 'contains(provide)': False, 'contains(completely)': False, 'contains(comparison)': False, 'contains(billy)': False, 'contains(issues)': False, 'contains(feelings)': False, 'contains(loved)': False, 'contains(jack)': False, 'contains(parody)': False, 'contains(figures)': False, 'contains(often)': True, 'contains(sequences)': False, 'contains(laughs)': False, 'contains(car)': False, 'contains(emotional)': False, 'contains(track)': False, 'contains(simon)': False, 'contains(filmed)': False, 'contains(subject)': False, 'contains(forces)': False, 'contains(worked)': False, 'contains(flicks)': False, 'contains(jr)': False, 'contains(costumes)': False, 'contains(plane)': False, 'contains(very)': True, 'contains(difference)': False, 'contains(pleasure)': False, 'contains(matthew)': False, 'contains(cops)': False, 'contains(supposed)': False, 'contains(like)': True, 'contains(security)': True, 'contains(audiences)': False, 'contains(fresh)': False, 'contains(walk)': False, 'contains(is)': True, 'contains(annie)': False, 'contains(bruce)': False, 'contains(ghost)': False, 'contains(wide)': False, 'contains(united)': False, 'contains(rose)': False, 'contains(mulan)': False, 'contains(watch)': True, 'contains(zero)': False, 'contains(nick)': False, 'contains(seen)': False, 'contains(york)': False, 'contains(features)': True, 'contains(100)': False, 'contains(touching)': False, 'contains(failed)': False, 'contains(snake)': False, 'contains(female)': False, 'contains(hidden)': False, 'contains(lover)': False, 'contains(subplot)': False, 'contains(hopkins)': False, 'contains(stand)': False, 'contains(creating)': False, 'contains(matters)': False, 'contains(black)': False, 'contains(apartment)': False, 'contains(century)': False, 'contains(pace)': False, 'contains(sister)': False, 'contains(wedding)': False, 'contains(enjoyed)': False, 'contains(overall)': False, 'contains(else)': False, 'contains(living)': False, 'contains(jones)': False, 'contains(=)': False, 'contains(overly)': False, 'contains(arts)': False, 'contains(succeeds)': False, 'contains(did)': False, 'contains(based)': False, 'contains(otherwise)': False, 'contains(sub)': False, 'contains(thanks)': False, 'contains(soldiers)': False, 'contains(true)': False, 'contains(nowhere)': False, 'contains(press)': False, 'contains(begins)': False, 'contains(unless)': False, 'contains(sequel)': False, 'contains(nearly)': False, 'contains(driving)': False, 'contains(studio)': False, 'contains(was)': False, 'contains(dollars)': False, 'contains(terrific)': False, 'contains(part)': False, 'contains(footage)': False, 'contains(concept)': False, 'contains(oh)': False, 'contains(sean)': False, 'contains(plan)': False, 'contains(nor)': False, 'contains(idea)': True, 'contains(common)': False, 'contains(d)': False, 'contains(waste)': False, 'contains(took)': False, 'contains(broken)': False, 'contains(producers)': False, 'contains(band)': False, 'contains(talent)': False, 'contains(faces)': False, 'contains(social)': False, 'contains(thin)': False, 'contains(extremely)': False, 'contains(murder)': False, 'contains(genuine)': False, 'contains(writers)': False, 'contains(thriller)': False, 'contains(which)': True, 'contains(beat)': False, 'contains(moving)': True, 'contains(sex)': False, 'contains(c)': False, 'contains(ends)': False, 'contains(creepy)': False, 'contains(few)': False, 'contains(today)': True, 'contains(solid)': False, 'contains(find)': False, 'contains(hear)': False, 'contains(final)': False, 'contains(for)': True, 'contains(more)': False, 'contains(results)': False, 'contains(decides)': False, 'contains(hanks)': False, 'contains(soul)': False, 'contains(academy)': False, 'contains(two)': True, 'contains(begin)': False, 'contains(strange)': False, 'contains(hour)': False, 'contains(saving)': False, 'contains(too)': False, 'contains(middle)': False, 'contains(thus)': False, 'contains(perhaps)': False, 'contains(they)': True, 'contains(popular)': False, 'contains(cares)': False, 'contains(yet)': False, 'contains(million)': True, 'contains(phantom)': False, 'contains(magic)': False, 'contains(women)': False, 'contains(mystery)': False, 'contains(brief)': True, 'contains(breaks)': False, 'contains(imagination)': False, 'contains(roberts)': False, 'contains(appeal)': False, 'contains(r)': False, 'contains(lacks)': False, 'contains(several)': False, 'contains(animated)': False, 'contains(past)': False, 'contains(drugs)': False, 'contains(know)': False, 'contains(discovers)': False, 'contains(came)': False, 'contains(onto)': False, 'contains(lots)': False, 'contains(williamson)': False, 'contains(intended)': False, 'contains(catch)': False, 'contains(effort)': False, 'contains(comic)': False, 'contains(likes)': False, 'contains(trust)': False, 'contains(reach)': False, 'contains(spectacular)': False, 'contains(hours)': False, 'contains(meanwhile)': False, 'contains(fox)': False, 'contains(themselves)': False, 'contains(charlie)': False, 'contains(event)': False, 'contains(apparent)': False, 'contains(chosen)': False, \"contains(')\": True, 'contains(myers)': False, 'contains(cheap)': False, 'contains(office)': False, 'contains(stars)': False, 'contains(kills)': False, 'contains(contains)': False, 'contains(means)': True, 'contains(vampire)': False, 'contains(aside)': False, 'contains(sometimes)': False, 'contains(great)': True, 'contains(alone)': False, 'contains(produced)': False, 'contains(can)': False, 'contains(kate)': False, 'contains(mood)': False, 'contains(kids)': False, 'contains(gangster)': True, 'contains(woo)': False, 'contains(enjoyable)': True, 'contains(appearance)': False, 'contains(space)': False, 'contains(hands)': False, 'contains(truly)': False, 'contains(effects)': False, 'contains(compelling)': False, 'contains(exist)': False, 'contains(flying)': False, 'contains(necessary)': False, 'contains(goes)': False, 'contains(child)': False, 'contains(unfortunately)': False, 'contains(normal)': False, 'contains(him)': True, 'contains(late)': False, 'contains(head)': False, 'contains(goal)': False, 'contains(fit)': False, 'contains(boy)': False, 'contains(imagine)': False, 'contains(alien)': False, 'contains(funniest)': False, 'contains(plain)': False, 'contains(these)': False, 'contains(how)': True, 'contains(weird)': False, 'contains(brings)': False, 'contains(bunch)': False, 'contains(material)': False, 'contains(girls)': False, 'contains(average)': False, 'contains(we)': False, 'contains(suppose)': False, 'contains(decided)': False, 'contains(didn)': False, 'contains(seeing)': True, 'contains(kid)': False, 'contains(development)': False, 'contains(plays)': False, 'contains(ago)': False, 'contains(richard)': False, 'contains(move)': False, 'contains(attitude)': False, 'contains(emotions)': False, 'contains(theater)': False, 'contains(powers)': False, 'contains(were)': False, 'contains(turn)': False, 'contains(willis)': False, 'contains(since)': False, 'contains(green)': False, 'contains(summer)': False, 'contains(wayne)': False, 'contains(attack)': False, 'contains(g)': False, 'contains(murphy)': False, 'contains(jerry)': False, 'contains(compared)': False, 'contains(cult)': False, 'contains(gave)': False, 'contains(now)': False, 'contains(single)': False, 'contains(kong)': True, 'contains(full)': False, 'contains(trip)': False, 'contains(action)': True, 'contains(inspired)': False, 'contains(-)': True, 'contains(shows)': False, 'contains(anne)': False, 'contains(nobody)': False, 'contains(considering)': False, 'contains(spends)': False, 'contains(austin)': False, 'contains(put)': False, 'contains(after)': False, 'contains(hollywood)': False, 'contains(mad)': False, 'contains(with)': True, 'contains(feel)': False, 'contains(rated)': False, 'contains(army)': False, 'contains(5)': False, 'contains(heart)': False, 'contains(30)': False, 'contains(culture)': False, 'contains(steal)': False, 'contains(ms)': False, 'contains(season)': False, 'contains(ed)': False, 'contains(lady)': False, 'contains(brooks)': False, 'contains(dark)': False, 'contains(points)': False, 'contains(low)': False, 'contains(classic)': False, 'contains(jimmy)': False, 'contains(life)': False, 'contains(times)': False, 'contains(us)': True, 'contains(surprising)': False, 'contains(don)': False, 'contains(potential)': False, 'contains(hurt)': False, 'contains(fact)': False, 'contains(an)': True, 'contains(post)': False, 'contains(cinematic)': False, 'contains(decide)': False, 'contains(becomes)': False, 'contains(prison)': False, 'contains(likely)': False, 'contains(beautiful)': False, 'contains(started)': False, 'contains(effective)': False, 'contains(away)': False, 'contains(wait)': False, 'contains(bring)': False, 'contains(device)': False, 'contains(reveal)': False, 'contains(seven)': False, 'contains(consider)': False, 'contains(crowd)': False, 'contains(intelligence)': False, 'contains(happen)': False, 'contains(pictures)': False, 'contains(directly)': False, 'contains(ill)': False, 'contains(writer)': False, 'contains(probably)': False, 'contains(minor)': False, 'contains(wife)': True, 'contains(honest)': False, 'contains(realized)': False, 'contains(looking)': False, 'contains(relationships)': False, 'contains(contrived)': False, 'contains(tone)': False, 'contains(opportunity)': False, 'contains(building)': False, 'contains(excellent)': False, 'contains(odd)': False, 'contains(+)': False, 'contains(guys)': False, 'contains(center)': False, 'contains(while)': True, 'contains(immediately)': False, 'contains(performances)': False, 'contains(adams)': False, 'contains(seemingly)': False, 'contains(later)': False, 'contains(go)': False, 'contains(made)': False, 'contains(drawn)': False, 'contains(towards)': False, 'contains(pg)': False, 'contains(alive)': False, 'contains(drug)': True, 'contains(brian)': False, 'contains(opens)': False, 'contains(as)': True, 'contains(actress)': False, 'contains(leads)': False, 'contains(note)': False, 'contains(course)': True, 'contains(follows)': False, 'contains(in)': True, 'contains(yeah)': False, 'contains(adult)': False, 'contains(rare)': False, 'contains(without)': False, 'contains(whether)': False, 'contains(complex)': False, 'contains(gary)': False, 'contains(mostly)': False, 'contains(t)': False, 'contains(something)': False, 'contains(desperate)': False, 'contains(superior)': False, 'contains(soon)': False, 'contains(house)': False, 'contains(chan)': True, 'contains(phone)': False, 'contains(character)': False, 'contains(master)': False, 'contains(actually)': True, 'contains(reading)': False, 'contains(that)': True, 'contains(teacher)': False, 'contains(shakespeare)': False, 'contains(casting)': False, 'contains(anderson)': False, 'contains(credits)': False, 'contains(kiss)': False, 'contains(jane)': False, 'contains(here)': True, 'contains(hell)': False, 'contains(due)': False, 'contains(comedic)': False, 'contains(said)': False, 'contains(win)': False, 'contains(then)': True, 'contains(appealing)': False, 'contains(weren)': False, 'contains(rent)': False, 'contains(those)': False, 'contains(changes)': False, 'contains(smile)': False, 'contains(moral)': False, 'contains(save)': False, 'contains(grace)': False, 'contains(she)': True, 'contains(filmmaker)': False, 'contains(witch)': False, 'contains(respect)': False, 'contains(ones)': False, 'contains(paced)': False, 'contains(window)': False, 'contains(robert)': False, 'contains(better)': False, 'contains(falls)': False, 'contains(shame)': False, 'contains(jennifer)': False, 'contains(lee)': False, 'contains(tough)': False, 'contains(system)': False, 'contains(doubt)': False, 'contains(ugly)': False, 'contains(view)': False, 'contains(alex)': False, 'contains(current)': False, 'contains(suspect)': False, 'contains(room)': False, 'contains(loving)': False, 'contains(movies)': True, 'contains(nothing)': False, 'contains(obvious)': False, 'contains(pointless)': False, 'contains(adaptation)': False, 'contains(incredibly)': False, 'contains(old)': False, 'contains(tommy)': False, 'contains(change)': False, 'contains(perfectly)': False, 'contains(ll)': False, 'contains([)': False, 'contains(/)': False, 'contains(killing)': False, 'contains(director)': False, 'contains(everyone)': False, 'contains(suspense)': False, 'contains(surface)': False, 'contains(promise)': False, 'contains(depth)': False, 'contains(drama)': False, 'contains(artist)': False, 'contains(kind)': True, 'contains(or)': False, 'contains(meant)': False, 'contains(horse)': False, 'contains(henry)': False, 'contains(performance)': False, 'contains())': True, 'contains(male)': False, 'contains(private)': False, 'contains(similar)': False, 'contains($)': False, 'contains(1996)': False, 'contains(tries)': True, 'contains(ok)': False, 'contains(impossible)': False, 'contains(the)': True, 'contains(script)': False, 'contains(quite)': False, 'contains(support)': False, 'contains(treat)': False, 'contains(jim)': False, 'contains(book)': False, 'contains(slightly)': False, 'contains(slow)': True, 'contains(virus)': False, 'contains(felt)': False, 'contains(goofy)': False, 'contains(generated)': False, 'contains(much)': False, 'contains(violence)': False, 'contains(theme)': False, 'contains(words)': False, 'contains(leading)': False, 'contains(constantly)': False, 'contains(states)': False, 'contains(latest)': False, 'contains(blame)': False, 'contains(saw)': False, 'contains(cast)': False, 'contains(violent)': False, 'contains(crime)': False, 'contains(silent)': False, 'contains(recently)': False, 'contains(sequence)': False, 'contains(beast)': False, 'contains(john)': False, 'contains(situations)': False, 'contains(recommend)': False, 'contains(but)': True, 'contains(says)': False, 'contains(damme)': False, 'contains(playing)': True, 'contains(peter)': False, 'contains(bug)': False, 'contains(baby)': False, 'contains(parents)': False, 'contains(stuck)': False, 'contains(headed)': False, 'contains(sounds)': False, 'contains(rating)': False, 'contains(family)': False, 'contains(:)': True, 'contains(heaven)': False, 'contains(especially)': True, 'contains(cover)': False, 'contains(carrey)': False, 'contains(main)': False, 'contains(ex)': False, 'contains(engaging)': False, 'contains(father)': False, 'contains(lynch)': False, 'contains(anything)': False, 'contains(could)': False, 'contains(keeping)': False, 'contains(unlike)': False, 'contains(looked)': False, 'contains(supporting)': False, 'contains(faith)': False, 'contains(quiet)': False, 'contains(told)': False, 'contains(strength)': False, 'contains(design)': False, 'contains(screen)': True, 'contains(neither)': False, 'contains(eight)': True, 'contains(ride)': False, 'contains(loud)': False, 'contains(still)': False, 'contains(brilliant)': False, 'contains(driver)': False, 'contains(fan)': False, 'contains(group)': False, 'contains(couple)': False, 'contains(blade)': False, 'contains(escape)': False, 'contains(decision)': False, 'contains(everything)': True, 'contains(standing)': True, 'contains(steve)': False, 'contains(high)': False, 'contains(()': True, 'contains(entertaining)': False, 'contains(college)': False, 'contains(spend)': False, 'contains(j)': False, 'contains(class)': False, 'contains(setting)': False, 'contains(forced)': False, 'contains(ryan)': False, 'contains(patrick)': False, 'contains(appear)': False, 'contains(mary)': False, 'contains(jail)': True, 'contains(blood)': False, 'contains(&)': False, 'contains(other)': True, 'contains(quickly)': False, 'contains(returns)': False, 'contains(washington)': False, 'contains(surprises)': False, 'contains(martin)': False, 'contains(manner)': False, 'contains(pair)': False, 'contains(mr)': False, 'contains(behind)': False, 'contains(again)': False, 'contains(sad)': False, 'contains(unique)': True, 'contains(there)': True, 'contains(your)': False, 'contains(fails)': False, 'contains(a)': True, 'contains(available)': False, 'contains(pick)': False, 'contains(visually)': False, 'contains(including)': False, 'contains(confused)': False, 'contains(strong)': False, 'contains(gun)': True, 'contains(puts)': False, 'contains(showing)': False, 'contains(rules)': False, 'contains(hasn)': False, 'contains(hill)': False, 'contains(less)': False, 'contains(utterly)': False, 'contains(guard)': True, 'contains(close)': False, 'contains(powerful)': False, 'contains(victim)': False, 'contains(plans)': False, 'contains(search)': False, 'contains(clich)': False, 'contains(cop)': False, 'contains(dead)': False, 'contains(water)': False, 'contains(fashion)': False, 'contains(daughter)': False, 'contains(`)': False, 'contains(married)': False, 'contains(premise)': False, 'contains(focus)': False, 'contains(sci)': False, 'contains(jokes)': False, 'contains(wasn)': False, 'contains(10)': False, 'contains(questions)': False, 'contains(menace)': False, 'contains(entirely)': False, 'contains(mention)': False, 'contains(seat)': False, 'contains(story)': False, 'contains(helen)': False, 'contains(using)': False, 'contains(maybe)': False, 'contains(ten)': False, 'contains(miss)': True, 'contains(given)': False, 'contains(silly)': False, 'contains(technical)': False, 'contains(model)': False, 'contains(state)': False, 'contains(b)': False, 'contains(fare)': False, 'contains(nature)': False, 'contains(motion)': False, 'contains(amount)': False, 'contains(score)': False, 'contains(country)': False, 'contains(comedy)': True, 'contains(frank)': False, 'contains(emotionally)': False, 'contains(because)': False, 'contains(apes)': False, 'contains(teenage)': False, 'contains(chemistry)': True, 'contains(early)': True, 'contains(total)': False, 'contains(aspect)': False, 'contains(helps)': False, 'contains(barely)': False, 'contains(will)': True, 'contains(usually)': False, 'contains(park)': False, 'contains(hospital)': False, 'contains(rising)': False, 'contains(buddy)': False, 'contains(allows)': False, 'contains(american)': False, 'contains(add)': False, 'contains(also)': True, 'contains(badly)': False, 'contains(nuclear)': False, 'contains(greatest)': False, 'contains(documentary)': False, 'contains(into)': True, 'contains(deserves)': False, 'contains(huge)': False, 'contains(form)': False, 'contains(creates)': False, 'contains(placed)': False, 'contains(ever)': True, 'contains(direction)': False, 'contains(sense)': False, 'contains(who)': True, 'contains(well)': True, 'contains(herself)': False, 'contains(fall)': False, 'contains(take)': False, 'contains(play)': False, 'contains(fbi)': False, 'contains(sets)': False, 'contains(budget)': False, 'contains(entertainment)': False, 'contains(really)': False, 'contains(detective)': False, 'contains(trek)': False, 'contains(re)': True, 'contains(finally)': False, 'contains(effect)': False}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# feature extraction of the function definition. (document) - > (including whether or not the word)\n",
    "def document_features(document): \n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words) \n",
    "    return features\n",
    "print(document_features(movie_reviews.words('pos/cv957_8737.txt')))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A ELE probability distribution must have at least one bin.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e9d38b621bc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# classification, machine learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# classifier = nltk.NaiveBayesClassifier.train(train_set)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\nltk\\classify\\naivebayes.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cls, labeled_featuresets, estimator)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;31m# Create the P(label) distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m         \u001b[0mlabel_probdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_freqdist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;31m# Create the P(fval|label, fname) distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, freqdist, bins)\u001b[0m\n\u001b[0;32m    848\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspecified\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mdefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfreqdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m         \"\"\"\n\u001b[1;32m--> 850\u001b[1;33m         \u001b[0mLidstoneProbDist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreqdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, freqdist, gamma, bins)\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m             raise ValueError('A %s probability distribution ' % name +\n\u001b[1;32m--> 730\u001b[1;33m                              'must have at least one bin.')\n\u001b[0m\u001b[0;32m    731\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mfreqdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m             \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: A ELE probability distribution must have at least one bin."
     ]
    }
   ],
   "source": [
    "# classification, machine learning\n",
    "# classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluation \n",
    "print('accuracy:', nltk.classify.accuracy(classifier, test_set))\n",
    "print(classifier.show_most_informative_features(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gc; gc.collect() # release memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "document classification (film review. through the analysis of # 2 (materials), not all the words), but not the (actor) as feature extraction, how to do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import names as name2gender\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the input data (documents, create a positive / negative)\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "# print(\"len(all_words):\", len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_names = set([name.lower() for name in name2gender.words('male.txt')] + \\\n",
    "[name.lower() for name in name2gender.words('female.txt')])  # list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if nltk.__version__.startswith('3.'):    \n",
    "    actor_names = [name.lower() for (name,v) in all_words.most_common() if name in _names] # film review in the list.\n",
    "else:\n",
    "    actor_names = [name.lower() for (name,v) in all_words.keys() if name in _names] # film review in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actor_names = actor_names[:2000] # the analysis and the conditions to be feature (name), a limited number of 2000.\n",
    "print(\"len(actor_names):\", len(actor_names), actor_names[:100], \"...\")\n",
    "print('jolie in actor_names:', 'jolie' in actor_names)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature extraction of the function definition. (document) - > (that contains the name of the actor)\n",
    "def document_features2(document): \n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in actor_names:\n",
    "        features['contains(%s)' % word] = (word in document_words) \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresets = [(document_features2(doc), category) for (doc, category) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "print(\"featuresets[0]:\", featuresets[0][0].items()[:20], \"...\", featuresets[0][1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classification\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluation\n",
    "print('accuracy:', nltk.classify.accuracy(classifier, test_set))\n",
    "print(classifier.show_most_informative_features(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gc; gc.collect() # release memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech Tagging\n",
    "\n",
    "\n",
    " brown corpus pos tags: http://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature extraction function definition (word) - > (suffix)\n",
    "suffix_fdist = nltk.FreqDist()\n",
    "print(\"len(brown.words()):\",  len(brown.words()))\n",
    "for word in brown.words()[:100000]: # a memory is used, too long, but as to some other use.\n",
    "    word = word.lower()\n",
    "    suffix_fdist[word[-1:]] += 1\n",
    "    suffix_fdist[word[-2:]] += 1\n",
    "    suffix_fdist[word[-3:]] += 1\n",
    "print(\"nltk.__version__:\", nltk.__version__)\n",
    "if nltk.__version__.startswith('3.'): \n",
    "    common_suffixes = [k for (k,v) in suffix_fdist.most_common(100)] # for nltk 3.x \n",
    "else:\n",
    "    common_suffixes = suffix_fdist.keys()[:100] # for nltk 2.x\n",
    "suffix_fdist=None\n",
    "print(\"common_suffixes:\", common_suffixes)    \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_features(word):\n",
    "    features = {}\n",
    "    for suffix in common_suffixes:\n",
    "        features['endswith(%s)' % suffix] = word.lower().endswith(suffix)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "def pos_features_print(word): # as long as the feature True is output. (in the book)\n",
    "    print(\"pos_features('\"+word+\"'):\", [(k, v) for (k, v) in pos_features(word).items() if v is True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_features_print('studied')      \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_words = brown.tagged_words(categories='news')\n",
    "print(\"len(tagged_words):\", len(tagged_words))\n",
    "tagged_words = tagged_words[:10000] # \n",
    "print(\"tagged_words:\", tagged_words[:10], \"...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "featuresets = [(pos_features(word), tag) for (word, tag) in tagged_words]\n",
    "size = int(len(featuresets) * 0.1) # test set size\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "tagged_words = None\n",
    "print(\"featuresets:\")\n",
    "pprint(featuresets[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = nltk.DecisionTreeClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"accuracy:\", nltk.classify.accuracy(classifier, test_set))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"classifier.classify(pos_features('cats')):\", classifier.classify(pos_features('cats'))) # NNS = plural noun\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classifier.pseudocode(depth=4))\n",
    "print(classifier.pp(depth=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc; gc.collect() # release memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploiting Context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals \n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_features(sentence, i):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"brown.sents()[0][7]:\", brown.sents()[0][7])\n",
    "print(\"brown.sents()[0][8]:\", brown.sents()[0][8])\n",
    "print(\"pos_features(brown.sents()[0], 8):\", pos_features(brown.sents()[0], 8))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "print(\"tagged_sents[0]:\", tagged_sents[0])\n",
    "print(\"nltk.tag.untag(tagged_sents[0]):\", nltk.tag.untag(tagged_sents[0]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresets = []\n",
    "for tagged_sent in tagged_sents:\n",
    "    untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "    for i, (word, tag) in enumerate(tagged_sent):\n",
    "        featuresets.append( (pos_features(untagged_sent, i), tag) )\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "print(\"train_set[0]:\", train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(\"featuresets[0]:\", featuresets[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"accuracy:\", nltk.classify.accuracy(classifier, test_set))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?nltk.NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals \n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pos_features(sentence, i, history):\n",
    "     features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                 \"suffix(2)\": sentence[i][-2:],\n",
    "                 \"suffix(3)\": sentence[i][-3:]}\n",
    "     if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "        features[\"prev-tag\"] = \"<START>\"\n",
    "     else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "        features[\"prev-tag\"] = history[i-1]\n",
    "     return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separator definition \n",
    "class ConsecutivePosTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = pos_features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = pos_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_sents = brown.tagged_sents(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]\n",
    "tagger = ConsecutivePosTagger(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"accuracy:\", tagger.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?nltk.TaggerI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Methods for Sequence Classification\n",
    "\n",
    "Hidden Markov Model (HMM) <BR> \n",
    "Maximum Entropy Markov Model (MEMM) <BR> \n",
    "Linear-Chain Conditional Random Field Model (CRF) <BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"Further\"></a>\n",
    "\n",
    "## <span style=\"color:#0b486b\">2. Further Examples of Supervised Classification</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals \n",
    "from pprint import pprint\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the input data generation (the word list, the boundary position)\n",
    "sents = nltk.corpus.treebank_raw.sents()\n",
    "tokens = []\n",
    "boundaries = set()  # the broken word position. (start from 0)\n",
    "offset = 0\n",
    "for sent in sents:    \n",
    "    tokens.extend(sent)\n",
    "    offset += len(sent)\n",
    "    boundaries.add(offset-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"len(sents):\", len(sents), sents[0:3], \"...\")\n",
    "print()\n",
    "print(\"len(tokens):\", len(tokens), tokens[0:30], \"...\")\n",
    "print()\n",
    "print(\"len(boundaries):\", len(boundaries), sorted(list(boundaries))[0:10], \"...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature extraction function definition (word list) - > (then the capital of the beginning of word, word, or a word or a text)\n",
    "def punct_features(tokens, i): # by punctuation\n",
    "    try:\n",
    "        return {'next-word-capitalized': tokens[i+1][0].isupper(),\n",
    "                'prevword': tokens[i-1].lower(),\n",
    "                'punct': tokens[i],\n",
    "                'prev-word-is-one-char': len(tokens[i-1]) == 1}\n",
    "    except:\n",
    "        return {'next-word-capitalized': False,\n",
    "                'prevword': '',\n",
    "                'punct': tokens[i],\n",
    "                'prev-word-is-one-char': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresets = [(punct_features(tokens, i), (i in boundaries))\n",
    "               for i in range(1, len(tokens)-1)\n",
    "               if tokens[i] in '.?!']\n",
    "print(\"featuresets:\", featuresets[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the study of test set generation (features and parts of speech)\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "print(\"train_set[0]:\", train_set[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classification\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluation\n",
    "print(\"accuracy:\", nltk.classify.accuracy(classifier, test_set))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the article separator\n",
    "def segment_sentences(words):\n",
    "    start = 0\n",
    "    sents = []\n",
    "    for i, word in enumerate(words):\n",
    "        if word in '.?!' and classifier.classify(punct_features(words, i)) == True: \n",
    "            sents.append(words[start:i+1])\n",
    "            start = i+1\n",
    "    if start < len(words):\n",
    "        sents.append(words[start:])\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the test separator\n",
    "sents = nltk.corpus.treebank_raw.sents()[:10]\n",
    "words=[]\n",
    "for s in sents:\n",
    "    words.extend(s)\n",
    "# print(\"words:\", words)\n",
    "# print()\n",
    "print(\"correct:\\n\", '\\n'.join([' '.join(s) for s in sents ]))\n",
    "print()\n",
    "print(\"guess:\\n\", '\\n'.join([' '.join(s) for s in segment_sentences(words)]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Dialogue Act Types  \n",
    "\n",
    " Act types: \"Statement,\" \"Emotion,\" \"ynQuestion\", and \"Continuer.\" \n",
    " \n",
    " Accept, Bye, Clarify, Continuer, Emotion, Emphasis, Greet, No Answer, Other, Reject, Statement, System, Wh-Question, Yes Answer, Yes/No Question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals \n",
    "from pprint import pprint\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posts = nltk.corpus.nps_chat.xml_posts()[:10000]\n",
    "print(\"posts[0]:\", posts[0].text)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dialogue_act_features(post):\n",
    "    features = {}\n",
    "    for word in nltk.word_tokenize(post):\n",
    "        features['contains(%s)' % word.lower()] = True\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresets = [(dialogue_act_features(post.text), post.get('class'))\n",
    "               for post in posts]\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(\"featuresets[0]:\", featuresets[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"accuracy:\", nltk.classify.accuracy(classifier, test_set))\n",
    "print(classifier.classify(dialogue_act_features(\"My name is Hyewoong\")))\n",
    "print(classifier.classify(dialogue_act_features(\"What a beautiful girl\")))\n",
    "print(classifier.classify(dialogue_act_features(\"Do you want my love\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognizing Textual Entailment\n",
    "\n",
    "Challenge 3, Pair 34 (True) <BR> <BR> T: Parviz Davudi was representing Iran at a meeting of the Shanghai Co-operation Organisation (SCO), the fledgling association that binds Russia, China and four former Soviet republics of central Asia together to fight terrorism.<BR> <BR> H: China is a member of SCO.<BR> <BR> <BR> <BR> Challenge 3, Pair 81 (False)<BR> <BR> T: According to NC Articles of Organization, the members of LLC company are H. Nelson Beavers, III, H. Chester Beavers and Jennie Beavers Stewart.<BR> <BR> H: Jennie Beavers Stewart is a share-holder of Carolina Analytical Laboratory.<BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals  \n",
    "from pprint import pprint\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rte_features(rtepair):\n",
    "    extractor = nltk.RTEFeatureExtractor(rtepair)\n",
    "    features = {}\n",
    "    features['word_overlap'] = len(extractor.overlap('word'))\n",
    "    features['word_hyp_extra'] = len(extractor.hyp_extra('word'))\n",
    "    features['ne_overlap'] = len(extractor.overlap('ne'))\n",
    "    features['ne_hyp_extra'] = len(extractor.hyp_extra('ne'))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rtepair = nltk.corpus.rte.pairs(['rte3_dev.xml'])[33]\n",
    "print(\"rtepair:\", rtepair.__dict__)\n",
    "print()\n",
    "print(\"text:\", rtepair.text)\n",
    "print()\n",
    "print(\"hypothesis(=keyword) :\", rtepair.hyp)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extractor = nltk.RTEFeatureExtractor(rtepair)\n",
    "print(\"text_words:\", extractor.text_words) \n",
    "print(\"overlap('word'):\", extractor.overlap('word'))\n",
    "print(\"overlap('ne')\", extractor.overlap('ne'))\n",
    "print(\"hyp_words:\", extractor.hyp_words)\n",
    "print(\"hyp_extra('word'):\", extractor.hyp_extra('word'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(help(extractor.overlap))\n",
    "print(help(extractor.hyp_extra))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Up to Large Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we recommend that you explore NLTK's facilities for interfacing with external machine learning packages <BR> ... to train classifier models significantly faster than the pure-Python classifier implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"Evaluation\"></a>\n",
    "\n",
    "## <span style=\"color:#0b486b\">3. Evaluation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Test Set / Accuracy\n",
    "\n",
    "However, it is very important that the test set be distinct from the training corpus: <BR> it is common to err on the side of safety by using 10% of the overall data for evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals  \n",
    "from pprint import pprint\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature extraction function definition (word) - > (suffix words, parts of speech in front of the door)\n",
    "def pos_features(sentence, i, history):\n",
    "     features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                 \"suffix(2)\": sentence[i][-2:],\n",
    "                 \"suffix(3)\": sentence[i][-3:]}\n",
    "     if i == 0:\n",
    "         features[\"prev-word\"] = \"<START>\"\n",
    "         features[\"prev-tag\"] = \"<START>\"\n",
    "     else:\n",
    "         features[\"prev-word\"] = sentence[i-1]\n",
    "         features[\"prev-tag\"] = history[i-1]\n",
    "     return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConsecutivePosTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = pos_features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = pos_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Not suitable for the test of 3 cases.\n",
    "# 1. a study of three types to create, test, and evaluation results, it is difficult to grasp. \n",
    "# 2. random. shuffle (), a document in the study of the test of the formation can be not good.\n",
    "tagged_sents = list(brown.tagged_sents(categories='news'))\n",
    "print(\"tagged_sents[0]:\", tagged_sents[0])\n",
    "random.shuffle(tagged_sents)\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size] \n",
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "print('Accuracy: %4.2f' % tagger.evaluate(test_sents))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sents = brown.tagged_sents(categories='news')\n",
    "test_sents = brown.tagged_sents(categories='fiction')\n",
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "print('Accuracy: %4.2f' % tagger.evaluate(test_sents))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_ids = brown.fileids(categories='news')\n",
    "size = int(len(file_ids) * 0.1)\n",
    "train_sents = brown.tagged_sents(file_ids[size:])\n",
    "test_sents = brown.tagged_sents(file_ids[:size])\n",
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "print('Accuracy: %4.2f' % tagger.evaluate(test_sents))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.nltk.org/images/precision-recall.png\" width=\"700\">\n",
    "<img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png\" width=\"700\">\n",
    "<img src=\"https://fbcdn-sphotos-c-a.akamaihd.net/hphotos-ak-xpa1/v/t1.0-9/10991051_844288612293942_8690474408857494396_n.jpg?oh=f4a68cc3875ebea360d2e2fbb1db68f8&oe=554DA29E&__gda__=1434765606_73492ef515b8cf34ddc9a82af0aff2d4\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F-Measure (F-Score, F1 score)\n",
    "\n",
    "http://en.wikipedia.org/wiki/F1_score <BR> <img src=\"http://upload.wikimedia.org/math/9/9/1/991d55cc29b4867c88c6c22d438265f9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?nltk.UnigramTagger\n",
    "?nltk.BigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals \n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_list(tagged_sents):\n",
    "    return [tag for sent in tagged_sents for (word, tag) in sent]\n",
    "def apply_tagger(tagger, corpus):\n",
    "    return [tagger.tag(nltk.tag.untag(sent)) for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gold = tag_list(brown.tagged_sents(categories='editorial')) # 사설"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = nltk.DefaultTagger('NN')\n",
    "test = tag_list(apply_tagger(t0, brown.tagged_sents(categories='editorial')))\n",
    "cm = nltk.ConfusionMatrix(gold, test)\n",
    "print(\"nltk.DefaultTagger('NN'):\")\n",
    "print(cm)\n",
    "# print(cm.pp(sort_by_count=True, show_percents=True, truncate=9))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "test = tag_list(apply_tagger(t1, brown.tagged_sents(categories='editorial')))\n",
    "cm = nltk.ConfusionMatrix(gold, test)\n",
    "print(\"nltk.UnigramTagger(train_sents):\")\n",
    "print(cm)\n",
    "# print(cm.pp(sort_by_count=True, show_percents=True, truncate=9))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "test = tag_list(apply_tagger(t2, brown.tagged_sents(categories='editorial')))\n",
    "cm = nltk.ConfusionMatrix(gold, test)\n",
    "print(\"nltk.BigramTagger(train_sents):\")\n",
    "print(cm)\n",
    "# print(cm.pp(sort_by_count=True, show_percents=True, truncate=9))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"Decision\"></a>\n",
    "\n",
    "## <span style=\"color:#0b486b\">4. Decision Trees</span>\n",
    "\n",
    "<img src=\"http://www.nltk.org/images/decision-tree.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy and Information Gain\n",
    "\n",
    "H = −Σl |in| labelsP(l) × log2P(l). <img src=\"http://www.nltk.org/images/Binary_entropy_plot.png\" width=\"500\"> <BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals \n",
    "from pprint import pprint\n",
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def entropy(labels):\n",
    "    freqdist = nltk.FreqDist(labels)\n",
    "    probs = [freqdist.freq(l) for l in nltk.FreqDist(labels)]\n",
    "    return -sum([p * math.log(p,2) for p in probs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"entropy(['male', 'male', 'male', 'male']):\", entropy(['male', 'male', 'male', 'male']))\n",
    "print(\"entropy(['male', 'female', 'male', 'male']):\", entropy(['male', 'female', 'male', 'male']))\n",
    "print(\"entropy(['female', 'male', 'female', 'male']):\", entropy(['female', 'male', 'female', 'male']))\n",
    "print(\"entropy(['female', 'female', 'male', 'female']):\", entropy(['female', 'female', 'male', 'female']))\n",
    "print(\"entropy(['female', 'female', 'female', 'female']):\", entropy(['female', 'female', 'female', 'female']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"Naive\"></a>\n",
    "\n",
    "## <span style=\"color:#0b486b\">5. Naive Bayes Classifiers</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.nltk.org/images/naive-bayes-triangle.png\" width=\"700\">\n",
    "\n",
    "<img src=\"http://www.nltk.org/images/naive_bayes_bargraph.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underlying Probabilistic Model\n",
    "\n",
    "<img src=\"http://www.nltk.org/images/naive_bayes_graph.png\" width=\"700\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [anaconda3]",
   "language": "python",
   "name": "Python [anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
