{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern Data Science\n",
    "**(Module 07: Natural Language Processing)**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use, but NOT allowed to change or distribute this package.\n",
    "\n",
    "Prepared by and for \n",
    "**Student Members** |\n",
    "2006-2019 [TULIP Lab](http://www.tulip.org.au)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session D - Text Classification\n",
    "\n",
    "## Contents\n",
    "\n",
    "1 [Supervised Classification](#Supervised)\n",
    "* Gender Identification\n",
    "* Choosing the Right Features\n",
    "* Error Aanalysis\n",
    "* Document Classification\n",
    "* Part-of-Speech Tagging\n",
    "* Exploiting Context \n",
    "* Sequence Classification\n",
    "* Other Methods for Sequence Classification\n",
    "\n",
    "\n",
    "\n",
    "2 [Further Examples of Supervised Classification](#Further)\n",
    "* Sentence Segmentation\n",
    "* Identifying Dialogue Act Types\n",
    "* Recognizing Textual Entailment\n",
    "* Scaling Up to Large Datasets\n",
    "\n",
    "\n",
    "3 [Evaluation](#Evaluation)\n",
    "* The Test Set / Accuracy\n",
    "* Precision and Recall\n",
    "* F-Measure\n",
    "* Confusion Matrices\n",
    "\n",
    "\n",
    "4 [Decision Trees](#Decision)\n",
    "\n",
    "\n",
    "5 [Naive Bayes Classifiers](#Naive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"Supervised\"></a>\n",
    "\n",
    "## <span style=\"color:#0b486b\">1. Supervised Classification</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is the task of choosing the correct class label for a given input.<BR> A classifier is called supervised if it is built based on training corpora containing the correct label for each input.\n",
    "\n",
    "<img src=\"http://www.nltk.org/images/supervised-classification.png\" width=\"700\"><BR>\n",
    "<center>(Figure 6-1) Supervised Classification</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import names as name2gender\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = ([(name, 'male') for name in name2gender.words('male.txt')] + \\\n",
    "[(name,'female') for name in name2gender.words('female.txt')])\n",
    "random.shuffle(names)\n",
    "\n",
    "print('len(names):', len(names))\n",
    "pprint(names[:10])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"gender_features('Shrek'):\", gender_features('Shrek'))\n",
    "print(\"names ended with 'k':\")\n",
    "pprint([(name, gender) for (name,gender) in names if gender_features(name)['last_letter']=='k'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresets = [(gender_features(name), gender) for (name,gender) in names]\n",
    "train_set = featuresets[500:]\n",
    "test_set = featuresets[:500] \n",
    "print('len(train_set):', len(train_set))\n",
    "print('len(test_set):', len(test_set))\n",
    "pprint(test_set[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier=nltk.NaiveBayesClassifier.train(train_set)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"classifier.classify(gender_features('Neo')):\", classifier.classify(gender_features('Neo'))) \n",
    "print(\"classifier.classify(gender_features('Trinity')):\", classifier.classify(gender_features('Trinity')))\n",
    "print(\"classifier.classify(gender_features('Tony')):\", classifier.classify(gender_features('Tony')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('accuracy:', nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "from pprint import pprint\n",
    "from nltk.classify import apply_features \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = ([(name, 'male') for name in name2gender.words('male.txt')] + \\\n",
    "[(name,'female') for name in name2gender.words('female.txt')])\n",
    "random.shuffle(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('len(names):', len(names))\n",
    "pprint(names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"gender_features('Shrek'):\", gender_features('Shrek'))\n",
    "pprint([(name, gender) for (name,gender) in names if gender_features(name)['last_letter']=='k'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%memit train_set = [(gender_features(name), gender) for (name,gender) in names][500:]\n",
    "print(\"train_set:\", type(train_set), sys.getsizeof(train_set), 'bytes')\n",
    "%memit classifier=nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%memit train_set2 = apply_features(gender_features, names[500:])\n",
    "print(\"train_set2:\", type(train_set2), sys.getsizeof(train_set2), 'bytes')\n",
    "%memit classifier=nltk.NaiveBayesClassifier.train(train_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_from_last_letter(names, letter): \n",
    "    li = []\n",
    "    for name, gender in names:\n",
    "        if name.endswith(letter):\n",
    "            li.append((name, gender))\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"ends with 'k'\")\n",
    "pprint(list_from_last_letter(names, 'k')[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Right Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import names as name2gender\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = ([(name, 'male') for name in name2gender.words('male.txt')] + \\\n",
    "[(name,'female') for name in name2gender.words('female.txt')])\n",
    "random.shuffle(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('len(names):', len(names))\n",
    "pprint(names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gender_features2(name):\n",
    "    features={}\n",
    "    features['firstletter']=name[0].lower()\n",
    "    features['lastletter']=name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features['count(%s)'%letter]=name.lower().count(letter)\n",
    "        features['has(%s)'%letter]=(letter in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"gender_features2('Shrek'):\")\n",
    "pprint(gender_features2('Shrek'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresets=[(gender_features2(name),gender) for (name, gender) in names]\n",
    "train_set=featuresets[500:]\n",
    "test_set=featuresets[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier=nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('accuracy:', nltk.classify.accuracy(classifier, test_set))\n",
    "classifier.show_most_informative_features(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Aanalysis \n",
    "\n",
    "<img src=\"http://www.nltk.org/images/corpus-org.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "development of the test set (dev - test) for each of several pieces, each of the test to perform error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import names as name2gender\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = ([(name, 'male') for name in name2gender.words('male.txt')] + \\\n",
    "[(name,'female') for name in name2gender.words('female.txt')])\n",
    "random.shuffle(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gender_features2(name):\n",
    "    features={}\n",
    "    features['firstletter']=name[0].lower()\n",
    "    features['lastletter']=name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features['count(%s)'%letter]=name.lower().count(letter)\n",
    "        features['has(%s)'%letter]=(letter in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_names=names[1500:] \n",
    "devtest_names=names[500:1500] \n",
    "# test_names=names[:500]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\"train_names: \", train_names)\n",
    "print (\"devtest_names: \", devtest_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = [(gender_features2(n), g) for (n,g) in train_names]\n",
    "devtest_set = [(gender_features2(n), g) for (n,g) in devtest_names]\n",
    "# test_set = [(gender_features2(n), g) for (n,g) in test_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errors=[]\n",
    "for(name, tag) in devtest_names:\n",
    "    guess=classifier.classify(gender_features(name))\n",
    "    if guess != tag:\n",
    "        errors.append((tag,guess,name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"error analysis (names ending with 'n')\")\n",
    "for (tag, guess, name) in sorted(errors):\n",
    "    if name.endswith('n'):\n",
    "        print('correct=%-8s guess=%-8s name=%-30s' % (tag, guess, name)) # the answer is that of the input data (name)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "document classification (film review by the sensibility analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "from pprint import pprint\n",
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"movie_reviews.categories():\", movie_reviews.categories()) # category pos or neg\n",
    "print(\"movie_reviews.fileids('pos'):\", movie_reviews.fileids('pos')[:10], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "# random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"documents[0]:\", documents[0][0][:10], \"...\", documents[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words()) # the emergence of the word list and sequence alignment\n",
    "print(\"len(all_words):\", len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"nltk.__version__:\", nltk.__version__)\n",
    "if nltk.__version__.startswith('3.'):\n",
    "    word_features = [k for (k,v) in all_words.most_common(2000)] # a list of common words (for nltk 3.x)\n",
    "else:\n",
    "    word_features = all_words.keys()[:2000] # a list of common words (for nltk 2.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"word_features:\", word_features[:10], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature extraction of the function definition. (document) - > (including whether or not the word)\n",
    "def document_features(document): \n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words) \n",
    "    return features\n",
    "pprint(document_features(movie_reviews.words('pos/cv957_8737.txt')).items()[:10])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresets = [(document_features(doc), category) for (doc, category) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "print(\"featuresets[0]:\", featuresets[0][0].items()[:20], \"...\", featuresets[0][1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classification, machine learning\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluation \n",
    "print('accuracy:', nltk.classify.accuracy(classifier, test_set))\n",
    "print(classifier.show_most_informative_features(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gc; gc.collect() # release memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "document classification (film review. through the analysis of # 2 (materials), not all the words), but not the (actor) as feature extraction, how to do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import names as name2gender\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the input data (documents, create a positive / negative)\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "# print(\"len(all_words):\", len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_names = set([name.lower() for name in name2gender.words('male.txt')] + \\\n",
    "[name.lower() for name in name2gender.words('female.txt')])  # list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if nltk.__version__.startswith('3.'):    \n",
    "    actor_names = [name.lower() for (name,v) in all_words.most_common() if name in _names] # film review in the list.\n",
    "else:\n",
    "    actor_names = [name.lower() for (name,v) in all_words.keys() if name in _names] # film review in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actor_names = actor_names[:2000] # the analysis and the conditions to be feature (name), a limited number of 2000.\n",
    "print(\"len(actor_names):\", len(actor_names), actor_names[:100], \"...\")\n",
    "print('jolie in actor_names:', 'jolie' in actor_names)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature extraction of the function definition. (document) - > (that contains the name of the actor)\n",
    "def document_features2(document): \n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in actor_names:\n",
    "        features['contains(%s)' % word] = (word in document_words) \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresets = [(document_features2(doc), category) for (doc, category) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "print(\"featuresets[0]:\", featuresets[0][0].items()[:20], \"...\", featuresets[0][1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classification\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluation\n",
    "print('accuracy:', nltk.classify.accuracy(classifier, test_set))\n",
    "print(classifier.show_most_informative_features(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gc; gc.collect() # release memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech Tagging\n",
    "\n",
    "\n",
    " brown corpus pos tags: http://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature extraction function definition (word) - > (suffix)\n",
    "suffix_fdist = nltk.FreqDist()\n",
    "print(\"len(brown.words()):\",  len(brown.words()))\n",
    "for word in brown.words()[:100000]: # a memory is used, too long, but as to some other use.\n",
    "    word = word.lower()\n",
    "    suffix_fdist[word[-1:]] += 1\n",
    "    suffix_fdist[word[-2:]] += 1\n",
    "    suffix_fdist[word[-3:]] += 1\n",
    "print(\"nltk.__version__:\", nltk.__version__)\n",
    "if nltk.__version__.startswith('3.'): \n",
    "    common_suffixes = [k for (k,v) in suffix_fdist.most_common(100)] # for nltk 3.x \n",
    "else:\n",
    "    common_suffixes = suffix_fdist.keys()[:100] # for nltk 2.x\n",
    "suffix_fdist=None\n",
    "print(\"common_suffixes:\", common_suffixes)    \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_features(word):\n",
    "    features = {}\n",
    "    for suffix in common_suffixes:\n",
    "        features['endswith(%s)' % suffix] = word.lower().endswith(suffix)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "def pos_features_print(word): # as long as the feature True is output. (in the book)\n",
    "    print(\"pos_features('\"+word+\"'):\", [(k, v) for (k, v) in pos_features(word).items() if v is True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_features_print('studied')      \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_words = brown.tagged_words(categories='news')\n",
    "print(\"len(tagged_words):\", len(tagged_words))\n",
    "tagged_words = tagged_words[:10000] # \n",
    "print(\"tagged_words:\", tagged_words[:10], \"...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresets = [(pos_features(word), tag) for (word, tag) in tagged_words]\n",
    "size = int(len(featuresets) * 0.1) # test set size\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "tagged_words = None\n",
    "print(\"featuresets:\")\n",
    "pprint(featuresets[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = nltk.DecisionTreeClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"accuracy:\", nltk.classify.accuracy(classifier, test_set))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"classifier.classify(pos_features('cats')):\", classifier.classify(pos_features('cats'))) # NNS = plural noun\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classifier.pseudocode(depth=4))\n",
    "print(classifier.pp(depth=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc; gc.collect() # release memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploiting Context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals \n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_features(sentence, i):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"brown.sents()[0][7]:\", brown.sents()[0][7])\n",
    "print(\"brown.sents()[0][8]:\", brown.sents()[0][8])\n",
    "print(\"pos_features(brown.sents()[0], 8):\", pos_features(brown.sents()[0], 8))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "print(\"tagged_sents[0]:\", tagged_sents[0])\n",
    "print(\"nltk.tag.untag(tagged_sents[0]):\", nltk.tag.untag(tagged_sents[0]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresets = []\n",
    "for tagged_sent in tagged_sents:\n",
    "    untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "    for i, (word, tag) in enumerate(tagged_sent):\n",
    "        featuresets.append( (pos_features(untagged_sent, i), tag) )\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "print(\"train_set[0]:\", train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(\"featuresets[0]:\", featuresets[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"accuracy:\", nltk.classify.accuracy(classifier, test_set))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?nltk.NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals \n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pos_features(sentence, i, history):\n",
    "     features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                 \"suffix(2)\": sentence[i][-2:],\n",
    "                 \"suffix(3)\": sentence[i][-3:]}\n",
    "     if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "        features[\"prev-tag\"] = \"<START>\"\n",
    "     else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "        features[\"prev-tag\"] = history[i-1]\n",
    "     return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separator definition \n",
    "class ConsecutivePosTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = pos_features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = pos_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_sents = brown.tagged_sents(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]\n",
    "tagger = ConsecutivePosTagger(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"accuracy:\", tagger.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?nltk.TaggerI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Methods for Sequence Classification\n",
    "\n",
    "Hidden Markov Model (HMM) <BR> \n",
    "Maximum Entropy Markov Model (MEMM) <BR> \n",
    "Linear-Chain Conditional Random Field Model (CRF) <BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"Further\"></a>\n",
    "\n",
    "## <span style=\"color:#0b486b\">2. Further Examples of Supervised Classification</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals \n",
    "from pprint import pprint\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the input data generation (the word list, the boundary position)\n",
    "sents = nltk.corpus.treebank_raw.sents()\n",
    "tokens = []\n",
    "boundaries = set()  # the broken word position. (start from 0)\n",
    "offset = 0\n",
    "for sent in sents:    \n",
    "    tokens.extend(sent)\n",
    "    offset += len(sent)\n",
    "    boundaries.add(offset-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"len(sents):\", len(sents), sents[0:3], \"...\")\n",
    "print()\n",
    "print(\"len(tokens):\", len(tokens), tokens[0:30], \"...\")\n",
    "print()\n",
    "print(\"len(boundaries):\", len(boundaries), sorted(list(boundaries))[0:10], \"...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature extraction function definition (word list) - > (then the capital of the beginning of word, word, or a word or a text)\n",
    "def punct_features(tokens, i): # by punctuation\n",
    "    try:\n",
    "        return {'next-word-capitalized': tokens[i+1][0].isupper(),\n",
    "                'prevword': tokens[i-1].lower(),\n",
    "                'punct': tokens[i],\n",
    "                'prev-word-is-one-char': len(tokens[i-1]) == 1}\n",
    "    except:\n",
    "        return {'next-word-capitalized': False,\n",
    "                'prevword': '',\n",
    "                'punct': tokens[i],\n",
    "                'prev-word-is-one-char': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresets = [(punct_features(tokens, i), (i in boundaries))\n",
    "               for i in range(1, len(tokens)-1)\n",
    "               if tokens[i] in '.?!']\n",
    "print(\"featuresets:\", featuresets[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the study of test set generation (features and parts of speech)\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "print(\"train_set[0]:\", train_set[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classification\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluation\n",
    "print(\"accuracy:\", nltk.classify.accuracy(classifier, test_set))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the article separator\n",
    "def segment_sentences(words):\n",
    "    start = 0\n",
    "    sents = []\n",
    "    for i, word in enumerate(words):\n",
    "        if word in '.?!' and classifier.classify(punct_features(words, i)) == True: \n",
    "            sents.append(words[start:i+1])\n",
    "            start = i+1\n",
    "    if start < len(words):\n",
    "        sents.append(words[start:])\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the test separator\n",
    "sents = nltk.corpus.treebank_raw.sents()[:10]\n",
    "words=[]\n",
    "for s in sents:\n",
    "    words.extend(s)\n",
    "# print(\"words:\", words)\n",
    "# print()\n",
    "print(\"correct:\\n\", '\\n'.join([' '.join(s) for s in sents ]))\n",
    "print()\n",
    "print(\"guess:\\n\", '\\n'.join([' '.join(s) for s in segment_sentences(words)]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Dialogue Act Types  \n",
    "\n",
    " Act types: \"Statement,\" \"Emotion,\" \"ynQuestion\", and \"Continuer.\" \n",
    " \n",
    " Accept, Bye, Clarify, Continuer, Emotion, Emphasis, Greet, No Answer, Other, Reject, Statement, System, Wh-Question, Yes Answer, Yes/No Question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals \n",
    "from pprint import pprint\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posts = nltk.corpus.nps_chat.xml_posts()[:10000]\n",
    "print(\"posts[0]:\", posts[0].text)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dialogue_act_features(post):\n",
    "    features = {}\n",
    "    for word in nltk.word_tokenize(post):\n",
    "        features['contains(%s)' % word.lower()] = True\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresets = [(dialogue_act_features(post.text), post.get('class'))\n",
    "               for post in posts]\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(\"featuresets[0]:\", featuresets[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"accuracy:\", nltk.classify.accuracy(classifier, test_set))\n",
    "print(classifier.classify(dialogue_act_features(\"My name is Hyewoong\")))\n",
    "print(classifier.classify(dialogue_act_features(\"What a beautiful girl\")))\n",
    "print(classifier.classify(dialogue_act_features(\"Do you want my love\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognizing Textual Entailment\n",
    "\n",
    "Challenge 3, Pair 34 (True) <BR> <BR> T: Parviz Davudi was representing Iran at a meeting of the Shanghai Co-operation Organisation (SCO), the fledgling association that binds Russia, China and four former Soviet republics of central Asia together to fight terrorism.<BR> <BR> H: China is a member of SCO.<BR> <BR> <BR> <BR> Challenge 3, Pair 81 (False)<BR> <BR> T: According to NC Articles of Organization, the members of LLC company are H. Nelson Beavers, III, H. Chester Beavers and Jennie Beavers Stewart.<BR> <BR> H: Jennie Beavers Stewart is a share-holder of Carolina Analytical Laboratory.<BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals  \n",
    "from pprint import pprint\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rte_features(rtepair):\n",
    "    extractor = nltk.RTEFeatureExtractor(rtepair)\n",
    "    features = {}\n",
    "    features['word_overlap'] = len(extractor.overlap('word'))\n",
    "    features['word_hyp_extra'] = len(extractor.hyp_extra('word'))\n",
    "    features['ne_overlap'] = len(extractor.overlap('ne'))\n",
    "    features['ne_hyp_extra'] = len(extractor.hyp_extra('ne'))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rtepair = nltk.corpus.rte.pairs(['rte3_dev.xml'])[33]\n",
    "print(\"rtepair:\", rtepair.__dict__)\n",
    "print()\n",
    "print(\"text:\", rtepair.text)\n",
    "print()\n",
    "print(\"hypothesis(=keyword) :\", rtepair.hyp)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extractor = nltk.RTEFeatureExtractor(rtepair)\n",
    "print(\"text_words:\", extractor.text_words) \n",
    "print(\"overlap('word'):\", extractor.overlap('word'))\n",
    "print(\"overlap('ne')\", extractor.overlap('ne'))\n",
    "print(\"hyp_words:\", extractor.hyp_words)\n",
    "print(\"hyp_extra('word'):\", extractor.hyp_extra('word'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(help(extractor.overlap))\n",
    "print(help(extractor.hyp_extra))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Up to Large Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we recommend that you explore NLTK's facilities for interfacing with external machine learning packages <BR> ... to train classifier models significantly faster than the pure-Python classifier implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"Evaluation\"></a>\n",
    "\n",
    "## <span style=\"color:#0b486b\">3. Evaluation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Test Set / Accuracy\n",
    "\n",
    "However, it is very important that the test set be distinct from the training corpus: <BR> it is common to err on the side of safety by using 10% of the overall data for evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals  \n",
    "from pprint import pprint\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature extraction function definition (word) - > (suffix words, parts of speech in front of the door)\n",
    "def pos_features(sentence, i, history):\n",
    "     features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                 \"suffix(2)\": sentence[i][-2:],\n",
    "                 \"suffix(3)\": sentence[i][-3:]}\n",
    "     if i == 0:\n",
    "         features[\"prev-word\"] = \"<START>\"\n",
    "         features[\"prev-tag\"] = \"<START>\"\n",
    "     else:\n",
    "         features[\"prev-word\"] = sentence[i-1]\n",
    "         features[\"prev-tag\"] = history[i-1]\n",
    "     return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConsecutivePosTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = pos_features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = pos_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Not suitable for the test of 3 cases.\n",
    "# 1. a study of three types to create, test, and evaluation results, it is difficult to grasp. \n",
    "# 2. random. shuffle (), a document in the study of the test of the formation can be not good.\n",
    "tagged_sents = list(brown.tagged_sents(categories='news'))\n",
    "print(\"tagged_sents[0]:\", tagged_sents[0])\n",
    "random.shuffle(tagged_sents)\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size] \n",
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "print('Accuracy: %4.2f' % tagger.evaluate(test_sents))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sents = brown.tagged_sents(categories='news')\n",
    "test_sents = brown.tagged_sents(categories='fiction')\n",
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "print('Accuracy: %4.2f' % tagger.evaluate(test_sents))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_ids = brown.fileids(categories='news')\n",
    "size = int(len(file_ids) * 0.1)\n",
    "train_sents = brown.tagged_sents(file_ids[size:])\n",
    "test_sents = brown.tagged_sents(file_ids[:size])\n",
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "print('Accuracy: %4.2f' % tagger.evaluate(test_sents))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.nltk.org/images/precision-recall.png\" width=\"700\">\n",
    "<img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png\" width=\"700\">\n",
    "<img src=\"https://fbcdn-sphotos-c-a.akamaihd.net/hphotos-ak-xpa1/v/t1.0-9/10991051_844288612293942_8690474408857494396_n.jpg?oh=f4a68cc3875ebea360d2e2fbb1db68f8&oe=554DA29E&__gda__=1434765606_73492ef515b8cf34ddc9a82af0aff2d4\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F-Measure (F-Score, F1 score)\n",
    "\n",
    "http://en.wikipedia.org/wiki/F1_score <BR> <img src=\"http://upload.wikimedia.org/math/9/9/1/991d55cc29b4867c88c6c22d438265f9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?nltk.UnigramTagger\n",
    "?nltk.BigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals \n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_list(tagged_sents):\n",
    "    return [tag for sent in tagged_sents for (word, tag) in sent]\n",
    "def apply_tagger(tagger, corpus):\n",
    "    return [tagger.tag(nltk.tag.untag(sent)) for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gold = tag_list(brown.tagged_sents(categories='editorial')) # 사설"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = nltk.DefaultTagger('NN')\n",
    "test = tag_list(apply_tagger(t0, brown.tagged_sents(categories='editorial')))\n",
    "cm = nltk.ConfusionMatrix(gold, test)\n",
    "print(\"nltk.DefaultTagger('NN'):\")\n",
    "print(cm)\n",
    "# print(cm.pp(sort_by_count=True, show_percents=True, truncate=9))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "test = tag_list(apply_tagger(t1, brown.tagged_sents(categories='editorial')))\n",
    "cm = nltk.ConfusionMatrix(gold, test)\n",
    "print(\"nltk.UnigramTagger(train_sents):\")\n",
    "print(cm)\n",
    "# print(cm.pp(sort_by_count=True, show_percents=True, truncate=9))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "test = tag_list(apply_tagger(t2, brown.tagged_sents(categories='editorial')))\n",
    "cm = nltk.ConfusionMatrix(gold, test)\n",
    "print(\"nltk.BigramTagger(train_sents):\")\n",
    "print(cm)\n",
    "# print(cm.pp(sort_by_count=True, show_percents=True, truncate=9))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"Decision\"></a>\n",
    "\n",
    "## <span style=\"color:#0b486b\">4. Decision Trees</span>\n",
    "\n",
    "<img src=\"http://www.nltk.org/images/decision-tree.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy and Information Gain\n",
    "\n",
    "H = −Σl |in| labelsP(l) × log2P(l). <img src=\"http://www.nltk.org/images/Binary_entropy_plot.png\" width=\"500\"> <BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals \n",
    "from pprint import pprint\n",
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def entropy(labels):\n",
    "    freqdist = nltk.FreqDist(labels)\n",
    "    probs = [freqdist.freq(l) for l in nltk.FreqDist(labels)]\n",
    "    return -sum([p * math.log(p,2) for p in probs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"entropy(['male', 'male', 'male', 'male']):\", entropy(['male', 'male', 'male', 'male']))\n",
    "print(\"entropy(['male', 'female', 'male', 'male']):\", entropy(['male', 'female', 'male', 'male']))\n",
    "print(\"entropy(['female', 'male', 'female', 'male']):\", entropy(['female', 'male', 'female', 'male']))\n",
    "print(\"entropy(['female', 'female', 'male', 'female']):\", entropy(['female', 'female', 'male', 'female']))\n",
    "print(\"entropy(['female', 'female', 'female', 'female']):\", entropy(['female', 'female', 'female', 'female']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"Naive\"></a>\n",
    "\n",
    "## <span style=\"color:#0b486b\">5. Naive Bayes Classifiers</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.nltk.org/images/naive-bayes-triangle.png\" width=\"700\">\n",
    "\n",
    "<img src=\"http://www.nltk.org/images/naive_bayes_bargraph.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underlying Probabilistic Model\n",
    "\n",
    "<img src=\"http://www.nltk.org/images/naive_bayes_graph.png\" width=\"700\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py2]",
   "language": "python",
   "name": "Python [py2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
