{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# FLIP(01):  Advanced Data Science\n**(Module 07: Natural Language Processing)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, but NOT allowed to change or distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2019 [TULIP Lab](http://www.tulip.org.au)\n\n---", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Session B - Accessing Text Corpora and Lexical Resources", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Contents\n\n1 [Accessing Text Corpora](#Corpora)\n* Gutenberg Corpus\n* Web and Chat Text\n* Brown Corpus\n* Regular Expression\n* Reuters \n* Inaugural Address Corpus\n* Corpora in Other Languages\n* Text Corpus Structure\n\n\n2 [Conditional Frequency Distributions](#Distributions)\n* Conditions and Events\n* Counting Words by Genre\n* Plotting and Tabulating Distributions\n\n3 [More Python: Reusing Code](#Code)\n* Creating Programs with a Text Editor\n* Functions\n* Modules\n\n4 [Lexical Resources](#Resources)\n* Wordlist Corpora\n* Comparative Wordlists\n* Shoebox and Toolbox Lexicons\n\n5 [WordNet](#WordNet)\n* Senses and Synonyms\n* The WordNet Hierarchy\n* More Lexical Relations\n* Semantic Similarity\n\n6 [Summary](#Summary)\n\n7 [Further Reading](#reading)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id = \"Corpora\"></a>\n\n## <span style=\"color:#0b486b\">1. Accessing Text Corpora</span>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Gutenberg Corpus\n\n- http://www.gutenberg.org", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def plural(word):\n    if word.endswith('y'):\n        return word[:-1] + 'ies'\n    elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:\n        return word + 'es'\n    elif word.endswith('an'):\n        return word[:-2] + 'en'\n    else:\n        return word + 's'"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import nltk"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "nltk.download()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from nltk import corpus"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "help(corpus.gutenberg.fileids)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "nltk.corpus.gutenberg.fileids()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\nemma"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(emma)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "help(nltk.Text)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))\nemma.concordance('surprize')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from nltk.corpus import gutenberg"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "gutenberg.fileids()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "emma = gutenberg.words('austen-emma.txt')"
        }, 
        {
            "source": "#### Interpret the for statement\n\n- num_chars: all characters\n- num_words: all words\n- num_sents: Any sentences or phrases ??\n- num_vocab: the only word in all words\n\n\n- All characters / All words, All words / All sentences or lists?\n- all words / all unique words, fileid\n- There's a description below.\n- average word length\n- average sentence length\n- On average how much each word appears in the text (verbal diversity score)\n\n> Note: What are you going to do with this? I have a purpose but I do not understand the purpose.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "for fileid in gutenberg.fileids():\n    num_chars = len(gutenberg.raw(fileid))\n    num_words = len(gutenberg.words(fileid))\n    num_sents = len(gutenberg.sents(fileid))\n    num_vocab = len(set([w.lower() for w in gutenberg.words(fileid)]))\n    print int(num_chars/num_words), int(num_words/num_sents), \\\n    int(num_words/num_vocab), fileid"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "help(gutenberg.raw)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(gutenberg.raw('austen-emma.txt'))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "help(gutenberg.words)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "gutenberg.words('austen-emma.txt')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "num_words2 = len(gutenberg.words('austen-emma.txt'))\nnum_words2"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "num_vocab2 = len(set([w.lower() for w in \n                      gutenberg.words('austen-emma.txt')]))\nnum_vocab2"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# How many of the words are the only words?\nnum_words2 / num_vocab2"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# sentence: sentence\n# utterance: words\nhelp(gutenberg.sents)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "gutenberg.sents('austen-emma.txt')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(gutenberg.sents('austen-emma.txt'))"
        }, 
        {
            "source": "- average word length: really 3, not 4, since the num_chars variable counts space characters", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "- sents(): divides the text up into its sentences, where each sentence is a list of words", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# how many letters occur in the text\nlen(gutenberg.raw('blake-poems.txt'))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')\nmacbeth_sentences"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(macbeth_sentences)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "macbeth_sentences[1037]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "longest_len = max([len(s) for s in macbeth_sentences])\nlongest_len"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "[s for s in macbeth_sentences if len(s) == longest_len][0][:20]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "' '.join([s for s in macbeth_sentences if len(s) == longest_len][0])"
        }, 
        {
            "source": "- Most NLTK corpus readers include various approaches such as `words ()`, `raw ()`, and `sents ()`.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Web and Chat Text\n\n- Gutenberg contains thousands of books: Represents published literature.\n- This is important. There are few formal language elements.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from nltk.corpus import webtext"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "help(webtext)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "for fileid in webtext.fileids():\n    print fileid, webtext.raw(fileid)[:65], '...'"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "webtext.fileids()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from nltk.corpus import nps_chat"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# 706 posts\n# 10/19\n# 20s\nchatroom = nps_chat.posts('10-19-20s_706posts.xml')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "chatroom[123]"
        }, 
        {
            "source": "### Brown Corpus\n\n- by genre. news, editorial, etc.\n- Where is Brown?\n- Where did you make a corpus?\n- I think this is the point ... If you made it on Reuters, you made it for your own news?", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from nltk.corpus import brown"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "brown.categories()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "brown.words(categories='news')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(brown.words(categories='news'))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "brown.sents(categories='news')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(brown.sents(categories='news'))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "' '.join(brown.sents(categories='news')[0])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "brown.sents(categories=['news', 'editorial', 'reviews'])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(brown.sents(categories=['news', 'editorial', 'reviews']))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from nltk.corpus import brown"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "news_text = brown.words(categories='news')\nnews_text"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(news_text)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "fdist = nltk.FreqDist([w.lower() for w in news_text])\nfdist"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "modals = ['can', 'could', 'may', 'might', 'must', 'will']"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "for m in modals:\n    print m + ':', fdist[m],"
        }, 
        {
            "source": "#### Your Turn\n\n- Choose a different section of the Brown Corpus, and adapt the preceding example to count a selection of wh words, such as what, when, where, who and why.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "other_text = brown.words(categories=['news', 'humor', 'fiction'])\nother_text"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wh_words = ['what', 'when', 'where', 'who', 'why']"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "fdist2 = nltk.FreqDist([w.lower() for w in other_text if 'wh' in w])\nfdist2"
        }, 
        {
            "source": "### Regular Expressions\n\n- If you do not use regular expressions, other things stick together.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import re"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "fdist3 = nltk.FreqDist([word.lower() for word in other_text \n                                    for wh_word in wh_words \n                                    if re.search('^'+wh_word.lower()+'$', \n                                                 word.lower())])\nfdist3"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "fdist3.N()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "other_text"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(other_text)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "lst = []\nfor word in other_text:\n    for wh_word in wh_words:\n        if wh_word.lower() in word.lower():\n            lst.append(word)\n        \nlen(lst)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# 'news', 'can'\n# 'hobbies', 'can'\ncfd = nltk.ConditionalFreqDist((genre, word)\n                               for genre in brown.categories()\n                               for word in brown.words(categories=genre))\ncfd"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(cfd)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "type(cfd)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd2 = nltk.ConditionalFreqDist()\nlen(cfd2)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', \n          'humor']"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "modals = ['can', 'could', 'may', 'might', 'must', 'will']"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd.tabulate(conditions=genres, samples=modals)"
        }, 
        {
            "source": "### Reuters Corpus\n\n- 10, 788 news documents, 1.3 million words\n- 90 topics\n- two groups: \"training\", \"test\"\n- test/14826\n- training and testing: Use the algorithm to automatically find the topic in the document.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from nltk.corpus import reuters"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(reuters.fileids())"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "reuters.fileids()[::500]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "reuters.categories()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(reuters.categories())"
        }, 
        {
            "source": "- Reuters CorpusCategories overlap each other.\n- Because news stories often cover a variety of topics.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "reuters.categories('training/9865')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "reuters.categories(['training/9865', 'training/9880'])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "reuters.fileids('barley')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "reuters.fileids(['barley', 'corn'])[::30]"
        }, 
        {
            "source": "- You can specify words or sentences. We want in fiels or categories.\n\n- The first few words in each text are titles. It is stored in upper case.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "reuters.words('training/9865')[:14]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(reuters.words('training/9865'))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "reuters.words(['training/9865', 'training/9880'])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(reuters.words(['training/9865', 'training/9880']))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "reuters.words(categories='barley')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(reuters.words(categories='barley'))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "reuters.words(categories=['barley', 'corn'])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(reuters.words(categories=['barley', 'corn']))"
        }, 
        {
            "source": "### Inaugural Address Corpus\n\n- word offset", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from nltk.corpus import inaugural"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "inaugural.fileids()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "[fileid[:4] for fileid in inaugural.fileids()]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The other way, the above method seems easier\n[fileid.split('-')[0] for fileid in inaugural.fileids()]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd = nltk.ConditionalFreqDist((target, fileid[:4])\n                               for fileid in inaugural.fileids()\n                               for w in inaugural.words(fileid)\n                               for target in ['america', 'citizen']\n                               if w.lower().startswith(target))\nlen(cfd)"
        }, 
        {
            "source": "#### Plot of a conditional frequency distribution\n\n- All words in the Inaugural Address Corpus that begin with america or citizen are counted;\n- separate counts are kept for each address;\n- these are plotted so that trends in usage over time can be observed;\n- counts are not normalized for document length.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "%matplotlib inline\ncfd.plot()"
        }, 
        {
            "source": "### Annotated Text Corpora\n\n- Corpus annotation is a special tagging task for the body of corpus to maximize the utilization of corpus. In other words, linguistic information can be given to primitive corpus to make annotation corpus.\n- Many text corpuses include linguistic annotations, partial tag expressions of speech, things called independent, syntactic structures, and semantic roles.\n- NLTK offers a variety of approaches to these corpus.\n- The data package contains a corpus. And corpus samples are freely downloadable for technical use and research use.\n- [More information](http://www.nltk.org/data)\n\n#### Table 2-2 Some of the corpora and corpus samples distributed with NLTK\n\nCorpus | Compiler | Contents\n--- | --- | ---\nBrown Corpus | Francis, Kucra | 15 genres, 1.15M words, tagged, categorized\nCESS Treebanks | CLiC-UB | 1M words, tagged and parsed(Catalan, Spanish)\n\u22ef | \u22ef | \u22ef", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Corpora in Other Languages\n\n- NLTK is made for many languages with corpus?\n- Despite many cases, you need to learn how to manipulate character encoding in Python. And let's use them.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "help(nltk.corpus.cess_esp)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Another corpus\nnltk.corpus.cess_esp.words()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(nltk.corpus.cess_esp.words())"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Other corpus\nnltk.corpus.floresta.words()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(nltk.corpus.floresta.words())"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "nltk.corpus.indian.words('hindi.pos')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(nltk.corpus.indian.words('hindi.pos'))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "nltk.corpus.udhr.fileids()[::50]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "nltk.corpus.udhr.words('Javanese-Latin1')[11:]"
        }, 
        {
            "source": "- udhr: Universal Declaration of Human Rights, 300 languages.\n- fileids: Contains information about the character encoding used in the file for this corpus. Things like UTF8 or Latin1.\n- Contains the condition frequency distribution to examine different points in the length of words in udhr corpus in selected languages.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from nltk.corpus import udhr"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "languages = ['Chickasaw', 'English', 'German_Deutsch',\n             'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd = nltk.ConditionalFreqDist((lang, len(word))\n                               for lang in languages\n                               for word in udhr.words(lang + '-Latin1'))\nlen(cfd)"
        }, 
        {
            "source": "#### Figure 2-2. Cumulative word length distribution\n\n- Six translations of the Universal Declaration of Human Rights are processed\n- this graph shows that words having five or fewer letters account for about 80% of Ibibio text, 60% of German text, and 25% of Inuktitut text.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Condition frequency distribution\ncfd.plot(cumulative=True)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd.plot(cumulative=False)"
        }, 
        {
            "source": "#### Your Turn\n\n- Pick a language of interest in udhr.fileids() and define a variable raw_text = udhr.raw(Language-Latin1)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "raw_text = udhr.raw(['Chickasaw-Latin1', 'English-Latin1'])\nnltk.FreqDist(raw_text).plot()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "nltk.FreqDist(raw_text).plot(cumulative=True)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "raw_text = udhr.raw(['Chickasaw-Latin1', 'English-Latin1', \n                     'German_Deutsch-Latin1'])\nnltk.FreqDist(raw_text).plot()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# nltk.FreqDist(raw_text).plot(cumulative=True)"
        }, 
        {
            "source": "- Unfortunately, for many languages, actual corpus is not yet available.\n- There is a lack of industrial support to develop for insufficient government or language resources.\n- There are no definite authoring systems in several languages.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Text Corpus Structure\n\n- A summary of the basic corpora functionality is shown in Table 2-3.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "help(nltk.corpus.reader)"
        }, 
        {
            "source": "#### Table 2-3. Basic corpus functionality defined in NLTK\n\nExample | Description\n--- | ---\nfileids() | The files of the corpus\nfileids([categories]) | The files of the corpus corresponding to these categories\ncategories() | The categories of the corpus\ncategories([fileids]) | The categories of the corpus corresponding to these files\nraw() | The raw content of the corpus\nraw(fileids=[f1, f2, f3]) | The raw content of the specified files\nraw(categories=[c1, c2]) | The raw content of the specified categoreis\nwords() | The words of the whole corpus\nwords(fileids=[f1, f2, f3]) | The words of the specified fileids\nwords(categories=[c1, c2]) | The words of the specified categoreis\nsents() | The sentences fo the specified categories\nsents(fileids=[f1, f2, f3]) | The sentences of the specified fileids\nsents(categories=[c1, c2]) | The sentences of the specified categories\nabspath(fileid) | The location of the given file on disk\nencoding(fileid) | The encoding of the file(if known)\nopen(fileid) | Open a stream for reading the given corpus file\nroot() | The path to the root of locally installed corpus\nreadme() | The contents of the README file of the corpus", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "raw = gutenberg.raw('burgess-busterbrown.txt')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "raw[1:20]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(raw)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "words = gutenberg.words('burgess-busterbrown.txt')\nwords[1:20]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "' '.join(words[1:20])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sents = gutenberg.sents('burgess-busterbrown.txt')\nsents[1:5]"
        }, 
        {
            "source": "### Loading Your Own Corpus\n\n- If you have your own collection of text files, you can access them using the methods already discussed. You can easily load them and NLTK's PlaintextCorpusReader will help you.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from nltk.corpus import PlaintextCorpusReader"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "corpus_root = '/usr/share/dict'"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wordlists = PlaintextCorpusReader(corpus_root, '.*')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wordlists.fileids()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!ls /usr/share/dict"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ws = wordlists.words('connectives')\nws"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "type(ws)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(wordlists.words('connectives'))"
        }, 
        {
            "source": "#### Another example. copy of Penn Treebank", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from nltk.corpus import BracketParseCorpusReader"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "corpus_root = '/Users/re4lfl0w/nltk_data/corpora/treebank/parsed/'"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "file_pattern = r'.*00[0-1][0-9]\\.prd'"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ptb = BracketParseCorpusReader(corpus_root, file_pattern)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ptb.fileids()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(ptb.sents())"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ptb.sents(fileids='wsj_0005.prd')[1]"
        }, 
        {
            "source": "<a id = \"Distributions\"></a>\n\n## <span style=\"color:#0b486b\">2. Conditional Frequency Distributions</span>\n\n- Text corpus is divided into various categories (genre, topic, author, etc.)\n- You can maintain a frequency distribution that is divided for each category.\n- conditional frequency distribution: A collection of frequency distributions. Each by different conditions.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Conditions and Events\n\n- The frequency distribution counts the observed events.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "text = ['The', 'Fulton', 'Country', 'Grand', 'Jury', 'said']"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# (condition, event)\npairs = [('news', 'The'), ('news', 'Fulton'), ('news', 'Country')]"
        }, 
        {
            "source": "- Brown Corpus by genre: 15 conditions, 1,161, 192 events(one per word)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Counting Words by Genre\n\n- FreqDist (): accepts a simple list as input\n- ConditionalFreqDist (): received list pair", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from nltk.corpus import brown"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd = nltk.ConditionalFreqDist((genre, word)\n                               for genre in brown.categories()\n                               for word in brown.words(categories=genre))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd.conditions()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd.plot()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "genre_word = [(genre, word)\n              for genre in ['news', 'romance']\n              for word in brown.words(categories=genre)]\nlen(genre_word)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "genre_word[:4]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "genre_word[-4:]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd = nltk.ConditionalFreqDist(genre_word)\ncfd"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd.conditions()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd['news']"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd['romance']"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "list(cfd['romance'])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd['romance']['could']"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd.plot()"
        }, 
        {
            "source": "### Plotting and Tabulating Distributions\n\n- When combining two or more frequency distributions,\n- It is easy to initialize, and CondtionalFreqDist provides some useful methods for table drawing and graph drawing.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from nltk.corpus import inaugural"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd = nltk.ConditionalFreqDist((target, fileid[:4])\n                               for fileid in inaugural.fileids()\n                               for w in inaugural.words(fileid)\n                               for target in ['america', 'citizen']\n                               if w.lower().startswith(target))\n\ncfd.plot()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from nltk.corpus import udhr"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "languages = ['Chickasaw', 'English', 'German_Deutsch',\n             'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd = nltk.ConditionalFreqDist((lang, len(word))\n                               for lang in languages\n                               for word in udhr.words(lang + '-Latin1'))\ncfd.plot()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd.plot(cumulative=True)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd.tabulate(conditions=['English', 'German_Deutsch'],\n             samples=range(10), cumulative=True)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# conditions\ncfd.tabulate(samples=range(10), cumulative=True)"
        }, 
        {
            "source": "### Generating Random Text with Bigrams\n\n- You can use conditional frequency distributions to create bigrams tables.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sent = ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven',\n        'and', 'the', 'earth', '.']"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# generator \nlist(nltk.bigrams(sent))"
        }, 
        {
            "source": "#### Example 2-1. Generating random text\n\n- This program obtains all bigrams from the text of the book of Genesis,\n- then constructs a conditional frequency distribution to record which words are most likely to follow a given word;\n- e.g., after the word living, the most likely word is creature;\n- the generate_model() function uses this data, and a seed word, to generate random text.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def generate_model(cfdist, word, num=15):\n    for i in range(num):\n        print word,\n        word = cfdist[word].max()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "text = nltk.corpus.genesis.words('english-kjv.txt')\ntext"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "bigrams = nltk.bigrams(text)\nlist(bigrams)[:10]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd = nltk.ConditionalFreqDist(bigrams)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print cfd['living']"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "generate_model(cfd, 'living', num=5)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "generate_model(cfd, 'living', num=2)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# living\ncfd['living'].max()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd['creature'].max()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd['living']"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "' '.join(cfd['living'])"
        }, 
        {
            "source": "- Conditional frequency distributions are a useful data structure for many NLP tasks", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Table 2-4. NLTK's conditional frequency distributions\n\nExample | Description\n--- | ---\ncfdist = ConditionalFreqDist(pairs) | Create a conditional frequency distribution from a list of pairs\ncdist.conditions() | Alphabetically sorted list of conditions\ncdist[condition] | The frequency distribution for this condition\ncdist\\[condition\\]\\[sample\\] | Frequency for the given sample for this condition\ncfdist.tabulate() | Tabulate the conditional frequency distribution\ncfdist.tabulate(samples, conditions) | Tabulation limited to the specified samples and conditions\ncfdist.plot() | Graphical plot of the conditional frequency distribution\ncfdist.plot(samples, conditions) | Graphical plot limited to the specified samples and conditions\ncfdist1 < cfdist2 | Test if samples in cfdist1 occur less frequently than in cfdist2", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id = \"Code\"></a>\n\n## <span style=\"color:#0b486b\">3. More Python: Reusing Code</span>\n\n  - Python functions", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Functions\n\n- function\n- parameters\n- return value", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from __future__ import division"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def lexical_diversity(text):\n    return len(text) / len(set(text))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "s = 'hello world hello'"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "lexical_diversity(s)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(s)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(set(s))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(s) / len(set(s))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def lexical_diversity(my_text_data):\n    word_count = len(my_text_data)\n    vocab_size = len(set(my_text_data))\n    diversity_score = word_count / vocab_size\n    return diversity_score"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "lexical_diversity(s)"
        }, 
        {
            "source": "- local variables", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Example 2-2 A Python function\n\n- This function tries to work out the plural form of any English noun", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def plural(word):\n    if word.endswith('y'):\n        return word[:-1] + 'ies'\n    elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:\n        return word + 'es'\n    elif word.endswith('an'):\n        return word[:-2] + 'en'\n    else:\n        return word + 's'"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "plural('fairy')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "plural('woman')"
        }, 
        {
            "source": "<a id = \"Resources\"></a>\n\n## <span style=\"color:#0b486b\">4. Lexical Resources</span>\n\n- lexicon or lexical resource: collection of words and/or phrases along with associated informatin, such as part-of-speech and sense definitions\n\n```python\nvocab = sorted(set(my_text))\nword_freq = FreqDist(my_text)\n```\n\n- vocab, word_freq: Simple vocabulary resources\n- lexical entries: Vocabulary entry\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Wordlist Corpora\n\n- NLTK: Has a few corpus, but nothing better than the wordlists.\n- Words Corpus /usr/dict/words", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Example 2-3 Filtering a text\n\n- text Of words.\n- Erase all items. It occurs in the existing wordlist. Only unusual or misspelled words remain.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def unusual_words(text):\n    text_vocab = set(w.lower() for w in text if w.isalpha())\n    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n#     text_vocab - english_vocab\n    unusual = text_vocab.difference(english_vocab)\n    return sorted(unusual)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(nltk.corpus.words.words())"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "nltk.corpus.words.words()[::10000]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "unusual_words(nltk.corpus.gutenberg.words('austen-sense.txt'))[::100]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "unusual_words(nltk.corpus.nps_chat.words())[::100]"
        }, 
        {
            "source": "- stopwords: Words that appear very high, the, to, also\n\u00a0\u00a0 - Sometimes I want to be out of this document.\n\u00a0\u00a0 - And do the rest.\n- Usually have few vocabulary contents. They exist in the text fails to distinguish them from other texts.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from nltk.corpus import stopwords"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "stopwords.words('english')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def content_fraction(text):\n    stopwords = nltk.corpus.stopwords.words('english')\n    content = [w for w in text if w.lower() not in stopwords]\n    return len(content) / len(text)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "content_fraction(nltk.corpus.reuters.words())"
        }, 
        {
            "source": "- help with stopsrods, filter text from 3 words\n- Combine two different corpus.\n- lexical resources are used to filter the content of text corpus.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "- wordlist: word puzzles, It is useful to solve.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "puzzle_letters = nltk.FreqDist('egivrvonl')\npuzzle_letters"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "obligatory = 'r'"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wordlist = nltk.corpus.words.words()\nlen(wordlist)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "[w for w in wordlist if len(w) >= 6\n                        and obligatory in w\n                        and nltk.FreqDist(w) <= puzzle_letters]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len([w for w in wordlist if len(w) >= 6\n                        and obligatory in w])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "names = nltk.corpus.names\nnames"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "names.fileids()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "male_names = names.words('male.txt')\nmale_names[::100]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(male_names)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "female_names = names.words('female.txt')\nfemale_names[::100]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(female_names)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "[w for w in male_names if w in female_names][::50]"
        }, 
        {
            "source": "#### Figure 2-7. Conditional frequency distribution\n\n- This plot shows the number of female and male names ending with each letter of the alphabet\n- most names ending with a, e, or i are female;\n- names ending in h and l are equally to be male or female;\n- names ending in k, o, r, s, and t are likely to be male.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd = nltk.ConditionalFreqDist((fileid, name[-1])\n                                for fileid in names.fileids()\n                                for name in names.words(fileid))\ncfd.plot()"
        }, 
        {
            "source": "### A Prononcing Dictionary\n\n- Some rich lexical resource table\n- When you include words and some attributes in each row\n- NLTK adds the CMU Pronouncing Dictionary.\n- It was designed by speech synthesizers (synthesizers) for use.\n- For each word, these vocabularies provide a list of voice codes.\n- distinguish labels with each contrasting sound.\n- [Arpabet - Wikipedia, the free encyclopedia](http://en.wikipedia.org/wiki/Arpabet)\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "entries = nltk.corpus.cmudict.entries()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(entries)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "for entry in entries[39943:39951]:\n    print entry"
        }, 
        {
            "source": "- For each word, the vocabulary provides a sound code for all lists.\n- Clear label For contrasting sound: phones\n- fire: 2 pronunciation: F AY1 R, F AY1 ERO\n- The symbols in the CMU Pronouncing Dictionary \n- http://en.wikipedia.org/wiki/Arpabet\n\n\nEach entry consists of two parts.\nEach person can proceed with a more complex version for the for statement.\n- As we loop around, the first part is the word, the second part is pron", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "for word, pron in entries:\n    if len(pron) == 3:\n        ph1, ph2, ph3 = pron\n        if ph1 == 'P' and ph3 == 'T':\n            print word, ph2"
        }, 
        {
            "source": "- The show program scanned what the vocabulary consists of 3 phones.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "syllable = ['N', 'IHO', 'K', 'S']"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Why? \n[word for word, pron in entries if pron[-4:] == syllable]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "entries[10]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "entries[10][-1:]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "entries[10][1][-2:]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "entries[10][1][-2]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "entries[10][1][-2:] == 'AH0'"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "entries[10][1][-2] == 'AH0'"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "entries[10][-4:]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "[w for w, pron in entries if pron[-1] == 'M' and w[-1] == 'n']"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sorted(set(w[:2] for w, pron in entries if pron[0] == 'N' and w[0] != 'n'))"
        }, 
        {
            "source": "#### pron, phone\n\n- What do you mean?\n- 0, 1, 2, What does this mean?\n- 1: primary stress\n- 2: secondary stress\n- 0: no stress", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def stress(pron):\n    ''' input -> u'AO1', u'R', u'ER0', u'Z'\n        Check each of A01, R, and ER0 for each number\n\u00a0\u00a0\u00a0\u00a0Output only numbers\n    '''\n    return [char for phone in pron for char in phone if char.isdigit()]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(entries)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len([w for w, pron in entries if stress(pron) == ['0', '1', '0', '2', '0']])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "[w for w, pron in entries if stress(pron) == ['0', '1', '0', '2', '0']][::50]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "[w for w, pron in entries if stress(pron) == ['0', '2', '0', '1', '0']][::50]"
        }, 
        {
            "source": "- stress(): list comprehension. Duplicate nested for loop.\n- p words consisting of three sounds\n- group them according to their first and last sounds", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "p3 = [(pron[0] + '-' + pron[2], word)\n      for (word, pron) in entries\n      if pron[0] == 'P' and len(pron) == 3]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "p3[::20]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd = nltk.ConditionalFreqDist(p3)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cfd.conditions()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "for template in cfd.conditions():\n    if len(cfd[template]) > 10:\n        words = cfd[template].keys()\n        wordlist = ' '.join(words)\n        print template, wordlist[:66] + '...'"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "prondict = nltk.corpus.cmudict.dict()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(prondict)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "prondict['fire']"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "prondict['blog'] = [['B', 'L', 'AA1', 'G']]\nprondict['blog']"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "text = ['natural', 'language', 'processing']"
        }, 
        {
            "source": "- You can use the linguistic resources of processing text. Filterable with a few linguistic properties (like nouns) or to all words in text\n- For example, the text-to-speech function looks at the text of each word in the pronunciation dictionary.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "[ph for w in text for ph in prondict[w][0]]"
        }, 
        {
            "source": "### Comparative Wordlists\n\n- comparative wordlist: Can the table vocabulary be comparable?\n- Swadesh wordlists in various languages: list of 200 common words.\n- ISO 639 two-letter code.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from nltk.corpus import swadesh"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "swadesh.fileids()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "swadesh.words('en')[:30]"
        }, 
        {
            "source": "- Use the entries () method in various languages and access to the etymology of words. Select a list of languages.\n- It can be changed to a simple dictionary.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "fr2en = swadesh.entries(['fr', 'en'])\nfr2en[:30]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "translate = dict(fr2en)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "translate['chien']"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "translate['jeter']"
        }, 
        {
            "source": "- Let's add two dictionaries.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "de2en = swadesh.entries(['de', 'en'])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "es2en = swadesh.entries(['es', 'en'])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "translate.update(dict(de2en))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "translate.update(dict(es2en))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "translate['Hund']"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "translate['perro']"
        }, 
        {
            "source": "- You can compare words in various Germanic and Romance languages.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "languages = ['en', 'de', 'nl', 'es', 'fr', 'pt', 'la']"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "for i in [139, 140, 141, 142]:\n    print swadesh.entries(languages)[i]"
        }, 
        {
            "source": "### Shoebox and Toolbox Lexicons\n\n- The toolbox is for data manipulated by verbally used in the most popular tool.\n- The Toolbox consists of a collection of entries.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from nltk.corpus import toolbox"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "toolbox.entries('rotokas.dic')[:3]"
        }, 
        {
            "source": "- It consists of attribute, value pair.\n- ('ps', 'V'): part-of-speech, verb\n- ('ge', 'gag'): gloss-into-English\n- The structure file of the unlocked Toolbox becomes difficult.\n- XML provides a powerful way to process this kind of corpus\n- Rotokas is a notable name in 12 phonemic inventions.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id = \"WordNet\"></a>\n\n## <span style=\"color:#0b486b\">5. WordNet</span>\n\n- WordNet: Semantically oriented English dictionary.\n- NLTK contains ENGLISH WordNet 155,287 words and 117,659 synonym sets.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Senses and Synonyms\n\n\n- a. Benz is credited with the invention of the motorcar.\n- b. Benz is credited with the invention of the automobile.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "- motorcar, automobile: synonyms", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from nltk.corpus import wordnet as wn"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wn.synsets('motorcar')"
        }, 
        {
            "source": "- car.n.01: synset(synonym set). collection of synonymous words(or \"lemmas\") entry?", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wn.synset('car.n.01').lemma_names()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wn.synset('car.n.01').definition()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wn.synset('car.n.01').examples()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# get all the lemmas for a given synset\nwn.synset('car.n.01').lemmas()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# look up a particular lemma\nwn.lemma('car.n.01.automobile')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# get the synset corresponding to a lemma\nwn.lemma('car.n.01.automobile').synset()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# get the \"name\" of a lemma\nwn.lemma('car.n.01.automobile').name()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wn.synsets('car')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "for synset in wn.synsets('car'):\n    print synset.lemma_names()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wn.lemmas('car')"
        }, 
        {
            "source": "### The WordNet Hierarchy\n\n- WordNet synsets: Matches abstract concept. It does not always match words used in English.\n- Their concepts are linked together hierarchically.\n- unique beginners.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "- Wordnet: Makes it easy to translate between concepts.\n- hyponyms: segment", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "motorcar = wn.synset('car.n.01')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "types_of_motorcar = motorcar.hyponyms()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "types_of_motorcar[26]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "types_of_motorcar"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sorted([lemma.name() for synset in types_of_motorcar for lemma in synset.lemmas()])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "motorcar.hypernyms()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "paths = motorcar.hypernym_paths()\npaths"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(paths)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "[synset.name() for synset in paths[0]]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "[synset.name() for synset in paths[1]]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Get the most general hypernyms(or root hypernyms)\nmotorcar.root_hypernyms()"
        }, 
        {
            "source": "### More Lexical Relations\n\n- lexical relations: Hypernym, hyponym\n- [meronyms](http://en.wikipedia.org/wiki/Meronymy): Semantic relations of linguistics\n- [Holonymy](http://en.wikipedia.org/wiki/Holonymy): semantic relation\n- substance_meronyms()", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wn.synset('tree.n.01').part_meronyms()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wn.synset('tree.n.01').substance_meronyms()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wn.synset('tree.n.01').member_holonyms()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "for synset in wn.synsets('mint', wn.NOUN):\n#     print synset.name() + ':', synset.definition()\n    print '{0}: {1}'.format(synset.name(), synset.definition())"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wn.synset('mint.n.04').part_holonyms()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wn.synset('mint.n.04').substance_holonyms()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wn.synset('walk.v.01').entailments()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wn.synset('eat.v.01').entailments()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wn.synset('tease.v.03').entailments()"
        }, 
        {
            "source": "- antonymy: Anti-wisdom", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Supply and Demand\nwn.lemma('supply.n.02.supply').antonyms()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wn.lemma('rush.v.01.rush').antonyms()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wn.lemma('horizontal.a.01.horizontal').antonyms()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wn.lemma('staccato.r.01.staccato').antonyms()"
        }, 
        {
            "source": "#### dir\n\n- Determine which method is defined", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dir(wn.synset('harmony.n.02'))"
        }, 
        {
            "source": "### Semantic Similarity\n\n- synsets are linked by complex network vocabulary relationships.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "right = wn.synset('right_whale.n.01')\nright"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "orca = wn.synset('orca.n.01')\norca"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "minke = wn.synset('minke_whale.n.01')\nminke"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tortoise = wn.synset('tortoise.n.01')\ntortoise"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "novel = wn.synset('novel.n.01')\nnovel"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "right.lowest_common_hypernyms(minke)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "right.lowest_common_hypernyms(orca)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "right.lowest_common_hypernyms(tortoise)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "right.lowest_common_hypernyms(novel)"
        }, 
        {
            "source": "- baleen: very specific\n- vertebrate: very common\n- entity: completely general", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wn.synset('minke_whale.n.01').min_depth()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wn.synset('baleen_whale.n.01').min_depth()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wn.synset('whale.n.02').min_depth()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wn.synset('vertebrate.n.01').min_depth()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wn.synset('entity.n.01').min_depth()"
        }, 
        {
            "source": "- Similarity measure\n- path_similarity: Scores at 0-1.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "right.path_similarity(minke)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "right.path_similarity(orca)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "right.path_similarity(tortoise)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "right.path_similarity(novel)"
        }, 
        {
            "source": "#### Several other similarity measures are available", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "help(wn)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "nltk.corpus.verbnet"
        }, 
        {
            "source": "<a id = \"Summary\"></a>\n\n## <span style=\"color:#0b486b\">6. Summary</span>\n\n- A text corpus is a large, structured collection of texts. NLTK comes with many corpora, e.g., the Brown Corpus, nltk.corpus.brown\n- Some text corpora are categorized e.g., by genre or topic; sometimes the categories of a corpus overlap each other.\n- A conditional frequency distribution is a collection of frequency distributions, each one for a different condition. They can be used for counting word frequencies, given a context or a genre.\n- Python programs more than a few lines long should be entered using a text editor, saved to a file with a.py extension, and accessed using an import statement\n- Python functions permit you to associate a name with a particular block of code, and reuse that code as often as necessary.\n- Some functions, known as \"methods\", are associated with an object, and we give the object name followed by a period followed by the method name, like this: **x.funct(y)**, e.g., **word.isalpha()**\n- To find out about somre variable v, type help(v) in the Python interactive interpreter to read the help entry for this kind of object\n- WordNet is a semantically oriented dictionary of English, consisting of synonymsets--or synsets--and organized into a network\n- Some functions are not available by default, but must be accessed using Python's import statement.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id = \"reading\"></a>\n\n## <span style=\"color:#0b486b\">7. Further Reading</span>\n\n- [Extra materials](http://www.nltk.org)\n- [Corpus HOWTO](http://www.nltk.org/howto)\n- Linguistic Data Consortium(LDC)\n- European Language Resource Agency(ELRA)\n- [OLAC Meta data](http://www.language-archives.org)\n- [Ethnologue](http://www.ethnologue.com)\n- [WordNet](http://globalwordnet.org)", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2.7", 
            "name": "python2", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.15", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}