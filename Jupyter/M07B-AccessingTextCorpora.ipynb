{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLIP(01):  Advanced Data Science\n",
    "**(Module 07: Natural Language Processing)**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use, change and distribute this package.\n",
    "- If you found any issue/bug for this document, please submit an issue at [tulip-lab/mds](https://github.com/tulip-lab/mds/issues)\n",
    "\n",
    "Prepared by and for \n",
    "**Student Members** |\n",
    "2006-2019 [TULIP Lab](http://www.tulip.org.au)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session B - Accessing Text Corpora and Lexical Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1 [Accessing Text Corpora](#Corpora)\n",
    "* Gutenberg Corpus\n",
    "* Web and Chat Text\n",
    "* Brown Corpus\n",
    "* Regular Expression\n",
    "* Reuters \n",
    "* Inaugural Address Corpus\n",
    "* Corpora in Other Languages\n",
    "* Text Corpus Structure\n",
    "\n",
    "\n",
    "2 [Conditional Frequency Distributions](#Distributions)\n",
    "* Conditions and Events\n",
    "* Counting Words by Genre\n",
    "* Plotting and Tabulating Distributions\n",
    "\n",
    "3 [More Python: Reusing Code](#Code)\n",
    "* Creating Programs with a Text Editor\n",
    "* Functions\n",
    "* Modules\n",
    "\n",
    "4 [Lexical Resources](#Resources)\n",
    "* Wordlist Corpora\n",
    "* Comparative Wordlists\n",
    "* Shoebox and Toolbox Lexicons\n",
    "\n",
    "5 [WordNet](#WordNet)\n",
    "* Senses and Synonyms\n",
    "* The WordNet Hierarchy\n",
    "* More Lexical Relations\n",
    "* Semantic Similarity\n",
    "\n",
    "6 [Summary](#Summary)\n",
    "\n",
    "7 [Further Reading](#reading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"Corpora\"></a>\n",
    "\n",
    "## <span style=\"color:#0b486b\">1. Accessing Text Corpora</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gutenberg Corpus\n",
    "\n",
    "- http://www.gutenberg.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plural(word):\n",
    "    if word.endswith('y'):\n",
    "        return word[:-1] + 'ies'\n",
    "    elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:\n",
    "        return word + 'es'\n",
    "    elif word.endswith('an'):\n",
    "        return word[:-2] + 'en'\n",
    "    else:\n",
    "        return word + 's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(corpus.gutenberg.fileids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "emma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(nltk.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))\n",
    "emma.concordance('surprize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emma = gutenberg.words('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpret the for statement\n",
    "\n",
    "- num_chars: all characters\n",
    "- num_words: all words\n",
    "- num_sents: Any sentences or phrases ??\n",
    "- num_vocab: the only word in all words\n",
    "\n",
    "\n",
    "- All characters / All words, All words / All sentences or lists?\n",
    "- all words / all unique words, fileid\n",
    "- There's a description below.\n",
    "- average word length\n",
    "- average sentence length\n",
    "- On average how much each word appears in the text (verbal diversity score)\n",
    "\n",
    "> Note: What are you going to do with this? I have a purpose but I do not understand the purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for fileid in gutenberg.fileids():\n",
    "    num_chars = len(gutenberg.raw(fileid))\n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))\n",
    "    num_vocab = len(set([w.lower() for w in gutenberg.words(fileid)]))\n",
    "    print int(num_chars/num_words), int(num_words/num_sents), \\\n",
    "    int(num_words/num_vocab), fileid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(gutenberg.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(gutenberg.raw('austen-emma.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(gutenberg.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gutenberg.words('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_words2 = len(gutenberg.words('austen-emma.txt'))\n",
    "num_words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_vocab2 = len(set([w.lower() for w in \n",
    "                      gutenberg.words('austen-emma.txt')]))\n",
    "num_vocab2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How many of the words are the only words?\n",
    "num_words2 / num_vocab2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentence: sentence\n",
    "# utterance: words\n",
    "help(gutenberg.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gutenberg.sents('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(gutenberg.sents('austen-emma.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- average word length: really 3, not 4, since the num_chars variable counts space characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sents(): divides the text up into its sentences, where each sentence is a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how many letters occur in the text\n",
    "len(gutenberg.raw('blake-poems.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')\n",
    "macbeth_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(macbeth_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "macbeth_sentences[1037]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "longest_len = max([len(s) for s in macbeth_sentences])\n",
    "longest_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[s for s in macbeth_sentences if len(s) == longest_len][0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "' '.join([s for s in macbeth_sentences if len(s) == longest_len][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most NLTK corpus readers include various approaches such as `words ()`, `raw ()`, and `sents ()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web and Chat Text\n",
    "\n",
    "- Gutenberg contains thousands of books: Represents published literature.\n",
    "- This is important. There are few formal language elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(webtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for fileid in webtext.fileids():\n",
    "    print fileid, webtext.raw(fileid)[:65], '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "webtext.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import nps_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 706 posts\n",
    "# 10/19\n",
    "# 20s\n",
    "chatroom = nps_chat.posts('10-19-20s_706posts.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chatroom[123]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brown Corpus\n",
    "\n",
    "- by genre. news, editorial, etc.\n",
    "- Where is Brown?\n",
    "- Where did you make a corpus?\n",
    "- I think this is the point ... If you made it on Reuters, you made it for your own news?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brown.words(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(brown.words(categories='news'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brown.sents(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(brown.sents(categories='news'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "' '.join(brown.sents(categories='news')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brown.sents(categories=['news', 'editorial', 'reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(brown.sents(categories=['news', 'editorial', 'reviews']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_text = brown.words(categories='news')\n",
    "news_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(news_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist([w.lower() for w in news_text])\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for m in modals:\n",
    "    print m + ':', fdist[m],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Turn\n",
    "\n",
    "- Choose a different section of the Brown Corpus, and adapt the preceding example to count a selection of wh words, such as what, when, where, who and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other_text = brown.words(categories=['news', 'humor', 'fiction'])\n",
    "other_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wh_words = ['what', 'when', 'where', 'who', 'why']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdist2 = nltk.FreqDist([w.lower() for w in other_text if 'wh' in w])\n",
    "fdist2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions\n",
    "\n",
    "- If you do not use regular expressions, other things stick together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdist3 = nltk.FreqDist([word.lower() for word in other_text \n",
    "                                    for wh_word in wh_words \n",
    "                                    if re.search('^'+wh_word.lower()+'$', \n",
    "                                                 word.lower())])\n",
    "fdist3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdist3.N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(other_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst = []\n",
    "for word in other_text:\n",
    "    for wh_word in wh_words:\n",
    "        if wh_word.lower() in word.lower():\n",
    "            lst.append(word)\n",
    "        \n",
    "len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'news', 'can'\n",
    "# 'hobbies', 'can'\n",
    "cfd = nltk.ConditionalFreqDist((genre, word)\n",
    "                               for genre in brown.categories()\n",
    "                               for word in brown.words(categories=genre))\n",
    "cfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(cfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(cfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd2 = nltk.ConditionalFreqDist()\n",
    "len(cfd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', \n",
    "          'humor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reuters Corpus\n",
    "\n",
    "- 10, 788 news documents, 1.3 million words\n",
    "- 90 topics\n",
    "- two groups: \"training\", \"test\"\n",
    "- test/14826\n",
    "- training and testing: Use the algorithm to automatically find the topic in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(reuters.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuters.fileids()[::500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuters.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(reuters.categories())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reuters CorpusCategories overlap each other.\n",
    "- Because news stories often cover a variety of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuters.categories('training/9865')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuters.categories(['training/9865', 'training/9880'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuters.fileids('barley')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuters.fileids(['barley', 'corn'])[::30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can specify words or sentences. We want in fiels or categories.\n",
    "\n",
    "- The first few words in each text are titles. It is stored in upper case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuters.words('training/9865')[:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(reuters.words('training/9865'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuters.words(['training/9865', 'training/9880'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(reuters.words(['training/9865', 'training/9880']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuters.words(categories='barley')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(reuters.words(categories='barley'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuters.words(categories=['barley', 'corn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(reuters.words(categories=['barley', 'corn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inaugural Address Corpus\n",
    "\n",
    "- word offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[fileid[:4] for fileid in inaugural.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The other way, the above method seems easier\n",
    "[fileid.split('-')[0] for fileid in inaugural.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist((target, fileid[:4])\n",
    "                               for fileid in inaugural.fileids()\n",
    "                               for w in inaugural.words(fileid)\n",
    "                               for target in ['america', 'citizen']\n",
    "                               if w.lower().startswith(target))\n",
    "len(cfd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot of a conditional frequency distribution\n",
    "\n",
    "- All words in the Inaugural Address Corpus that begin with america or citizen are counted;\n",
    "- separate counts are kept for each address;\n",
    "- these are plotted so that trends in usage over time can be observed;\n",
    "- counts are not normalized for document length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotated Text Corpora\n",
    "\n",
    "- Corpus annotation is a special tagging task for the body of corpus to maximize the utilization of corpus. In other words, linguistic information can be given to primitive corpus to make annotation corpus.\n",
    "- Many text corpuses include linguistic annotations, partial tag expressions of speech, things called independent, syntactic structures, and semantic roles.\n",
    "- NLTK offers a variety of approaches to these corpus.\n",
    "- The data package contains a corpus. And corpus samples are freely downloadable for technical use and research use.\n",
    "- [More information](http://www.nltk.org/data)\n",
    "\n",
    "#### Table 2-2 Some of the corpora and corpus samples distributed with NLTK\n",
    "\n",
    "Corpus | Compiler | Contents\n",
    "--- | --- | ---\n",
    "Brown Corpus | Francis, Kucra | 15 genres, 1.15M words, tagged, categorized\n",
    "CESS Treebanks | CLiC-UB | 1M words, tagged and parsed(Catalan, Spanish)\n",
    "⋯ | ⋯ | ⋯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpora in Other Languages\n",
    "\n",
    "- NLTK is made for many languages with corpus?\n",
    "- Despite many cases, you need to learn how to manipulate character encoding in Python. And let's use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(nltk.corpus.cess_esp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Another corpus\n",
    "nltk.corpus.cess_esp.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(nltk.corpus.cess_esp.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Other corpus\n",
    "nltk.corpus.floresta.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(nltk.corpus.floresta.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.corpus.indian.words('hindi.pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(nltk.corpus.indian.words('hindi.pos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.corpus.udhr.fileids()[::50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.corpus.udhr.words('Javanese-Latin1')[11:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- udhr: Universal Declaration of Human Rights, 300 languages.\n",
    "- fileids: Contains information about the character encoding used in the file for this corpus. Things like UTF8 or Latin1.\n",
    "- Contains the condition frequency distribution to examine different points in the length of words in udhr corpus in selected languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "languages = ['Chickasaw', 'English', 'German_Deutsch',\n",
    "             'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist((lang, len(word))\n",
    "                               for lang in languages\n",
    "                               for word in udhr.words(lang + '-Latin1'))\n",
    "len(cfd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 2-2. Cumulative word length distribution\n",
    "\n",
    "- Six translations of the Universal Declaration of Human Rights are processed\n",
    "- this graph shows that words having five or fewer letters account for about 80% of Ibibio text, 60% of German text, and 25% of Inuktitut text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Condition frequency distribution\n",
    "cfd.plot(cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd.plot(cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Turn\n",
    "\n",
    "- Pick a language of interest in udhr.fileids() and define a variable raw_text = udhr.raw(Language-Latin1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_text = udhr.raw(['Chickasaw-Latin1', 'English-Latin1'])\n",
    "nltk.FreqDist(raw_text).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.FreqDist(raw_text).plot(cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_text = udhr.raw(['Chickasaw-Latin1', 'English-Latin1', \n",
    "                     'German_Deutsch-Latin1'])\n",
    "nltk.FreqDist(raw_text).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.FreqDist(raw_text).plot(cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unfortunately, for many languages, actual corpus is not yet available.\n",
    "- There is a lack of industrial support to develop for insufficient government or language resources.\n",
    "- There are no definite authoring systems in several languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Corpus Structure\n",
    "\n",
    "- A summary of the basic corpora functionality is shown in Table 2-3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(nltk.corpus.reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 2-3. Basic corpus functionality defined in NLTK\n",
    "\n",
    "Example | Description\n",
    "--- | ---\n",
    "fileids() | The files of the corpus\n",
    "fileids([categories]) | The files of the corpus corresponding to these categories\n",
    "categories() | The categories of the corpus\n",
    "categories([fileids]) | The categories of the corpus corresponding to these files\n",
    "raw() | The raw content of the corpus\n",
    "raw(fileids=[f1, f2, f3]) | The raw content of the specified files\n",
    "raw(categories=[c1, c2]) | The raw content of the specified categoreis\n",
    "words() | The words of the whole corpus\n",
    "words(fileids=[f1, f2, f3]) | The words of the specified fileids\n",
    "words(categories=[c1, c2]) | The words of the specified categoreis\n",
    "sents() | The sentences fo the specified categories\n",
    "sents(fileids=[f1, f2, f3]) | The sentences of the specified fileids\n",
    "sents(categories=[c1, c2]) | The sentences of the specified categories\n",
    "abspath(fileid) | The location of the given file on disk\n",
    "encoding(fileid) | The encoding of the file(if known)\n",
    "open(fileid) | Open a stream for reading the given corpus file\n",
    "root() | The path to the root of locally installed corpus\n",
    "readme() | The contents of the README file of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw = gutenberg.raw('burgess-busterbrown.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = gutenberg.words('burgess-busterbrown.txt')\n",
    "words[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "' '.join(words[1:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents = gutenberg.sents('burgess-busterbrown.txt')\n",
    "sents[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Your Own Corpus\n",
    "\n",
    "- If you have your own collection of text files, you can access them using the methods already discussed. You can easily load them and NLTK's PlaintextCorpusReader will help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_root = '/usr/share/dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordlists = PlaintextCorpusReader(corpus_root, '.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordlists.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls /usr/share/dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ws = wordlists.words('connectives')\n",
    "ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(wordlists.words('connectives'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another example. copy of Penn Treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import BracketParseCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_root = '/Users/re4lfl0w/nltk_data/corpora/treebank/parsed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_pattern = r'.*00[0-1][0-9]\\.prd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ptb = BracketParseCorpusReader(corpus_root, file_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ptb.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(ptb.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ptb.sents(fileids='wsj_0005.prd')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"Distributions\"></a>\n",
    "\n",
    "## <span style=\"color:#0b486b\">2. Conditional Frequency Distributions</span>\n",
    "\n",
    "- Text corpus is divided into various categories (genre, topic, author, etc.)\n",
    "- You can maintain a frequency distribution that is divided for each category.\n",
    "- conditional frequency distribution: A collection of frequency distributions. Each by different conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditions and Events\n",
    "\n",
    "- The frequency distribution counts the observed events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = ['The', 'Fulton', 'Country', 'Grand', 'Jury', 'said']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (condition, event)\n",
    "pairs = [('news', 'The'), ('news', 'Fulton'), ('news', 'Country')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Brown Corpus by genre: 15 conditions, 1,161, 192 events(one per word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Words by Genre\n",
    "\n",
    "- FreqDist (): accepts a simple list as input\n",
    "- ConditionalFreqDist (): received list pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist((genre, word)\n",
    "                               for genre in brown.categories()\n",
    "                               for word in brown.words(categories=genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd.conditions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genre_word = [(genre, word)\n",
    "              for genre in ['news', 'romance']\n",
    "              for word in brown.words(categories=genre)]\n",
    "len(genre_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genre_word[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genre_word[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(genre_word)\n",
    "cfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd.conditions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd['news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd['romance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(cfd['romance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd['romance']['could']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting and Tabulating Distributions\n",
    "\n",
    "- When combining two or more frequency distributions,\n",
    "- It is easy to initialize, and CondtionalFreqDist provides some useful methods for table drawing and graph drawing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist((target, fileid[:4])\n",
    "                               for fileid in inaugural.fileids()\n",
    "                               for w in inaugural.words(fileid)\n",
    "                               for target in ['america', 'citizen']\n",
    "                               if w.lower().startswith(target))\n",
    "\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "languages = ['Chickasaw', 'English', 'German_Deutsch',\n",
    "             'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist((lang, len(word))\n",
    "                               for lang in languages\n",
    "                               for word in udhr.words(lang + '-Latin1'))\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd.plot(cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd.tabulate(conditions=['English', 'German_Deutsch'],\n",
    "             samples=range(10), cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conditions\n",
    "cfd.tabulate(samples=range(10), cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Random Text with Bigrams\n",
    "\n",
    "- You can use conditional frequency distributions to create bigrams tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven',\n",
    "        'and', 'the', 'earth', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generator \n",
    "list(nltk.bigrams(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2-1. Generating random text\n",
    "\n",
    "- This program obtains all bigrams from the text of the book of Genesis,\n",
    "- then constructs a conditional frequency distribution to record which words are most likely to follow a given word;\n",
    "- e.g., after the word living, the most likely word is creature;\n",
    "- the generate_model() function uses this data, and a seed word, to generate random text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_model(cfdist, word, num=15):\n",
    "    for i in range(num):\n",
    "        print word,\n",
    "        word = cfdist[word].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = nltk.corpus.genesis.words('english-kjv.txt')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigrams = nltk.bigrams(text)\n",
    "list(bigrams)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print cfd['living']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_model(cfd, 'living', num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_model(cfd, 'living', num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# living\n",
    "cfd['living'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd['creature'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd['living']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "' '.join(cfd['living'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conditional frequency distributions are a useful data structure for many NLP tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 2-4. NLTK's conditional frequency distributions\n",
    "\n",
    "Example | Description\n",
    "--- | ---\n",
    "cfdist = ConditionalFreqDist(pairs) | Create a conditional frequency distribution from a list of pairs\n",
    "cdist.conditions() | Alphabetically sorted list of conditions\n",
    "cdist[condition] | The frequency distribution for this condition\n",
    "cdist\\[condition\\]\\[sample\\] | Frequency for the given sample for this condition\n",
    "cfdist.tabulate() | Tabulate the conditional frequency distribution\n",
    "cfdist.tabulate(samples, conditions) | Tabulation limited to the specified samples and conditions\n",
    "cfdist.plot() | Graphical plot of the conditional frequency distribution\n",
    "cfdist.plot(samples, conditions) | Graphical plot limited to the specified samples and conditions\n",
    "cfdist1 < cfdist2 | Test if samples in cfdist1 occur less frequently than in cfdist2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"Code\"></a>\n",
    "\n",
    "## <span style=\"color:#0b486b\">3. More Python: Reusing Code</span>\n",
    "\n",
    "  - Python functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "- function\n",
    "- parameters\n",
    "- return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(text) / len(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = 'hello world hello'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexical_diversity(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(set(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(s) / len(set(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(my_text_data):\n",
    "    word_count = len(my_text_data)\n",
    "    vocab_size = len(set(my_text_data))\n",
    "    diversity_score = word_count / vocab_size\n",
    "    return diversity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexical_diversity(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- local variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2-2 A Python function\n",
    "\n",
    "- This function tries to work out the plural form of any English noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plural(word):\n",
    "    if word.endswith('y'):\n",
    "        return word[:-1] + 'ies'\n",
    "    elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:\n",
    "        return word + 'es'\n",
    "    elif word.endswith('an'):\n",
    "        return word[:-2] + 'en'\n",
    "    else:\n",
    "        return word + 's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plural('fairy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plural('woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"Resources\"></a>\n",
    "\n",
    "## <span style=\"color:#0b486b\">4. Lexical Resources</span>\n",
    "\n",
    "- lexicon or lexical resource: collection of words and/or phrases along with associated informatin, such as part-of-speech and sense definitions\n",
    "\n",
    "```python\n",
    "vocab = sorted(set(my_text))\n",
    "word_freq = FreqDist(my_text)\n",
    "```\n",
    "\n",
    "- vocab, word_freq: Simple vocabulary resources\n",
    "- lexical entries: Vocabulary entry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordlist Corpora\n",
    "\n",
    "- NLTK: Has a few corpus, but nothing better than the wordlists.\n",
    "- Words Corpus /usr/dict/words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2-3 Filtering a text\n",
    "\n",
    "- text Of words.\n",
    "- Erase all items. It occurs in the existing wordlist. Only unusual or misspelled words remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unusual_words(text):\n",
    "    text_vocab = set(w.lower() for w in text if w.isalpha())\n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "#     text_vocab - english_vocab\n",
    "    unusual = text_vocab.difference(english_vocab)\n",
    "    return sorted(unusual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.corpus.words.words()[::10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unusual_words(nltk.corpus.gutenberg.words('austen-sense.txt'))[::100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unusual_words(nltk.corpus.nps_chat.words())[::100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stopwords: Words that appear very high, the, to, also\n",
    "   - Sometimes I want to be out of this document.\n",
    "   - And do the rest.\n",
    "- Usually have few vocabulary contents. They exist in the text fails to distinguish them from other texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_fraction(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return len(content) / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_fraction(nltk.corpus.reuters.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- help with stopsrods, filter text from 3 words\n",
    "- Combine two different corpus.\n",
    "- lexical resources are used to filter the content of text corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- wordlist: word puzzles, It is useful to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "puzzle_letters = nltk.FreqDist('egivrvonl')\n",
    "puzzle_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obligatory = 'r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordlist = nltk.corpus.words.words()\n",
    "len(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[w for w in wordlist if len(w) >= 6\n",
    "                        and obligatory in w\n",
    "                        and nltk.FreqDist(w) <= puzzle_letters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len([w for w in wordlist if len(w) >= 6\n",
    "                        and obligatory in w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = nltk.corpus.names\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "male_names = names.words('male.txt')\n",
    "male_names[::100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(male_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "female_names = names.words('female.txt')\n",
    "female_names[::100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(female_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[w for w in male_names if w in female_names][::50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 2-7. Conditional frequency distribution\n",
    "\n",
    "- This plot shows the number of female and male names ending with each letter of the alphabet\n",
    "- most names ending with a, e, or i are female;\n",
    "- names ending in h and l are equally to be male or female;\n",
    "- names ending in k, o, r, s, and t are likely to be male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist((fileid, name[-1])\n",
    "                                for fileid in names.fileids()\n",
    "                                for name in names.words(fileid))\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Prononcing Dictionary\n",
    "\n",
    "- Some rich lexical resource table\n",
    "- When you include words and some attributes in each row\n",
    "- NLTK adds the CMU Pronouncing Dictionary.\n",
    "- It was designed by speech synthesizers (synthesizers) for use.\n",
    "- For each word, these vocabularies provide a list of voice codes.\n",
    "- distinguish labels with each contrasting sound.\n",
    "- [Arpabet - Wikipedia, the free encyclopedia](http://en.wikipedia.org/wiki/Arpabet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entries = nltk.corpus.cmudict.entries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for entry in entries[39943:39951]:\n",
    "    print entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each word, the vocabulary provides a sound code for all lists.\n",
    "- Clear label For contrasting sound: phones\n",
    "- fire: 2 pronunciation: F AY1 R, F AY1 ERO\n",
    "- The symbols in the CMU Pronouncing Dictionary \n",
    "- http://en.wikipedia.org/wiki/Arpabet\n",
    "\n",
    "\n",
    "Each entry consists of two parts.\n",
    "Each person can proceed with a more complex version for the for statement.\n",
    "- As we loop around, the first part is the word, the second part is pron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word, pron in entries:\n",
    "    if len(pron) == 3:\n",
    "        ph1, ph2, ph3 = pron\n",
    "        if ph1 == 'P' and ph3 == 'T':\n",
    "            print word, ph2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The show program scanned what the vocabulary consists of 3 phones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syllable = ['N', 'IHO', 'K', 'S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Why? \n",
    "[word for word, pron in entries if pron[-4:] == syllable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entries[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entries[10][-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entries[10][1][-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entries[10][1][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entries[10][1][-2:] == 'AH0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entries[10][1][-2] == 'AH0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entries[10][-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[w for w, pron in entries if pron[-1] == 'M' and w[-1] == 'n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(set(w[:2] for w, pron in entries if pron[0] == 'N' and w[0] != 'n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pron, phone\n",
    "\n",
    "- What do you mean?\n",
    "- 0, 1, 2, What does this mean?\n",
    "- 1: primary stress\n",
    "- 2: secondary stress\n",
    "- 0: no stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stress(pron):\n",
    "    ''' input -> u'AO1', u'R', u'ER0', u'Z'\n",
    "        Check each of A01, R, and ER0 for each number\n",
    "    Output only numbers\n",
    "    '''\n",
    "    return [char for phone in pron for char in phone if char.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len([w for w, pron in entries if stress(pron) == ['0', '1', '0', '2', '0']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[w for w, pron in entries if stress(pron) == ['0', '1', '0', '2', '0']][::50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[w for w, pron in entries if stress(pron) == ['0', '2', '0', '1', '0']][::50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stress(): list comprehension. Duplicate nested for loop.\n",
    "- p words consisting of three sounds\n",
    "- group them according to their first and last sounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p3 = [(pron[0] + '-' + pron[2], word)\n",
    "      for (word, pron) in entries\n",
    "      if pron[0] == 'P' and len(pron) == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p3[::20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd.conditions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for template in cfd.conditions():\n",
    "    if len(cfd[template]) > 10:\n",
    "        words = cfd[template].keys()\n",
    "        wordlist = ' '.join(words)\n",
    "        print template, wordlist[:66] + '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prondict = nltk.corpus.cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(prondict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prondict['fire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prondict['blog'] = [['B', 'L', 'AA1', 'G']]\n",
    "prondict['blog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = ['natural', 'language', 'processing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can use the linguistic resources of processing text. Filterable with a few linguistic properties (like nouns) or to all words in text\n",
    "- For example, the text-to-speech function looks at the text of each word in the pronunciation dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[ph for w in text for ph in prondict[w][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative Wordlists\n",
    "\n",
    "- comparative wordlist: Can the table vocabulary be comparable?\n",
    "- Swadesh wordlists in various languages: list of 200 common words.\n",
    "- ISO 639 two-letter code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import swadesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "swadesh.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "swadesh.words('en')[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the entries () method in various languages and access to the etymology of words. Select a list of languages.\n",
    "- It can be changed to a simple dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fr2en = swadesh.entries(['fr', 'en'])\n",
    "fr2en[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translate = dict(fr2en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translate['chien']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translate['jeter']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's add two dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "de2en = swadesh.entries(['de', 'en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es2en = swadesh.entries(['es', 'en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translate.update(dict(de2en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translate.update(dict(es2en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translate['Hund']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translate['perro']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can compare words in various Germanic and Romance languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "languages = ['en', 'de', 'nl', 'es', 'fr', 'pt', 'la']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in [139, 140, 141, 142]:\n",
    "    print swadesh.entries(languages)[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shoebox and Toolbox Lexicons\n",
    "\n",
    "- The toolbox is for data manipulated by verbally used in the most popular tool.\n",
    "- The Toolbox consists of a collection of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toolbox.entries('rotokas.dic')[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It consists of attribute, value pair.\n",
    "- ('ps', 'V'): part-of-speech, verb\n",
    "- ('ge', 'gag'): gloss-into-English\n",
    "- The structure file of the unlocked Toolbox becomes difficult.\n",
    "- XML provides a powerful way to process this kind of corpus\n",
    "- Rotokas is a notable name in 12 phonemic inventions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"WordNet\"></a>\n",
    "\n",
    "## <span style=\"color:#0b486b\">5. WordNet</span>\n",
    "\n",
    "- WordNet: Semantically oriented English dictionary.\n",
    "- NLTK contains ENGLISH WordNet 155,287 words and 117,659 synonym sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Senses and Synonyms\n",
    "\n",
    "\n",
    "- a. Benz is credited with the invention of the motorcar.\n",
    "- b. Benz is credited with the invention of the automobile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- motorcar, automobile: synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.synsets('motorcar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- car.n.01: synset(synonym set). collection of synonymous words(or \"lemmas\") entry?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.synset('car.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.synset('car.n.01').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all the lemmas for a given synset\n",
    "wn.synset('car.n.01').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look up a particular lemma\n",
    "wn.lemma('car.n.01.automobile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the synset corresponding to a lemma\n",
    "wn.lemma('car.n.01.automobile').synset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the \"name\" of a lemma\n",
    "wn.lemma('car.n.01.automobile').name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.synsets('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for synset in wn.synsets('car'):\n",
    "    print synset.lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.lemmas('car')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The WordNet Hierarchy\n",
    "\n",
    "- WordNet synsets: Matches abstract concept. It does not always match words used in English.\n",
    "- Their concepts are linked together hierarchically.\n",
    "- unique beginners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wordnet: Makes it easy to translate between concepts.\n",
    "- hyponyms: segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "motorcar = wn.synset('car.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "types_of_motorcar = motorcar.hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "types_of_motorcar[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "types_of_motorcar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted([lemma.name() for synset in types_of_motorcar for lemma in synset.lemmas()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "motorcar.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paths = motorcar.hypernym_paths()\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[synset.name() for synset in paths[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[synset.name() for synset in paths[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the most general hypernyms(or root hypernyms)\n",
    "motorcar.root_hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Lexical Relations\n",
    "\n",
    "- lexical relations: Hypernym, hyponym\n",
    "- [meronyms](http://en.wikipedia.org/wiki/Meronymy): Semantic relations of linguistics\n",
    "- [Holonymy](http://en.wikipedia.org/wiki/Holonymy): semantic relation\n",
    "- substance_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.synset('tree.n.01').part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.synset('tree.n.01').substance_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.synset('tree.n.01').member_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for synset in wn.synsets('mint', wn.NOUN):\n",
    "#     print synset.name() + ':', synset.definition()\n",
    "    print '{0}: {1}'.format(synset.name(), synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.synset('mint.n.04').part_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.synset('mint.n.04').substance_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.synset('walk.v.01').entailments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.synset('eat.v.01').entailments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.synset('tease.v.03').entailments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- antonymy: Anti-wisdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Supply and Demand\n",
    "wn.lemma('supply.n.02.supply').antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.lemma('rush.v.01.rush').antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.lemma('horizontal.a.01.horizontal').antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.lemma('staccato.r.01.staccato').antonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dir\n",
    "\n",
    "- Determine which method is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir(wn.synset('harmony.n.02'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Similarity\n",
    "\n",
    "- synsets are linked by complex network vocabulary relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "right = wn.synset('right_whale.n.01')\n",
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orca = wn.synset('orca.n.01')\n",
    "orca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minke = wn.synset('minke_whale.n.01')\n",
    "minke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tortoise = wn.synset('tortoise.n.01')\n",
    "tortoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "novel = wn.synset('novel.n.01')\n",
    "novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "right.lowest_common_hypernyms(minke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "right.lowest_common_hypernyms(orca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "right.lowest_common_hypernyms(tortoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "right.lowest_common_hypernyms(novel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- baleen: very specific\n",
    "- vertebrate: very common\n",
    "- entity: completely general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.synset('minke_whale.n.01').min_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.synset('baleen_whale.n.01').min_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.synset('whale.n.02').min_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.synset('vertebrate.n.01').min_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.synset('entity.n.01').min_depth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similarity measure\n",
    "- path_similarity: Scores at 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "right.path_similarity(minke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "right.path_similarity(orca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "right.path_similarity(tortoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "right.path_similarity(novel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Several other similarity measures are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.corpus.verbnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"Summary\"></a>\n",
    "\n",
    "## <span style=\"color:#0b486b\">6. Summary</span>\n",
    "\n",
    "- A text corpus is a large, structured collection of texts. NLTK comes with many corpora, e.g., the Brown Corpus, nltk.corpus.brown\n",
    "- Some text corpora are categorized e.g., by genre or topic; sometimes the categories of a corpus overlap each other.\n",
    "- A conditional frequency distribution is a collection of frequency distributions, each one for a different condition. They can be used for counting word frequencies, given a context or a genre.\n",
    "- Python programs more than a few lines long should be entered using a text editor, saved to a file with a.py extension, and accessed using an import statement\n",
    "- Python functions permit you to associate a name with a particular block of code, and reuse that code as often as necessary.\n",
    "- Some functions, known as \"methods\", are associated with an object, and we give the object name followed by a period followed by the method name, like this: **x.funct(y)**, e.g., **word.isalpha()**\n",
    "- To find out about somre variable v, type help(v) in the Python interactive interpreter to read the help entry for this kind of object\n",
    "- WordNet is a semantically oriented dictionary of English, consisting of synonymsets--or synsets--and organized into a network\n",
    "- Some functions are not available by default, but must be accessed using Python's import statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"reading\"></a>\n",
    "\n",
    "## <span style=\"color:#0b486b\">7. Further Reading</span>\n",
    "\n",
    "- [Extra materials](http://www.nltk.org)\n",
    "- [Corpus HOWTO](http://www.nltk.org/howto)\n",
    "- Linguistic Data Consortium(LDC)\n",
    "- European Language Resource Agency(ELRA)\n",
    "- [OLAC Meta data](http://www.language-archives.org)\n",
    "- [Ethnologue](http://www.ethnologue.com)\n",
    "- [WordNet](http://globalwordnet.org)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
